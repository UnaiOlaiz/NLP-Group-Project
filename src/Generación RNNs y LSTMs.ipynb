{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generación de Versos con Modelos Secuenciales\n",
    "En este apartado trabajaremos con modelos secuenciales como *Recurrent Neural Networks* (`RNNs`) y *Long Short Term Memories* (`LSTMs`), modelo que aunque no sean tan potentes como algunos de los modelos generativos que veremos después, hemos considerado útiles de desarrollar para reflejar la evolución real que tuvieron los modelos generativos a lo largo de los años.\n",
    "\n",
    "Debido a que inicialmente consideramos que el número de instancias que ofrecía el dataset que usabamos para manipular el **Corán**, hemos decidido trabajar con un segundo conjunto de datos el cual ofrece casi diez veces el número de instancias entrenables que el primero. [El dataset disponible en Kaggle](https://www.kaggle.com/datasets/fahd09/hadith-dataset), colleciona varios `Hadith`-s, representaciones de acciones o palabras dichas por el **Profeta Mohammed**. No obstante, este archivo presenta una estrutura completamente diferente al habitual, por lo tanto, requerirá de una limpieza y manipulación diferente. \n",
    "\n",
    "A lo largo de este cuaderno, crearemos todas las clases y funciones necesarias para crear y usar los modelos secuenciales generativos precedentes a los **Transformers**, aún así, en la documentación principal profundizaremos más en el análisis completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674c0ca",
   "metadata": {},
   "source": [
    "### Librerías Necearias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fasttext\n",
    "import unicodedata\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893a7df",
   "metadata": {},
   "source": [
    "### Código de Clases + Funciones Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221afb",
   "metadata": {},
   "source": [
    "Clase Vocabulary general, donde se definen las funciones principales que heredará la clase del vocabulario específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c545a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        # inicializar atributos\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # función para añadir token (nuevo) al diccionario\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        # función para añadir N > 1 tokens al diccionario\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        # función para obtener el token del idx introducido\n",
    "        if index not in self._idx_to_token:\n",
    "            return \"<UNK>\"\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # devuelve el tamaño del diccionario\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        # devuelve el tamaño del vocabulario\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecf5b",
   "metadata": {},
   "source": [
    "Vocabulary especial Corán con los tokens especiales \\<eos>, \\<bos>, ... Hereda de la clase Vocabulario principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        # añadimos los tokens especiales a nuestro diccionario\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a364e89",
   "metadata": {},
   "source": [
    "Vectorizer - Nuestro vectorizador que será responsable de convertir los labels a vectores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf208cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        # constructor de vocabulario \n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        # función donde vectorizamos texto\n",
    "\n",
    "        indices = [self.char_vocab.begin_seq_index] # añadimos <bos> al principio\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text) # añadimos los tokens restantes en medio de la oración\n",
    "        indices.append(self.char_vocab.end_seq_index) # añadimos <eos> al final\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector será <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector será os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5897a",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento (métricas de evaluación, argumentos de entrenamiento, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    # genera batches para mandarlos al cpu/gpu (si tenemos cuda)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # loss function, en nuestro caso el cross entropy loss. Ya que compararemos la distribución de predicciones con la ground truth\n",
    "    # B, T y V son las dimensiones de nuestro tensor predicho\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "    # calculamos la comparación entre distribuciones predichas y verdaderas\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # función para calcular la accuracy, comparando cada caracter predicho con el ground truth\n",
    "    y_hat = y_pred.argmax(dim=-1)  \n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    # sacado del notebook de ALUD, argumentos de entrenamiento\n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    # función para tener en cuenta mejora/desmejora de rendimiento -> early_stopping\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    # código para el early_stopping\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c155f",
   "metadata": {},
   "source": [
    "Funciones para obtener y mostrar los nuevos versos una vez entrenados los modelos, emplearemos estas funciones una vez realizados los entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caeef39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8, top_k=None):\n",
    "    # Función para coger los nuevos versos generados y mostrarlos\n",
    "    # En nuestro caso 10 samples\n",
    "    model.eval()\n",
    "    vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(x, apply_softmax=False)         \n",
    "                next_logits = logits[0, -1] / max(temperature, 1e-8)\n",
    "\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, ix = torch.topk(next_logits, k=top_k)\n",
    "                    filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                    filtered[ix] = v\n",
    "                    next_logits = filtered\n",
    "\n",
    "                probs = torch.softmax(next_logits, dim=0)\n",
    "                next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == vocab.end_seq_index:\n",
    "                break\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    # Función para devoler los labels de los índices conseguidos en la función anterior\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecede2",
   "metadata": {},
   "source": [
    "Como usaremos los pesos del modelo de embeddings usado anteriormente (`fastText`), los importaremos aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pesos(vectorizer, modelo_ft):\n",
    "    vocab = vectorizer.char_vocab\n",
    "    token_to_idx = vocab._token_to_idx\n",
    "    tamaño_vocab = len(token_to_idx)\n",
    "    embedding_dim = modelo_ft.get_dimension()\n",
    "    pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "    for token, idx in token_to_idx.items():\n",
    "        pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "    return torch.FloatTensor(pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476beb1",
   "metadata": {},
   "source": [
    "Función para devolver y mostrar resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e89f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuevos_versos(nombre_modelo, nombre_vectorizer):\n",
    "    num_names = 10\n",
    "\n",
    "    model = nombre_modelo.cpu()\n",
    "\n",
    "    sampled_verses = decode_samples(\n",
    "        sample_from_model(\n",
    "            model,\n",
    "            nombre_vectorizer,\n",
    "            num_samples=num_names,\n",
    "            max_length=300,\n",
    "            temperature=0.8\n",
    "        ),\n",
    "        nombre_vectorizer\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(num_names):\n",
    "        print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e27712",
   "metadata": {},
   "source": [
    "### Funciones para los entrenamientos: RNN y LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a0b72",
   "metadata": {},
   "source": [
    "Clase Dataset del Corán, lo amoldaremos al formato en que viene escrito el **Corán**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aca21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2 # el +2 incluye los tokens del diccionario + <bos> y <eos>\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    # a partir de aquí hay metodos necesarios para manipular nuestro dataset específico\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0b718",
   "metadata": {},
   "source": [
    "Función de entrenamiento RNN, arquitectura que usaremos para la NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8dbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(coran_path, output_path, ruta_ft):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # tamaño del hidden state del RNN\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3, # lr\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    # código para guardar/cargar archivos\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    # aquí configuramos y llamamos a la función de pesos de fastText\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ruta_ft)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    # Y creamos el modelo\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=None\n",
    "    ).to(args.device)\n",
    "\n",
    "    # A partir de aquí, entrenamiento normal\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Validation\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cb61e",
   "metadata": {},
   "source": [
    "Función de entrenamiento LSTM, casi idéntico a la arquitectura RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f3ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(coran_path, output_path, ruta_ft):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ruta_ft)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=None\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\") \n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa682576",
   "metadata": {},
   "source": [
    "## Dataset del Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a989e",
   "metadata": {},
   "source": [
    "### RNN - Corán\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e50241",
   "metadata": {},
   "source": [
    "Modelo RNN para el Corán, arquitectura interna de la NN con sus funciones típicas de *\\__init__* y *forward*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6640ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    # nuestro modelo nn para el rnn\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5,\n",
    "                 pretrained_embeddings_ft = None):\n",
    "        super().__init__()\n",
    "        # arquitectura de nuestra rnn\n",
    "\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) # capa de inicio del tamaño del vocabulario\n",
    "        # Aquí metemos los embeddings (pesos) del fasttext\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\") # rnn\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) # fully connected\n",
    "        self.dropout_p = dropout_p # probabilidad de dropout de neuronas\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)             \n",
    "        y_out, _ = self.rnn(x_emb)               \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  \n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1124485",
   "metadata": {},
   "source": [
    "Entrenamiento del RNN para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8865e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/776746513.py:76: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29701ff3facf4cc58d48e701ef0a5484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.1985 | val_loss=2.7052 | val_acc=0.2970\n",
      "Epoch 002 | train_loss=2.6372 | val_loss=2.5042 | val_acc=0.3285\n",
      "Epoch 003 | train_loss=2.4997 | val_loss=2.4174 | val_acc=0.3423\n",
      "Epoch 004 | train_loss=2.4226 | val_loss=2.3597 | val_acc=0.3512\n",
      "Epoch 005 | train_loss=2.3700 | val_loss=2.3174 | val_acc=0.3588\n",
      "Epoch 006 | train_loss=2.3306 | val_loss=2.2840 | val_acc=0.3661\n",
      "Epoch 007 | train_loss=2.2970 | val_loss=2.2542 | val_acc=0.3722\n",
      "Epoch 008 | train_loss=2.2708 | val_loss=2.2276 | val_acc=0.3773\n",
      "Epoch 009 | train_loss=2.2473 | val_loss=2.2050 | val_acc=0.3790\n",
      "Epoch 010 | train_loss=2.2271 | val_loss=2.1864 | val_acc=0.3832\n",
      "Epoch 011 | train_loss=2.2093 | val_loss=2.1680 | val_acc=0.3900\n",
      "Epoch 012 | train_loss=2.1937 | val_loss=2.1518 | val_acc=0.3942\n",
      "Epoch 013 | train_loss=2.1798 | val_loss=2.1378 | val_acc=0.3974\n",
      "Epoch 014 | train_loss=2.1669 | val_loss=2.1244 | val_acc=0.4013\n",
      "Epoch 015 | train_loss=2.1550 | val_loss=2.1116 | val_acc=0.4049\n",
      "Epoch 016 | train_loss=2.1446 | val_loss=2.1013 | val_acc=0.4076\n",
      "Epoch 017 | train_loss=2.1345 | val_loss=2.0941 | val_acc=0.4085\n",
      "Epoch 018 | train_loss=2.1248 | val_loss=2.0827 | val_acc=0.4121\n",
      "Epoch 019 | train_loss=2.1183 | val_loss=2.0719 | val_acc=0.4145\n",
      "Epoch 020 | train_loss=2.1096 | val_loss=2.0665 | val_acc=0.4180\n",
      "Epoch 021 | train_loss=2.1027 | val_loss=2.0605 | val_acc=0.4195\n",
      "Epoch 022 | train_loss=2.0984 | val_loss=2.0535 | val_acc=0.4204\n",
      "Epoch 023 | train_loss=2.0909 | val_loss=2.0470 | val_acc=0.4227\n",
      "Epoch 024 | train_loss=2.0865 | val_loss=2.0411 | val_acc=0.4250\n",
      "Epoch 025 | train_loss=2.0776 | val_loss=2.0358 | val_acc=0.4267\n",
      "Epoch 026 | train_loss=2.0743 | val_loss=2.0309 | val_acc=0.4295\n",
      "Epoch 027 | train_loss=2.0686 | val_loss=2.0274 | val_acc=0.4302\n",
      "Epoch 028 | train_loss=2.0628 | val_loss=2.0209 | val_acc=0.4340\n",
      "Epoch 029 | train_loss=2.0595 | val_loss=2.0171 | val_acc=0.4334\n",
      "Epoch 030 | train_loss=2.0568 | val_loss=2.0106 | val_acc=0.4372\n",
      "Epoch 031 | train_loss=2.0525 | val_loss=2.0082 | val_acc=0.4391\n",
      "Epoch 032 | train_loss=2.0483 | val_loss=2.0042 | val_acc=0.4406\n",
      "Epoch 033 | train_loss=2.0457 | val_loss=2.0007 | val_acc=0.4400\n",
      "Epoch 034 | train_loss=2.0400 | val_loss=1.9986 | val_acc=0.4407\n",
      "Epoch 035 | train_loss=2.0374 | val_loss=1.9964 | val_acc=0.4420\n",
      "Epoch 036 | train_loss=2.0326 | val_loss=1.9921 | val_acc=0.4437\n",
      "Epoch 037 | train_loss=2.0304 | val_loss=1.9873 | val_acc=0.4443\n",
      "Epoch 038 | train_loss=2.0281 | val_loss=1.9862 | val_acc=0.4458\n",
      "Epoch 039 | train_loss=2.0232 | val_loss=1.9826 | val_acc=0.4451\n",
      "Epoch 040 | train_loss=2.0190 | val_loss=1.9805 | val_acc=0.4492\n",
      "Epoch 041 | train_loss=2.0183 | val_loss=1.9756 | val_acc=0.4494\n",
      "Epoch 042 | train_loss=2.0161 | val_loss=1.9763 | val_acc=0.4482\n",
      "Epoch 043 | train_loss=2.0142 | val_loss=1.9741 | val_acc=0.4485\n",
      "Epoch 044 | train_loss=2.0112 | val_loss=1.9701 | val_acc=0.4511\n",
      "Epoch 045 | train_loss=2.0063 | val_loss=1.9664 | val_acc=0.4503\n",
      "Epoch 046 | train_loss=2.0046 | val_loss=1.9660 | val_acc=0.4502\n",
      "Epoch 047 | train_loss=2.0055 | val_loss=1.9647 | val_acc=0.4563\n",
      "Epoch 048 | train_loss=2.0015 | val_loss=1.9612 | val_acc=0.4535\n",
      "Epoch 049 | train_loss=2.0002 | val_loss=1.9598 | val_acc=0.4530\n",
      "Epoch 050 | train_loss=1.9970 | val_loss=1.9581 | val_acc=0.4547\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/coran/coran_rnn_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154e9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "اقول الله ۚ ولذي شكرون\n",
      "\n",
      " Verse 2:\n",
      "وبالله وازوهه منهم بشر الددكم فاسر والذين كان الذيو ينشاب\n",
      "\n",
      " Verse 3:\n",
      "وليا اكاتاتم ورجدوك من يبير مهمنما\n",
      "\n",
      " Verse 4:\n",
      "وان الان لن عذاب ۚ ولك الي طبعن الصادهم فضل الا الصول الي الله روجعا في الالغروا الشار فالذين يحسم اولي المعينع م شرك به الله ويوم يابع ني بانا هم تقولي وما انتم لهم من عليه الي يرجون علي الوالنوا يصدونا من يحم ۚ واللما توتياخر لله ان ارجامه للمنهم بهم كنفتم معضد الاكيب\n",
      "\n",
      " Verse 5:\n",
      "قال لفكلا ممتكم علي الين تملانا عليم\n",
      "\n",
      " Verse 6:\n",
      "ان اللم\n",
      "\n",
      " Verse 7:\n",
      "من تجتم ولا تكبرين\n",
      "\n",
      " Verse 8:\n",
      "فالله رسله اا الذين تنبرون عبيلها ۖ فاتان تبال قالوا اعلم ۚ ونال الناق تلكم من الذروه علي البراء وكنوا ويقبم الاتين الذاطين لا تمتمون\n",
      "\n",
      " Verse 9:\n",
      "فاسمعوا الشهسك ۚ وان اسماوا يعاب لوا ايتربا\n",
      "\n",
      " Verse 10:\n",
      "فلا يس فيما انت عليم\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85149e9f",
   "metadata": {},
   "source": [
    "Entrenamiento RNN Corán en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15c8f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/776746513.py:76: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e443f0329614a39af5870ccfd166fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8400 | val_loss=2.3406 | val_acc=0.3593\n",
      "Epoch 002 | train_loss=2.3090 | val_loss=2.1265 | val_acc=0.3883\n",
      "Epoch 003 | train_loss=2.1560 | val_loss=2.0118 | val_acc=0.4208\n",
      "Epoch 004 | train_loss=2.0622 | val_loss=1.9314 | val_acc=0.4351\n",
      "Epoch 005 | train_loss=1.9947 | val_loss=1.8686 | val_acc=0.4501\n",
      "Epoch 006 | train_loss=1.9403 | val_loss=1.8171 | val_acc=0.4638\n",
      "Epoch 007 | train_loss=1.8990 | val_loss=1.7746 | val_acc=0.4781\n",
      "Epoch 008 | train_loss=1.8628 | val_loss=1.7388 | val_acc=0.4881\n",
      "Epoch 009 | train_loss=1.8335 | val_loss=1.7066 | val_acc=0.4986\n",
      "Epoch 010 | train_loss=1.8062 | val_loss=1.6793 | val_acc=0.5024\n",
      "Epoch 011 | train_loss=1.7831 | val_loss=1.6552 | val_acc=0.5120\n",
      "Epoch 012 | train_loss=1.7643 | val_loss=1.6347 | val_acc=0.5170\n",
      "Epoch 013 | train_loss=1.7457 | val_loss=1.6155 | val_acc=0.5212\n",
      "Epoch 014 | train_loss=1.7298 | val_loss=1.5978 | val_acc=0.5268\n",
      "Epoch 015 | train_loss=1.7161 | val_loss=1.5837 | val_acc=0.5301\n",
      "Epoch 016 | train_loss=1.7035 | val_loss=1.5692 | val_acc=0.5337\n",
      "Epoch 017 | train_loss=1.6893 | val_loss=1.5573 | val_acc=0.5370\n",
      "Epoch 018 | train_loss=1.6807 | val_loss=1.5445 | val_acc=0.5400\n",
      "Epoch 019 | train_loss=1.6704 | val_loss=1.5348 | val_acc=0.5441\n",
      "Epoch 020 | train_loss=1.6620 | val_loss=1.5251 | val_acc=0.5484\n",
      "Epoch 021 | train_loss=1.6518 | val_loss=1.5163 | val_acc=0.5507\n",
      "Epoch 022 | train_loss=1.6439 | val_loss=1.5086 | val_acc=0.5537\n",
      "Epoch 023 | train_loss=1.6354 | val_loss=1.5006 | val_acc=0.5556\n",
      "Epoch 024 | train_loss=1.6295 | val_loss=1.4933 | val_acc=0.5582\n",
      "Epoch 025 | train_loss=1.6224 | val_loss=1.4873 | val_acc=0.5590\n",
      "Epoch 026 | train_loss=1.6153 | val_loss=1.4808 | val_acc=0.5624\n",
      "Epoch 027 | train_loss=1.6109 | val_loss=1.4758 | val_acc=0.5632\n",
      "Epoch 028 | train_loss=1.6046 | val_loss=1.4683 | val_acc=0.5650\n",
      "Epoch 029 | train_loss=1.6005 | val_loss=1.4664 | val_acc=0.5652\n",
      "Epoch 030 | train_loss=1.5964 | val_loss=1.4607 | val_acc=0.5670\n",
      "Epoch 031 | train_loss=1.5923 | val_loss=1.4552 | val_acc=0.5678\n",
      "Epoch 032 | train_loss=1.5872 | val_loss=1.4510 | val_acc=0.5703\n",
      "Epoch 033 | train_loss=1.5828 | val_loss=1.4477 | val_acc=0.5711\n",
      "Epoch 034 | train_loss=1.5809 | val_loss=1.4426 | val_acc=0.5725\n",
      "Epoch 035 | train_loss=1.5750 | val_loss=1.4400 | val_acc=0.5744\n",
      "Epoch 036 | train_loss=1.5703 | val_loss=1.4356 | val_acc=0.5741\n",
      "Epoch 037 | train_loss=1.5681 | val_loss=1.4316 | val_acc=0.5773\n",
      "Epoch 038 | train_loss=1.5631 | val_loss=1.4286 | val_acc=0.5787\n",
      "Epoch 039 | train_loss=1.5610 | val_loss=1.4244 | val_acc=0.5785\n",
      "Epoch 040 | train_loss=1.5585 | val_loss=1.4233 | val_acc=0.5798\n",
      "Epoch 041 | train_loss=1.5563 | val_loss=1.4199 | val_acc=0.5791\n",
      "Epoch 042 | train_loss=1.5504 | val_loss=1.4177 | val_acc=0.5812\n",
      "Epoch 043 | train_loss=1.5488 | val_loss=1.4132 | val_acc=0.5830\n",
      "Epoch 044 | train_loss=1.5466 | val_loss=1.4133 | val_acc=0.5820\n",
      "Epoch 045 | train_loss=1.5464 | val_loss=1.4117 | val_acc=0.5813\n",
      "Epoch 046 | train_loss=1.5418 | val_loss=1.4067 | val_acc=0.5841\n",
      "Epoch 047 | train_loss=1.5393 | val_loss=1.4032 | val_acc=0.5858\n",
      "Epoch 048 | train_loss=1.5360 | val_loss=1.4007 | val_acc=0.5864\n",
      "Epoch 049 | train_loss=1.5340 | val_loss=1.3986 | val_acc=0.5858\n",
      "Epoch 050 | train_loss=1.5320 | val_loss=1.3980 | val_acc=0.5871\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/coran/coran_rnn_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1140e",
   "metadata": {},
   "source": [
    "Obtenemos los nuevos versos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e0aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and have sery re and you o creaty forper forced\n",
      "\n",
      " Verse 2:\n",
      "but fead when they not grow and comp ones in the silde to you crelinge the parade umong them with him do not send and while you indeed has but feat to evelely believe what they will be indous and he were but they will have forming sees were you wealishe pavessed to you and it is they peace from alla\n",
      "\n",
      " Verse 3:\n",
      "there to you and will not had seople in the seak and will to the encingder me not and has been the sack so we who are allah are not has nirnent it hat and allah for then we are suongis of people accoterith wemall and their lord and except the songer to have prome and indean intaht air ahthersand you\n",
      "\n",
      " Verse 4:\n",
      "there are this way rithen servealy with the people dis ahment igrilt diddented you astahe sorovess on the lord to ligh inlent om thise weurre slise whine thet frith of if their relued hak indeed allah made afy you and them allis and erseng hig the comselves what wh is it it\n",
      "\n",
      " Verse 5:\n",
      "and they hive is nes punder the servess what they will stire the destife it is ever ay of is whoever will\n",
      "\n",
      " Verse 6:\n",
      "and we he peringen hes is believer of the were they did indeed therese they his doen in allah af and thind them woult say and we casarated them and allinveared him spell and abatter as walled have a tay of youk ond is on the requithes the torn and we have were sfir igces as his fore of allah from al\n",
      "\n",
      " Verse 7:\n",
      "and they have believed will they will not for a galiever of the peromisty and and in the willh and has noy oun this the mistaht have mare goul mone with our so to the fores and is the eerse the torr and the peos manfut the mid uland and sibuse wion the peamess saffer in when them they will be belie\n",
      "\n",
      " Verse 8:\n",
      "them all allah will you wian them mos and has bum from the are averses of tors and and onvericusest them and woves and entire tard them and they worevist them indeed us will he will be a touls do not to reared for whithans is indient thee then not for you the forgiving that abreets hhace\n",
      "\n",
      " Verse 9:\n",
      "and hay saccent in the have theor in allah and ir the issume theresi not are that it and wirh and the sows whine you should have me then wx and be then what allah of them those whos the will allah will but thit and in the ship the peoplas they sata in the believers and they will be of are the his th\n",
      "\n",
      " Verse 10:\n",
      "and in the courned to the panassed of whowe tor the tored to have deptelising and were cancatfinh clace beljed and they conceams and distosot to you but the and you they wera ackso beret that you said oul is alles\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd760ebc",
   "metadata": {},
   "source": [
    "### LSTM - Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee0af9",
   "metadata": {},
   "source": [
    "Modelo del LSTM para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5, pretrained_embeddings_ft=None):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              \n",
    "        y_out, _ = self.lstm(x_emb)              \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                 \n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f224",
   "metadata": {},
   "source": [
    "Entrenamiento del LSTM para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0784b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/790377902.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327100c2003343c583973a933406ee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.5568 | val_loss=2.2842 | val_acc=0.3650\n",
      "Epoch 002 | train_loss=2.1703 | val_loss=2.1156 | val_acc=0.4034\n",
      "Epoch 003 | train_loss=2.0399 | val_loss=2.0243 | val_acc=0.4309\n",
      "Epoch 004 | train_loss=1.9612 | val_loss=1.9674 | val_acc=0.4482\n",
      "Epoch 005 | train_loss=1.9054 | val_loss=1.9163 | val_acc=0.4618\n",
      "Epoch 006 | train_loss=1.8642 | val_loss=1.8829 | val_acc=0.4705\n",
      "Epoch 007 | train_loss=1.8284 | val_loss=1.8591 | val_acc=0.4769\n",
      "Epoch 008 | train_loss=1.7998 | val_loss=1.8333 | val_acc=0.4840\n",
      "Epoch 009 | train_loss=1.7745 | val_loss=1.8199 | val_acc=0.4902\n",
      "Epoch 010 | train_loss=1.7542 | val_loss=1.8036 | val_acc=0.4917\n",
      "Epoch 011 | train_loss=1.7324 | val_loss=1.7955 | val_acc=0.4943\n",
      "Epoch 012 | train_loss=1.7134 | val_loss=1.7818 | val_acc=0.5011\n",
      "Epoch 013 | train_loss=1.6980 | val_loss=1.7750 | val_acc=0.4994\n",
      "Epoch 014 | train_loss=1.6874 | val_loss=1.7642 | val_acc=0.5060\n",
      "Epoch 015 | train_loss=1.6705 | val_loss=1.7570 | val_acc=0.5062\n",
      "Epoch 016 | train_loss=1.6565 | val_loss=1.7504 | val_acc=0.5082\n",
      "Epoch 017 | train_loss=1.6455 | val_loss=1.7485 | val_acc=0.5112\n",
      "Epoch 018 | train_loss=1.6322 | val_loss=1.7350 | val_acc=0.5134\n",
      "Epoch 019 | train_loss=1.6210 | val_loss=1.7269 | val_acc=0.5170\n",
      "Epoch 020 | train_loss=1.6110 | val_loss=1.7208 | val_acc=0.5161\n",
      "Epoch 021 | train_loss=1.6010 | val_loss=1.7201 | val_acc=0.5184\n",
      "Epoch 022 | train_loss=1.5934 | val_loss=1.7159 | val_acc=0.5179\n",
      "Epoch 023 | train_loss=1.5817 | val_loss=1.7124 | val_acc=0.5197\n",
      "Epoch 024 | train_loss=1.5727 | val_loss=1.6996 | val_acc=0.5208\n",
      "Epoch 025 | train_loss=1.5650 | val_loss=1.6957 | val_acc=0.5231\n",
      "Epoch 026 | train_loss=1.5557 | val_loss=1.6986 | val_acc=0.5235\n",
      "Epoch 027 | train_loss=1.5462 | val_loss=1.7029 | val_acc=0.5264\n",
      "Epoch 028 | train_loss=1.5266 | val_loss=1.6919 | val_acc=0.5274\n",
      "Epoch 029 | train_loss=1.5212 | val_loss=1.6940 | val_acc=0.5264\n",
      "Epoch 030 | train_loss=1.5145 | val_loss=1.6947 | val_acc=0.5269\n",
      "Epoch 031 | train_loss=1.5019 | val_loss=1.6882 | val_acc=0.5276\n",
      "Epoch 032 | train_loss=1.4975 | val_loss=1.6905 | val_acc=0.5279\n",
      "Epoch 033 | train_loss=1.4950 | val_loss=1.6907 | val_acc=0.5274\n",
      "Epoch 034 | train_loss=1.4893 | val_loss=1.6880 | val_acc=0.5277\n",
      "Epoch 035 | train_loss=1.4873 | val_loss=1.6883 | val_acc=0.5281\n",
      "Epoch 036 | train_loss=1.4861 | val_loss=1.6885 | val_acc=0.5279\n",
      "Epoch 037 | train_loss=1.4816 | val_loss=1.6878 | val_acc=0.5289\n",
      "Epoch 038 | train_loss=1.4812 | val_loss=1.6888 | val_acc=0.5284\n",
      "Epoch 039 | train_loss=1.4822 | val_loss=1.6872 | val_acc=0.5294\n",
      "Epoch 040 | train_loss=1.4784 | val_loss=1.6898 | val_acc=0.5285\n",
      "Epoch 041 | train_loss=1.4786 | val_loss=1.6878 | val_acc=0.5292\n",
      "Epoch 042 | train_loss=1.4781 | val_loss=1.6877 | val_acc=0.5294\n",
      "Epoch 043 | train_loss=1.4775 | val_loss=1.6877 | val_acc=0.5290\n",
      "Epoch 044 | train_loss=1.4768 | val_loss=1.6877 | val_acc=0.5295\n",
      "Early stopping activado.\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/coran/coran_lstm_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02fecd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "وضلغ عليه فرعون\n",
      "\n",
      " Verse 2:\n",
      "واذ قالوا الذين يعلمون ان الذين كفروا طايام عميل ما جاءوا شهرا فاتوا بالجحمن منه الي انفسهم ۚ لهم هي يوم يقوم القيم العرف لاكثر ال نعمل مهلكم والله كربا وهم اله الرحمه ۖ ان الله الحكم وانا لتقول به ۚ وكان الله لا يتبعون ما تسجه الاتينا ۚ والذين يستكونوا عن ما اضتدوا به والجنه لا تيتهاعم الاخره ۖ وان\n",
      "\n",
      " Verse 3:\n",
      "وان الله لا يستكرون بالبتغير\n",
      "\n",
      " Verse 4:\n",
      "لا يستطعون انقلبوا من السماء الا ما كنتم تعلمون\n",
      "\n",
      " Verse 5:\n",
      "قال رب اني انزل اليس لعلمه ياتينا الي الظالمين\n",
      "\n",
      " Verse 6:\n",
      "ان يقول الله ونتكم اخره ۖ وكانوا قالوا الا الموت ۚ ان الله يعلم الموعاهم ان ايتهم صابرين غير البرسه وفي الملائكه وهو وحق الله واخذهم ولا يؤمنون\n",
      "\n",
      " Verse 7:\n",
      "نستاف فيه من المسراه مجرمين\n",
      "\n",
      " Verse 8:\n",
      "فلما اتخذ من السماء او تفرقوا في الحسانه ۗ والله الام بعلي شهيدا وقلوبهم او كانوا يعلمون\n",
      "\n",
      " Verse 9:\n",
      "والذين اخذلون ظلام ۗ والله لا يكبن بالحق ۚ واتخذوا المؤمنات والنار ثم يريون واتيناه من المؤمنين\n",
      "\n",
      " Verse 10:\n",
      "ربهم جنات عليه ۗ ۚ الا ان الله في الارض وما يدعون الي الله زين يحاره ۚ هل مستجد بنيا اخرا جنب اليك ۖ وان امتوا من ربكم ۖ واخذنا عليهم المن قريدا ولا تري بعض الا فضل الله بما جاءوا ۖ وليا وقال لمن القوا الله بغير الرحيم\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d3128",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento inglés de LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb9054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/790377902.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031587244cd543aca31be22e55d481ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.2428 | val_loss=1.8082 | val_acc=0.4615\n",
      "Epoch 002 | train_loss=1.7094 | val_loss=1.5443 | val_acc=0.5356\n",
      "Epoch 003 | train_loss=1.5275 | val_loss=1.4156 | val_acc=0.5723\n",
      "Epoch 004 | train_loss=1.4246 | val_loss=1.3365 | val_acc=0.5938\n",
      "Epoch 005 | train_loss=1.3559 | val_loss=1.2767 | val_acc=0.6129\n",
      "Epoch 006 | train_loss=1.3056 | val_loss=1.2412 | val_acc=0.6248\n",
      "Epoch 007 | train_loss=1.2663 | val_loss=1.2099 | val_acc=0.6325\n",
      "Epoch 008 | train_loss=1.2352 | val_loss=1.1853 | val_acc=0.6394\n",
      "Epoch 009 | train_loss=1.2091 | val_loss=1.1635 | val_acc=0.6445\n",
      "Epoch 010 | train_loss=1.1881 | val_loss=1.1513 | val_acc=0.6501\n",
      "Epoch 011 | train_loss=1.1698 | val_loss=1.1328 | val_acc=0.6528\n",
      "Epoch 012 | train_loss=1.1544 | val_loss=1.1248 | val_acc=0.6570\n",
      "Epoch 013 | train_loss=1.1396 | val_loss=1.1145 | val_acc=0.6597\n",
      "Epoch 014 | train_loss=1.1275 | val_loss=1.1108 | val_acc=0.6620\n",
      "Epoch 015 | train_loss=1.1143 | val_loss=1.0992 | val_acc=0.6658\n",
      "Epoch 016 | train_loss=1.1032 | val_loss=1.0896 | val_acc=0.6685\n",
      "Epoch 017 | train_loss=1.0935 | val_loss=1.0804 | val_acc=0.6703\n",
      "Epoch 018 | train_loss=1.0823 | val_loss=1.0748 | val_acc=0.6730\n",
      "Epoch 019 | train_loss=1.0738 | val_loss=1.0706 | val_acc=0.6744\n",
      "Epoch 020 | train_loss=1.0671 | val_loss=1.0636 | val_acc=0.6756\n",
      "Epoch 021 | train_loss=1.0595 | val_loss=1.0591 | val_acc=0.6764\n",
      "Epoch 022 | train_loss=1.0524 | val_loss=1.0551 | val_acc=0.6802\n",
      "Epoch 023 | train_loss=1.0445 | val_loss=1.0503 | val_acc=0.6794\n",
      "Epoch 024 | train_loss=1.0390 | val_loss=1.0460 | val_acc=0.6810\n",
      "Epoch 025 | train_loss=1.0319 | val_loss=1.0433 | val_acc=0.6834\n",
      "Epoch 026 | train_loss=1.0265 | val_loss=1.0383 | val_acc=0.6844\n",
      "Epoch 027 | train_loss=1.0203 | val_loss=1.0371 | val_acc=0.6853\n",
      "Epoch 028 | train_loss=1.0147 | val_loss=1.0322 | val_acc=0.6858\n",
      "Epoch 029 | train_loss=1.0107 | val_loss=1.0326 | val_acc=0.6866\n",
      "Epoch 030 | train_loss=1.0041 | val_loss=1.0281 | val_acc=0.6876\n",
      "Epoch 031 | train_loss=1.0009 | val_loss=1.0274 | val_acc=0.6865\n",
      "Epoch 032 | train_loss=0.9954 | val_loss=1.0222 | val_acc=0.6886\n",
      "Epoch 033 | train_loss=0.9921 | val_loss=1.0214 | val_acc=0.6903\n",
      "Epoch 034 | train_loss=0.9871 | val_loss=1.0211 | val_acc=0.6905\n",
      "Epoch 035 | train_loss=0.9835 | val_loss=1.0210 | val_acc=0.6899\n",
      "Epoch 036 | train_loss=0.9799 | val_loss=1.0142 | val_acc=0.6916\n",
      "Epoch 037 | train_loss=0.9743 | val_loss=1.0164 | val_acc=0.6920\n",
      "Epoch 038 | train_loss=0.9716 | val_loss=1.0143 | val_acc=0.6924\n",
      "Epoch 039 | train_loss=0.9540 | val_loss=1.0055 | val_acc=0.6949\n",
      "Epoch 040 | train_loss=0.9495 | val_loss=1.0038 | val_acc=0.6956\n",
      "Epoch 041 | train_loss=0.9458 | val_loss=1.0029 | val_acc=0.6948\n",
      "Epoch 042 | train_loss=0.9456 | val_loss=1.0031 | val_acc=0.6957\n",
      "Epoch 043 | train_loss=0.9407 | val_loss=1.0035 | val_acc=0.6950\n",
      "Epoch 044 | train_loss=0.9332 | val_loss=1.0000 | val_acc=0.6960\n",
      "Epoch 045 | train_loss=0.9309 | val_loss=0.9996 | val_acc=0.6973\n",
      "Epoch 046 | train_loss=0.9287 | val_loss=1.0001 | val_acc=0.6973\n",
      "Epoch 047 | train_loss=0.9278 | val_loss=0.9994 | val_acc=0.6967\n",
      "Epoch 048 | train_loss=0.9260 | val_loss=1.0003 | val_acc=0.6971\n",
      "Epoch 049 | train_loss=0.9265 | val_loss=0.9991 | val_acc=0.6971\n",
      "Epoch 050 | train_loss=0.9232 | val_loss=1.0002 | val_acc=0.6962\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/coran/coran_lstm_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0afe795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and indeed we to allah he will surely be of the obliging therein and you have done thereof but fear the wrongdoing people\n",
      "\n",
      " Verse 2:\n",
      "the women side and remember the most compensation how many a party of them they never come and we saved you who does not know the home of your lord but those who disbelieve should be in the promise and thus have care are the in pertain that you meat one who denied the hearts of need of you a wrong t\n",
      "\n",
      " Verse 3:\n",
      "allah this worldly life and it is allah who will be among the hearts are disbelievers\n",
      "\n",
      " Verse 4:\n",
      "say if only may give them from his signs and we will say with they say o my people will him is a servant in the hereafter has come to you one who disbelieved as the light not but as a messenger and the prostration within the hereafter and the day he will be reconcerned and he associate anything afte\n",
      "\n",
      " Verse 5:\n",
      "and they signs before you so allah do indeed they were in life a great corruption\n",
      "\n",
      " Verse 6:\n",
      "and we were to whom we not conceal the decree of allah it would allah has have properted to righteousness and do not i punish them indeed allah is knowing of all things in provision for whom we marden for them we will surely my wamen pure corrigtance the one whom you have already of the abraham inde\n",
      "\n",
      " Verse 7:\n",
      "whoever you give him a truth have seared him of them while you carry one who invents it do not disbelieve in the cause of allah as forgiveness of allah and they will say he is to the land and we revealed to any of them who say indeed we can have troy for what they believe with now or with a clear an\n",
      "\n",
      " Verse 8:\n",
      "indeed in that are that you invoke for it while they believe in the corrupters\n",
      "\n",
      " Verse 9:\n",
      "and it affair are warners and say this is the tray endiruld our verses and we have sent down from them but they do not arn toding therein and the nore as their creation of the earth will be given and whom he wills and allah is ever of them it you were upon them and whoever does any harm for one for \n",
      "\n",
      " Verse 10:\n",
      "o you who have believed do not touch you but their instrayed as a sign so have the daid indeed allah is fire\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c641da9",
   "metadata": {},
   "source": [
    "## Dataset Hadith-s (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fd42b",
   "metadata": {},
   "source": [
    "Visualizamos el la estructura del df, cogeremos las columnas (hadith-s) que nos interesan: `text_ar` y `text_en`. Como el archivo viene estructurado de una manera poco usual, realizaremos una limpieza exhaustiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3451d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hadith_id</th>\n",
       "      <th>source</th>\n",
       "      <th>chapter_no</th>\n",
       "      <th>hadith_no</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chain_indx</th>\n",
       "      <th>text_ar</th>\n",
       "      <th>text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30418, 20005, 11062, 11213, 11042, 3</td>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...</td>\n",
       "      <td>Narrated 'Umar bin Al-Khattab:          ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hadith_id           source  chapter_no hadith_no  \\\n",
       "0   0          1   Sahih Bukhari            1        1    \n",
       "\n",
       "                       chapter                            chain_indx  \\\n",
       "0  Revelation - كتاب بدء الوحى  30418, 20005, 11062, 11213, 11042, 3   \n",
       "\n",
       "                                             text_ar  \\\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...   \n",
       "\n",
       "                                             text_en  \n",
       "0        Narrated 'Umar bin Al-Khattab:          ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_df = pd.read_csv(\"../data/hadith_dataset/all_hadiths_clean.csv\")\n",
    "hadith_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b0fc5",
   "metadata": {},
   "source": [
    "Árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c5b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ar    34433\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_ar = hadith_df[\"text_ar\"]\n",
    "hadith_ar = pd.DataFrame(hadith_ar).dropna()\n",
    "hadith_ar.to_csv(\"../data/hadith_dataset/hadith_ar/hadith_ar.csv\", index=False, encoding=\"utf-8\")\n",
    "print(hadith_ar.count())\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "hadith_ar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b0149",
   "metadata": {},
   "source": [
    "Limpieza árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3b08968",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "# Diacríticos árabes (harakat) + marcas coránicas comunes\n",
    "ARABIC_DIACRITICS = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "# Rangos Unicode típicos para árabe (básico + extendidos)\n",
    "ARABIC_RANGES = r\"\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def normalize_arabic(text: str, remove_diacritics: bool = True) -> str:\n",
    "    # Normalización Unicode (unifica formas)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Quitar tatweel (kashida)\n",
    "    text = text.replace(\"\\u0640\", \"\")\n",
    "\n",
    "    # Unificar algunas variantes comunes (opcional, útil en muchos corpus)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\")\n",
    "    text = text.replace(\"ة\", \"ه\")  # si prefieres mantenerla, comenta esta línea\n",
    "\n",
    "    if remove_diacritics:\n",
    "        text = re.sub(ARABIC_DIACRITICS, \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_hadith_text_ar(text, remove_diacritics: bool = True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Espacios/saltos de línea\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "    # 2) Quitar comillas envolventes\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    # 3) Normalización árabe (sin lower)\n",
    "    text = normalize_arabic(text, remove_diacritics=remove_diacritics)\n",
    "\n",
    "    # 4) Eliminar narrador (si el encabezado está en inglés, como en tu caso)\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, \"\", text).strip()\n",
    "\n",
    "    # 5) Quitar comillas residuales\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', \"\", text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', \"\", text)\n",
    "\n",
    "    # 6) Mantener: letras árabes, números, espacios y puntuación básica.\n",
    "    # Incluye puntuación árabe: ، ؛ ؟  (comma/semicolon/question mark)\n",
    "    allowed = rf\"[^0-9\\s{ARABIC_RANGES}\\.,!?'\\-\\(\\)«»\\\"،؛؟]\"\n",
    "    text = re.sub(allowed, \" \", text)\n",
    "\n",
    "    # 7) Colapsar espacios\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68252da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_ar = hadith_df[[\"text_ar\"]].copy()\n",
    "\n",
    "hadith_ar = hadith_ar.dropna(subset=[\"text_ar\"])\n",
    "\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar[\"text_ar\"] = hadith_ar[\"text_ar\"].apply(clean_hadith_text_ar)\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar = hadith_ar[hadith_ar[\"text_ar\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\"\n",
    "\n",
    "hadith_ar.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575a5f5",
   "metadata": {},
   "source": [
    "Inglés + función de limpieza inglesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "089ced52",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        # ^\\s*[\"'“”...]+ (captura comillas al inicio) y [\"'“”...]+\\s*$ (al final)\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def clean_hadith_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    text = text.replace('\"\"', '\"').lower()\n",
    "\n",
    "    # Limpieza del formato original del .csv: narrated by (nommbre del narrador) + texto que queremos\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, '', text).strip()\n",
    "\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\-\\(\\)]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f8ffd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_en = hadith_df[[\"text_en\"]].copy()\n",
    "\n",
    "hadith_en = hadith_en.dropna(subset=[\"text_en\"])\n",
    "\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en[\"text_en\"] = hadith_en[\"text_en\"].apply(clean_hadith_text)\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en = hadith_en[hadith_en[\"text_en\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\"\n",
    "\n",
    "hadith_en.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa20bd5",
   "metadata": {},
   "source": [
    "Clase Dataset del Hadith dataset, adaptado al formato del dataset de *Kaggle*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc92ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col):\n",
    "        # text_col: text_en (hadith_en) y text_ar (hadith_ar)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "        self._max_seq_length = min(int(self.df[text_col].astype(str).map(len).max()) + 2, 500)        \n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, hadith_csv, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, hadith_csv, vectorizer_filepath, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long), \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bba93",
   "metadata": {},
   "source": [
    "### RNN - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a413054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(hadith_path, output_path, text_col, ft_ruta):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # 256-ekin peatau itenzatek\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    print(args.batch_size)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ft_ruta)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=None\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b4da3",
   "metadata": {},
   "source": [
    "Entrenamiento RNN con dataset Hadith en árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e07996b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/3905058435.py:74: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca31b3ff287447f9e95a75d3e944371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.4272 | val_loss=1.9430 | val_acc=0.4766\n",
      "Epoch 002 | train_loss=1.9415 | val_loss=1.7300 | val_acc=0.5292\n",
      "Epoch 003 | train_loss=1.8107 | val_loss=1.6277 | val_acc=0.5493\n",
      "Epoch 004 | train_loss=1.7418 | val_loss=1.5645 | val_acc=0.5646\n",
      "Epoch 005 | train_loss=1.6981 | val_loss=1.5274 | val_acc=0.5728\n",
      "Epoch 006 | train_loss=1.6696 | val_loss=1.5026 | val_acc=0.5824\n",
      "Epoch 007 | train_loss=1.6496 | val_loss=1.4840 | val_acc=0.5888\n",
      "Epoch 008 | train_loss=1.6339 | val_loss=1.4710 | val_acc=0.5917\n",
      "Epoch 009 | train_loss=1.6217 | val_loss=1.4605 | val_acc=0.5948\n",
      "Epoch 010 | train_loss=1.6108 | val_loss=1.4547 | val_acc=0.6012\n",
      "Epoch 011 | train_loss=1.6021 | val_loss=1.4449 | val_acc=0.6002\n",
      "Epoch 012 | train_loss=1.5952 | val_loss=1.4351 | val_acc=0.6051\n",
      "Epoch 013 | train_loss=1.5879 | val_loss=1.4306 | val_acc=0.6057\n",
      "Epoch 014 | train_loss=1.5830 | val_loss=1.4259 | val_acc=0.6075\n",
      "Epoch 015 | train_loss=1.5773 | val_loss=1.4225 | val_acc=0.6068\n",
      "Epoch 016 | train_loss=1.5725 | val_loss=1.4183 | val_acc=0.6088\n",
      "Epoch 017 | train_loss=1.5682 | val_loss=1.4144 | val_acc=0.6087\n",
      "Epoch 018 | train_loss=1.5645 | val_loss=1.4126 | val_acc=0.6082\n",
      "Epoch 019 | train_loss=1.5606 | val_loss=1.4065 | val_acc=0.6112\n",
      "Epoch 020 | train_loss=1.5573 | val_loss=1.4037 | val_acc=0.6125\n",
      "Epoch 021 | train_loss=1.5539 | val_loss=1.4023 | val_acc=0.6125\n",
      "Epoch 022 | train_loss=1.5505 | val_loss=1.3995 | val_acc=0.6124\n",
      "Epoch 023 | train_loss=1.5483 | val_loss=1.4004 | val_acc=0.6106\n",
      "Epoch 024 | train_loss=1.5462 | val_loss=1.3956 | val_acc=0.6136\n",
      "Epoch 025 | train_loss=1.5437 | val_loss=1.3934 | val_acc=0.6137\n",
      "Epoch 026 | train_loss=1.5414 | val_loss=1.3915 | val_acc=0.6155\n",
      "Epoch 027 | train_loss=1.5390 | val_loss=1.3891 | val_acc=0.6149\n",
      "Epoch 028 | train_loss=1.5368 | val_loss=1.3872 | val_acc=0.6158\n",
      "Epoch 029 | train_loss=1.5356 | val_loss=1.3868 | val_acc=0.6151\n",
      "Epoch 030 | train_loss=1.5333 | val_loss=1.3842 | val_acc=0.6166\n",
      "Epoch 031 | train_loss=1.5315 | val_loss=1.3811 | val_acc=0.6159\n",
      "Epoch 032 | train_loss=1.5297 | val_loss=1.3812 | val_acc=0.6150\n",
      "Epoch 033 | train_loss=1.5285 | val_loss=1.3802 | val_acc=0.6163\n",
      "Epoch 034 | train_loss=1.5266 | val_loss=1.3800 | val_acc=0.6173\n",
      "Epoch 035 | train_loss=1.5252 | val_loss=1.3760 | val_acc=0.6168\n",
      "Epoch 036 | train_loss=1.5241 | val_loss=1.3773 | val_acc=0.6158\n",
      "Epoch 037 | train_loss=1.5226 | val_loss=1.3753 | val_acc=0.6182\n",
      "Epoch 038 | train_loss=1.5210 | val_loss=1.3739 | val_acc=0.6177\n",
      "Epoch 039 | train_loss=1.5198 | val_loss=1.3726 | val_acc=0.6182\n",
      "Epoch 040 | train_loss=1.5189 | val_loss=1.3722 | val_acc=0.6172\n",
      "Epoch 041 | train_loss=1.5175 | val_loss=1.3705 | val_acc=0.6195\n",
      "Epoch 042 | train_loss=1.5171 | val_loss=1.3733 | val_acc=0.6151\n",
      "Epoch 043 | train_loss=1.5159 | val_loss=1.3710 | val_acc=0.6160\n",
      "Epoch 044 | train_loss=1.5130 | val_loss=1.3690 | val_acc=0.6187\n",
      "Epoch 045 | train_loss=1.5126 | val_loss=1.3683 | val_acc=0.6192\n",
      "Epoch 046 | train_loss=1.5121 | val_loss=1.3674 | val_acc=0.6166\n",
      "Epoch 047 | train_loss=1.5116 | val_loss=1.3669 | val_acc=0.6200\n",
      "Epoch 048 | train_loss=1.5107 | val_loss=1.3672 | val_acc=0.6199\n",
      "Epoch 049 | train_loss=1.5104 | val_loss=1.3680 | val_acc=0.6195\n",
      "Epoch 050 | train_loss=1.5087 | val_loss=1.3654 | val_acc=0.6173\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_ar\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f40cda4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثناه العياوب وحبنا عنرادين بن يحيي بن حبير بن عائشه، ان رسول الله صلي الله عليه وسلم قتله، فقال له عن يليمه، انه سود اغيره الالحا فقد النيل فلاالك رحيت او يقتل الليت الله ومن امعاما قال حفيده عن ابن عبيد الله الجابي ثمره بيد ما لا يسيك بن عصر بالسمع فخرف عن ابي هذا هذي الملاج بهل يشاكل بيله ولس حم\n",
      "\n",
      " Verse 2:\n",
      "حدثنا ابو العبا بن حدثني، تحار واما منا ويلم ابن صالع اله خفيان في الناعمه وسمع بيوه فقال قال عنيي كما يقرا يم ما انضر بعضاه في وابا علي ابن شعبه من ابي اسله مر اللكه .\n",
      "\n",
      " Verse 3:\n",
      "حدثني ابو التمول الولا والملرص فيه في سلمه علي السهاء يا رسول الله لا اسمذي ان اليصيت عن رسول الله صلي الله عليه وسلم فانتا واولي ابن عمر وجدي البجراع شهس الاسلعم \" .\n",
      "\n",
      " Verse 4:\n",
      "حدثنا محمد بن ابي مالي، عن سعيد بن ابي علي، قال قال رسول الله صلي الله علي الله والوخرا والعلي بن النبي صلي الله عليه وسلم \" انه تعتلها علي الاساق ورسول الله صلي الله عليه وسلم يحدث ان يسطر ينك بالله في الصلاه الهما ان رسول الله صلي الله عليه وسلم قال \" لي كوا فما انا جعلم الهارد وما كلنا النبي صلي \n",
      "\n",
      " Verse 5:\n",
      "حدثنا ابن المناجر، عن مكاب، عن عبد الله بن عمر، عن قلت الن ومع الخوان قال ابن يحيي النبي صلي الله عليه وسلم وقال كان رسول الله صلي الله عليه وسلم قال \" من سعراه ابر الوبا فقال فجهل الي الجيبه بانه ان النبي صلي الله عليه وسلم قال \" من الاصلي وبالاما والكتين من الجمه القداه الا ان ارسم الا رسول الله ص\n",
      "\n",
      " Verse 6:\n",
      "حدثنا يوزير، حدثنا ابو بارفي بن ابي قال، عن ابي هريره، يقول الله فا رجل المسوله فاسلمها في اسلا رياي يحين الذه النمر ولا النسما الدمز فيكلنا كم بانه الجهم في البيو ولا تمال الزالص الاسعنه حزيا يحول النبي صلي الله عليه وسلم \" . من لا اكراب الي حاتب عائشه سمعت سل بيجه رحيي فراله بين مسلم عاله المبير و\n",
      "\n",
      " Verse 7:\n",
      "حدثنا اسماعيل بن الجنف، ان ابن عليه، اعود التيلي ما بقرا \" . قال ابو عو الخبران \" .\n",
      "\n",
      " Verse 8:\n",
      "حدثنا بكر بن ابراهيم، عن ابي صراعد، حدثنا عبد الله بن علي بن عطي، حدثنا ارضي، عن زهير بن اساتين من يساله لما طار الذا ما باسلال الا البوا الالا القال ما امصاب الاالك المجرب ثم العرح ارابه \" . فاشتر شي قال \" الانتن لواني والتكره بالهاوا وذا . قلم ينعم الاهم ان الي الجه عليه الاو المنكر الا من كان امن\n",
      "\n",
      " Verse 9:\n",
      "حدثنا يعلمه، حدثنا ابت ينان . فقال عائا القران، قال الرجل ان عبد الله فان سلمه .\n",
      "\n",
      " Verse 10:\n",
      "حدثنا وفيتان، عن ابي هريره، قال قال طلب الماس رسول الله صلي الله عليه وسلم \" . قال \" من قيل الا رسول الله صلي الله عليه وسلم يهي باصلا ولفه علون حاس الصلاه الصابه فالازاه \" .\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e013a8",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith RNN inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bcf8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/3905058435.py:74: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9a8785cf6a40c48f6d9ff9725a6dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.4778 | val_loss=2.0084 | val_acc=0.4385\n",
      "Epoch 002 | train_loss=2.0395 | val_loss=1.8146 | val_acc=0.4940\n",
      "Epoch 003 | train_loss=1.9119 | val_loss=1.7096 | val_acc=0.5117\n",
      "Epoch 004 | train_loss=1.8377 | val_loss=1.6460 | val_acc=0.5303\n",
      "Epoch 005 | train_loss=1.7901 | val_loss=1.6046 | val_acc=0.5391\n",
      "Epoch 006 | train_loss=1.7570 | val_loss=1.5759 | val_acc=0.5439\n",
      "Epoch 007 | train_loss=1.7324 | val_loss=1.5540 | val_acc=0.5498\n",
      "Epoch 008 | train_loss=1.7141 | val_loss=1.5397 | val_acc=0.5537\n",
      "Epoch 009 | train_loss=1.6990 | val_loss=1.5253 | val_acc=0.5564\n",
      "Epoch 010 | train_loss=1.6866 | val_loss=1.5149 | val_acc=0.5592\n",
      "Epoch 011 | train_loss=1.6768 | val_loss=1.5058 | val_acc=0.5617\n",
      "Epoch 012 | train_loss=1.6682 | val_loss=1.4992 | val_acc=0.5633\n",
      "Epoch 013 | train_loss=1.6607 | val_loss=1.4932 | val_acc=0.5675\n",
      "Epoch 014 | train_loss=1.6544 | val_loss=1.4858 | val_acc=0.5699\n",
      "Epoch 015 | train_loss=1.6488 | val_loss=1.4833 | val_acc=0.5681\n",
      "Epoch 016 | train_loss=1.6440 | val_loss=1.4793 | val_acc=0.5721\n",
      "Epoch 017 | train_loss=1.6399 | val_loss=1.4740 | val_acc=0.5734\n",
      "Epoch 018 | train_loss=1.6354 | val_loss=1.4697 | val_acc=0.5741\n",
      "Epoch 019 | train_loss=1.6311 | val_loss=1.4678 | val_acc=0.5730\n",
      "Epoch 020 | train_loss=1.6281 | val_loss=1.4648 | val_acc=0.5762\n",
      "Epoch 021 | train_loss=1.6254 | val_loss=1.4635 | val_acc=0.5771\n",
      "Epoch 022 | train_loss=1.6229 | val_loss=1.4593 | val_acc=0.5756\n",
      "Epoch 023 | train_loss=1.6198 | val_loss=1.4542 | val_acc=0.5823\n",
      "Epoch 024 | train_loss=1.6171 | val_loss=1.4558 | val_acc=0.5773\n",
      "Epoch 025 | train_loss=1.6145 | val_loss=1.4517 | val_acc=0.5797\n",
      "Epoch 026 | train_loss=1.6126 | val_loss=1.4491 | val_acc=0.5809\n",
      "Epoch 027 | train_loss=1.6103 | val_loss=1.4469 | val_acc=0.5793\n",
      "Epoch 028 | train_loss=1.6087 | val_loss=1.4467 | val_acc=0.5819\n",
      "Epoch 029 | train_loss=1.6069 | val_loss=1.4445 | val_acc=0.5842\n",
      "Epoch 030 | train_loss=1.6051 | val_loss=1.4431 | val_acc=0.5830\n",
      "Epoch 031 | train_loss=1.6037 | val_loss=1.4422 | val_acc=0.5836\n",
      "Epoch 032 | train_loss=1.6020 | val_loss=1.4412 | val_acc=0.5836\n",
      "Epoch 033 | train_loss=1.6007 | val_loss=1.4434 | val_acc=0.5804\n",
      "Epoch 034 | train_loss=1.5994 | val_loss=1.4399 | val_acc=0.5812\n",
      "Epoch 035 | train_loss=1.5982 | val_loss=1.4409 | val_acc=0.5841\n",
      "Epoch 036 | train_loss=1.5968 | val_loss=1.4395 | val_acc=0.5841\n",
      "Epoch 037 | train_loss=1.5954 | val_loss=1.4362 | val_acc=0.5865\n",
      "Epoch 038 | train_loss=1.5944 | val_loss=1.4351 | val_acc=0.5819\n",
      "Epoch 039 | train_loss=1.5939 | val_loss=1.4361 | val_acc=0.5854\n",
      "Epoch 040 | train_loss=1.5926 | val_loss=1.4322 | val_acc=0.5860\n",
      "Epoch 041 | train_loss=1.5910 | val_loss=1.4339 | val_acc=0.5822\n",
      "Epoch 042 | train_loss=1.5906 | val_loss=1.4336 | val_acc=0.5828\n",
      "Epoch 043 | train_loss=1.5877 | val_loss=1.4300 | val_acc=0.5843\n",
      "Epoch 044 | train_loss=1.5863 | val_loss=1.4288 | val_acc=0.5845\n",
      "Epoch 045 | train_loss=1.5859 | val_loss=1.4293 | val_acc=0.5840\n",
      "Epoch 046 | train_loss=1.5859 | val_loss=1.4288 | val_acc=0.5868\n",
      "Epoch 047 | train_loss=1.5843 | val_loss=1.4280 | val_acc=0.5845\n",
      "Epoch 048 | train_loss=1.5836 | val_loss=1.4279 | val_acc=0.5869\n",
      "Epoch 049 | train_loss=1.5838 | val_loss=1.4273 | val_acc=0.5872\n",
      "Epoch 050 | train_loss=1.5828 | val_loss=1.4268 | val_acc=0.5846\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_en\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "747f38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "igh in the anoungilen to the bra' gour bin ald before his wame not nor mand to remater and in his hands wan for his camen came of all hammed buts properte whan with the messenger of upo thes sar and the sun) the prophet who said and and the prophet ( ) said when he who was would narration. him the p\n",
      "\n",
      " Verse 2:\n",
      "in while it and canep it in the promertidia's for that antwe of make the for it and and then are the ritting the messerch that when i with entrong to we more leasted and they thel would, who we the butren in the prophet ( ) upon him) har the som and abna le taman abdullah be for hand of ook.\n",
      "\n",
      " Verse 3:\n",
      "said and is with them to said a man from them and he rashed of in perood as allah be parmodeven the perpoat in the last in the sow unon the sayevers ham and she serno sacepll that he wat and the the are that had allah if he of a torning has dightrea the prophet houbadass with the apostle sood he not\n",
      "\n",
      " Verse 4:\n",
      "the runt of his allahs of ibn quray upon thes) and there for go man has beed and when the peace be upon him) with the was tinred 'it aidan) would fight his grounged the prophet and bit he were allah be of a dadeening where the menst it to the reson, the propeed and he will come and have been narrate\n",
      "\n",
      " Verse 5:\n",
      "abu hainas b. 'abrara he mati's in allah and the prophet propertl with she apostle of allah malt tham nisill, and i while the abonged the hased that them) and then heme and the pid and any ighing the pristsee of mance se cherted and in the hadith with her his did laak to the houle saig the and, me n\n",
      "\n",
      " Verse 6:\n",
      "abu haar land of the ero allah's messented wien of reported allah's messenger (may peace be oin is be ave on allah's is al- al-samun his fast to the mes of allah 'fathather mare the ladith hain and said, equed of the prophet and he who said it mu.\n",
      "\n",
      " Verse 7:\n",
      "abu resima wall if the prophet ( ) prayer, and he have  aid with sthes said and his ofled the in herniying their water and the messenger of allah ( ) sook and in me then wh of ons for salam. theerreghal a deder fol awat his have bece day and im as say so the apostle of allah (may messenger of allah,\n",
      "\n",
      " Verse 8:\n",
      "the prophet ( ) gand salit abo he expopt the boingis in the messely of allah who were was a veritition and beich aspring even whotlains longing with hth you wive the ter when the minst was has the same was allemina may almay he and said the prophes with twe the out the dion and said is the did to di\n",
      "\n",
      " Verse 9:\n",
      "said bring me as ied you sho he percepran and then allah said and meangh but the prophet (. allama for were a man but and in cophem will nos your prophet ( he perform the gorty to the callses in this hearn (the yours upon ho meanging a ofon the messenger of allah (may peace be upto thee so qut hurim\n",
      "\n",
      " Verse 10:\n",
      "in the hand talleint of the istaymat, and the most of the premeg upen to abu daid bin there and his foon the compingers and the same for ands by allhe beess ti then a salla till if a prope homat and i word (the people one of it allah would slellee in, for he with him, and by the hadith and on pleate\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5818ab3",
   "metadata": {},
   "source": [
    "### LSTM - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "574bf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(hadith_path, output_path, text_col, ft_ruta):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_modelo = fasttext.load_model(ft_ruta)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_modelo)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=None\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21033cc",
   "metadata": {},
   "source": [
    "Entrenamiento LSTM Hadith árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbe6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/4188364936.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3670c570e814847a3c52e991b0a26b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=1.7276 | val_loss=1.3263 | val_acc=0.6259\n",
      "Epoch 002 | train_loss=1.3555 | val_loss=1.2133 | val_acc=0.6521\n",
      "Epoch 003 | train_loss=1.2678 | val_loss=1.1617 | val_acc=0.6646\n",
      "Epoch 004 | train_loss=1.2200 | val_loss=1.1318 | val_acc=0.6727\n",
      "Epoch 005 | train_loss=1.1885 | val_loss=1.1050 | val_acc=0.6813\n",
      "Epoch 006 | train_loss=1.1653 | val_loss=1.0935 | val_acc=0.6826\n",
      "Epoch 007 | train_loss=1.1478 | val_loss=1.0778 | val_acc=0.6885\n",
      "Epoch 008 | train_loss=1.1332 | val_loss=1.0643 | val_acc=0.6916\n",
      "Epoch 009 | train_loss=1.1221 | val_loss=1.0554 | val_acc=0.6941\n",
      "Epoch 010 | train_loss=1.1112 | val_loss=1.0470 | val_acc=0.6966\n",
      "Epoch 011 | train_loss=1.1030 | val_loss=1.0426 | val_acc=0.6972\n",
      "Epoch 012 | train_loss=1.0953 | val_loss=1.0392 | val_acc=0.6986\n",
      "Epoch 013 | train_loss=1.0887 | val_loss=1.0299 | val_acc=0.7010\n",
      "Epoch 014 | train_loss=1.0828 | val_loss=1.0252 | val_acc=0.7020\n",
      "Epoch 015 | train_loss=1.0779 | val_loss=1.0237 | val_acc=0.7022\n",
      "Epoch 016 | train_loss=1.0731 | val_loss=1.0205 | val_acc=0.7005\n",
      "Epoch 017 | train_loss=1.0687 | val_loss=1.0160 | val_acc=0.7047\n",
      "Epoch 018 | train_loss=1.0649 | val_loss=1.0141 | val_acc=0.7049\n",
      "Epoch 019 | train_loss=1.0610 | val_loss=1.0097 | val_acc=0.7059\n",
      "Epoch 020 | train_loss=1.0578 | val_loss=1.0059 | val_acc=0.7071\n",
      "Epoch 021 | train_loss=1.0554 | val_loss=1.0074 | val_acc=0.7058\n",
      "Epoch 022 | train_loss=1.0517 | val_loss=1.0064 | val_acc=0.7063\n",
      "Epoch 023 | train_loss=1.0412 | val_loss=0.9948 | val_acc=0.7101\n",
      "Epoch 024 | train_loss=1.0388 | val_loss=0.9944 | val_acc=0.7099\n",
      "Epoch 025 | train_loss=1.0372 | val_loss=0.9921 | val_acc=0.7110\n",
      "Epoch 026 | train_loss=1.0361 | val_loss=0.9936 | val_acc=0.7101\n",
      "Epoch 027 | train_loss=1.0346 | val_loss=0.9934 | val_acc=0.7094\n",
      "Epoch 028 | train_loss=1.0284 | val_loss=0.9868 | val_acc=0.7127\n",
      "Epoch 029 | train_loss=1.0274 | val_loss=0.9867 | val_acc=0.7124\n",
      "Epoch 030 | train_loss=1.0265 | val_loss=0.9869 | val_acc=0.7132\n",
      "Epoch 031 | train_loss=1.0236 | val_loss=0.9850 | val_acc=0.7129\n",
      "Epoch 032 | train_loss=1.0230 | val_loss=0.9841 | val_acc=0.7130\n",
      "Epoch 033 | train_loss=1.0224 | val_loss=0.9849 | val_acc=0.7123\n",
      "Epoch 034 | train_loss=1.0220 | val_loss=0.9855 | val_acc=0.7123\n",
      "Epoch 035 | train_loss=1.0202 | val_loss=0.9838 | val_acc=0.7130\n",
      "Epoch 036 | train_loss=1.0198 | val_loss=0.9834 | val_acc=0.7131\n",
      "Epoch 037 | train_loss=1.0193 | val_loss=0.9840 | val_acc=0.7127\n",
      "Epoch 038 | train_loss=1.0192 | val_loss=0.9830 | val_acc=0.7132\n",
      "Epoch 039 | train_loss=1.0191 | val_loss=0.9827 | val_acc=0.7132\n",
      "Epoch 040 | train_loss=1.0188 | val_loss=0.9835 | val_acc=0.7126\n",
      "Epoch 041 | train_loss=1.0190 | val_loss=0.9834 | val_acc=0.7129\n",
      "Epoch 042 | train_loss=1.0182 | val_loss=0.9825 | val_acc=0.7130\n",
      "Epoch 043 | train_loss=1.0177 | val_loss=0.9821 | val_acc=0.7134\n",
      "Epoch 044 | train_loss=1.0178 | val_loss=0.9821 | val_acc=0.7135\n",
      "Epoch 045 | train_loss=1.0176 | val_loss=0.9823 | val_acc=0.7132\n",
      "Epoch 046 | train_loss=1.0168 | val_loss=0.9820 | val_acc=0.7132\n",
      "Epoch 047 | train_loss=1.0169 | val_loss=0.9822 | val_acc=0.7132\n",
      "Epoch 048 | train_loss=1.0169 | val_loss=0.9820 | val_acc=0.7133\n",
      "Epoch 049 | train_loss=1.0166 | val_loss=0.9818 | val_acc=0.7134\n",
      "Epoch 050 | train_loss=1.0166 | val_loss=0.9821 | val_acc=0.7132\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_ar\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2ecc683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثنا محمد بن بشار، حدثنا يحيي، - يعني ابن زيد - حدثنا معمر، عن الزهري، عن سالم، عن ابن عمر، ان رسول الله صلي الله عليه وسلم قال \" اذا اصبح ان يسترض بلته فقد صاع الناس لعله وتقام الكلب القالي \" .\n",
      "\n",
      " Verse 2:\n",
      "حدثنا ابو نعيم، حدثنا ابن ابي مريم، حدثنا عمرو بن الحارث، عن ابي الزبير، عن جابر، قال قال رسول الله صلي الله عليه وسلم \" باسل جعله عصره بني السمع الا الاعلي والخشيه الي الصبي وابو بهم من من اكثره بالفرق له \" . قال يحيي في قوله والله لا اغتسل والا امرت الا المهدي لربع اني الزناء او ان يستحبها اللقيه \n",
      "\n",
      " Verse 3:\n",
      "حدثنا اسماعيل، قال حدثني مالك، عن ابي الزبير، عن ابي الاعمش، عن ابي هريره، قال قال رسول الله صلي الله عليه وسلم \" لا يلبس المشركين . قال فاعيني ان يسال عن امر الخير واخذها في عرده ابو حمزه الخفين \" . قال مالك وهي التراويه . فقال لي رسول الله صلي الله عليه وسلم \" ان شما وان يعقد الاباري العبد المحاخي\n",
      "\n",
      " Verse 4:\n",
      "حدثنا سليمان بن حرب، حدثنا شعبه، عن يزيد بن ابي ابي، عبد الله عن عائشه رضي الله عنها انها قالت انبات الشمس في الاعام كلامته . فقال ان الخطب اقران، فالذي نزلت رجلا الا في سعفوه .\n",
      "\n",
      " Verse 5:\n",
      "حدثنا اسماعيل، قال حدثنا محمد بن جعفر الزاربي، ان ابا حميد، - وهو عند ابن ابي سلمه عن عقبه بن عبد الرحمن يقول هذا الحديث \" من عادوا موسي راسه \" . قال فان السماء بن المدامه مقتسي من اليكون يا ابن عبد الرحمن ان اقدم لهما فانها يانا الصلاه فانه يخطب لها بعض الصلاه، بالحق ثلث منكم منها مع ملائكه وشعبهم \n",
      "\n",
      " Verse 6:\n",
      "حدثنا ابو الرباعه، حدثنا ابو اسحاق، عن الاعمش، عن شداد، - يعني الطاهيام - ان ابا سلمه بن عبد الرحمن، ان رسول الله صلي الله عليه وسلم قال \" من قرا ركعتين . قال وهو اخبر من النار ورايت منه بعير لقد اريد التي \" .\n",
      "\n",
      " Verse 7:\n",
      "حدثنا ابو القلاب، حدثنا شعبه، قال سمعت ابراهيم، عن ابيه، عن عبد الله بن عمرو، قال انا بالدعم، وكنت تنتمي يوم الثوب الا ان يقول بلله فقال \" انه ابطا ابو بكر ونافر في راسه \" .\n",
      "\n",
      " Verse 8:\n",
      "حدثنا ابو غرين، حدثنا زهير بن حرب، حدثنا سفيان، عن هشام، عن عاصم، عن محمد، عن ابي هريره، ان رسول الله صلي الله عليه وسلم قال \" ان العالث \" . قال وفي الباب عن جابر بن ابي الفضل البصري مثله .\n",
      "\n",
      " Verse 9:\n",
      "حدثنا عبد الله بن مسلمه، عن مالك، عن ابي الزناد، عن الاعرج، عن ابي هريره، قال قال رسول الله صلي الله عليه وسلم \" ان عليك الله منه قال ابن رجل من كل من الامه \" . قال ابو عيسي هذا حديث حسن صحيح . وقد روي هذا الحديث بهذا الاسناد مثله .\n",
      "\n",
      " Verse 10:\n",
      "حدثنا احمد بن يونس، قال حدثنا محمد بن فضيل، عن ابي حازم، عن ابي جعفر، عن سعيد بن المسيب، عن ابي هريره، قال قال رسول الله صلي الله عليه وسلم \" لا تيوا الامر من الصلاه \" .\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a805e4",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith LSTM inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d39017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_8046/4188364936.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1f43d43fb7422e8f5ff600ac33f00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=1.8283 | val_loss=1.3833 | val_acc=0.5927\n",
      "Epoch 002 | train_loss=1.3998 | val_loss=1.2415 | val_acc=0.6284\n",
      "Epoch 003 | train_loss=1.2932 | val_loss=1.1730 | val_acc=0.6460\n",
      "Epoch 004 | train_loss=1.2379 | val_loss=1.1359 | val_acc=0.6558\n",
      "Epoch 005 | train_loss=1.2025 | val_loss=1.1139 | val_acc=0.6595\n",
      "Epoch 006 | train_loss=1.1758 | val_loss=1.0901 | val_acc=0.6675\n",
      "Epoch 007 | train_loss=1.1563 | val_loss=1.0759 | val_acc=0.6709\n",
      "Epoch 008 | train_loss=1.1409 | val_loss=1.0704 | val_acc=0.6722\n",
      "Epoch 009 | train_loss=1.1278 | val_loss=1.0549 | val_acc=0.6766\n",
      "Epoch 010 | train_loss=1.1175 | val_loss=1.0462 | val_acc=0.6812\n",
      "Epoch 011 | train_loss=1.1080 | val_loss=1.0432 | val_acc=0.6793\n",
      "Epoch 012 | train_loss=1.1005 | val_loss=1.0356 | val_acc=0.6832\n",
      "Epoch 013 | train_loss=1.0935 | val_loss=1.0314 | val_acc=0.6838\n",
      "Epoch 014 | train_loss=1.0875 | val_loss=1.0249 | val_acc=0.6859\n",
      "Epoch 015 | train_loss=1.0823 | val_loss=1.0229 | val_acc=0.6867\n",
      "Epoch 016 | train_loss=1.0774 | val_loss=1.0196 | val_acc=0.6864\n",
      "Epoch 017 | train_loss=1.0728 | val_loss=1.0136 | val_acc=0.6893\n",
      "Epoch 018 | train_loss=1.0690 | val_loss=1.0108 | val_acc=0.6919\n",
      "Epoch 019 | train_loss=1.0652 | val_loss=1.0081 | val_acc=0.6917\n",
      "Epoch 020 | train_loss=1.0618 | val_loss=1.0098 | val_acc=0.6932\n",
      "Epoch 021 | train_loss=1.0585 | val_loss=1.0034 | val_acc=0.6932\n",
      "Epoch 022 | train_loss=1.0555 | val_loss=1.0049 | val_acc=0.6897\n",
      "Epoch 023 | train_loss=1.0528 | val_loss=1.0028 | val_acc=0.6915\n",
      "Epoch 024 | train_loss=1.0503 | val_loss=1.0001 | val_acc=0.6918\n",
      "Epoch 025 | train_loss=1.0483 | val_loss=1.0023 | val_acc=0.6943\n",
      "Epoch 026 | train_loss=1.0460 | val_loss=0.9989 | val_acc=0.6934\n",
      "Epoch 027 | train_loss=1.0442 | val_loss=0.9972 | val_acc=0.6955\n",
      "Epoch 028 | train_loss=1.0419 | val_loss=0.9957 | val_acc=0.6963\n",
      "Epoch 029 | train_loss=1.0398 | val_loss=0.9978 | val_acc=0.6924\n",
      "Epoch 030 | train_loss=1.0382 | val_loss=0.9924 | val_acc=0.6974\n",
      "Epoch 031 | train_loss=1.0367 | val_loss=0.9899 | val_acc=0.6977\n",
      "Epoch 032 | train_loss=1.0350 | val_loss=0.9865 | val_acc=0.6986\n",
      "Epoch 033 | train_loss=1.0332 | val_loss=0.9897 | val_acc=0.6983\n",
      "Epoch 034 | train_loss=1.0322 | val_loss=0.9885 | val_acc=0.6973\n",
      "Epoch 035 | train_loss=1.0213 | val_loss=0.9827 | val_acc=0.7010\n",
      "Epoch 036 | train_loss=1.0196 | val_loss=0.9824 | val_acc=0.6986\n",
      "Epoch 037 | train_loss=1.0184 | val_loss=0.9801 | val_acc=0.6987\n",
      "Epoch 038 | train_loss=1.0174 | val_loss=0.9803 | val_acc=0.7014\n",
      "Epoch 039 | train_loss=1.0165 | val_loss=0.9812 | val_acc=0.6987\n",
      "Epoch 040 | train_loss=1.0107 | val_loss=0.9764 | val_acc=0.6996\n",
      "Epoch 041 | train_loss=1.0100 | val_loss=0.9772 | val_acc=0.6991\n",
      "Epoch 042 | train_loss=1.0091 | val_loss=0.9775 | val_acc=0.6981\n",
      "Epoch 043 | train_loss=1.0064 | val_loss=0.9756 | val_acc=0.7002\n",
      "Epoch 044 | train_loss=1.0054 | val_loss=0.9756 | val_acc=0.7000\n",
      "Epoch 045 | train_loss=1.0053 | val_loss=0.9759 | val_acc=0.7000\n",
      "Epoch 046 | train_loss=1.0038 | val_loss=0.9753 | val_acc=0.7004\n",
      "Epoch 047 | train_loss=1.0034 | val_loss=0.9752 | val_acc=0.7004\n",
      "Epoch 048 | train_loss=1.0034 | val_loss=0.9749 | val_acc=0.7005\n",
      "Epoch 049 | train_loss=1.0032 | val_loss=0.9748 | val_acc=0.7006\n",
      "Epoch 050 | train_loss=1.0029 | val_loss=0.9753 | val_acc=0.7002\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_en\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cf2f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "abu sa'id al-khudri reported that allah's messenger (may peace be upon him) was slaid that the prophet clothes his (umra) and then said, the messenger of allah (may peace be upon him) said he came to allah's messenger (may peace be upon him) as some of the fire shall be in the truth of it is the all\n",
      "\n",
      " Verse 2:\n",
      "the messenger of allah ( ) said indeed i profess more than three and the shoulder of the night of his wife. i were under his latter and he about the side committing water.\n",
      "\n",
      " Verse 3:\n",
      "allah's apostle asked about the sun as a dream in the messenger of allah ( ) on the day of nahr until the day of judgment who blessed 'abdullah said to him who then appeared the camels ((i.e. the people) replied yes, he would say this is a narration stones the most response by a prevented (with his \n",
      "\n",
      " Verse 4:\n",
      "the prophet ( ) said if they used to say indeed will the day of resurrection.\n",
      "\n",
      " Verse 5:\n",
      "abu hurairah said a man came to mu'adh is the second person who asked about the day of al- muzabira is very stends, he said he who was saying the people grant me guided.\n",
      "\n",
      " Verse 6:\n",
      "abu hurairah narrated that the messenger of allah said lispes of intercention while his companions from the people on the day of resurrection together and requested (the other hadishim) he who takes his bed and said, 'wah al-walid granted him.\n",
      "\n",
      " Verse 7:\n",
      "abu auqas al-habika from the messenger of allah (may peace be upon him) to see.\n",
      "\n",
      " Verse 8:\n",
      "allah's apostle forbade abu sa'id i am led.' i said if they will do of him and belongs the luntes with it. he performed materal- and it a ormay prayer a place for i and abu bakr said, whoever pay him for the prayer before the ansar and the possession of the other shade. 'ibn as-samit and was the clo\n",
      "\n",
      " Verse 9:\n",
      "abu huraira reported that allah's messenger (may peace be upon him) said approve believed in the call saying by allah while i saw delay it on the day of salam will trees in the day of najd performed behind.\n",
      "\n",
      " Verse 10:\n",
      "abu huraira reported that the messenger of allah (may peace be upon him) provided allah's apostle while he was in my not medina. then the rest of the hadith is the same, and i was the (constant do not do not yes) and when allah's apostle asked abdur-rahman and umar (i.e. her son) in whose hand which\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29436c2",
   "metadata": {},
   "source": [
    "Realizaremos las comparaciones y aclararemos nuestras conclusiones en la documentación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
