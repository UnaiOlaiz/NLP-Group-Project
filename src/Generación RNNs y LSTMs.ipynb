{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generación de Versos con Modelos Secuenciales\n",
    "En este apartado trabajaremos con modelos secuenciales como *Recurrent Neural Networks* (`RNNs`) y *Long Short Term Memories* (`LSTMs`), modelo que aunque no sean tan potentes como algunos de los modelos generativos que veremos después, hemos considerado útiles de desarrollar para reflejar la evolución real que tuvieron los modelos generativos a lo largo de los años.\n",
    "\n",
    "Debido a que inicialmente consideramos que el número de instancias que ofrecía el dataset que usabamos para manipular el **Corán**, hemos decidido trabajar con un segundo conjunto de datos el cual ofrece casi diez veces el número de instancias entrenables que el primero. [El dataset disponible en Kaggle](https://www.kaggle.com/datasets/fahd09/hadith-dataset), colleciona varios `Hadith`-s, representaciones de acciones o palabras dichas por el **Profeta Mohammed**. No obstante, este archivo presenta una estrutura completamente diferente al habitual, por lo tanto, requerirá de una limpieza y manipulación diferente. \n",
    "\n",
    "A lo largo de este cuaderno, crearemos todas las clases y funciones necesarias para crear y usar los modelos secuenciales generativos precedentes a los **Transformers**, aún así, en la documentación principal profundizaremos más en el análisis completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674c0ca",
   "metadata": {},
   "source": [
    "### Librerías Necearias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fasttext\n",
    "import unicodedata\n",
    "from tqdm import tqdm_notebook\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893a7df",
   "metadata": {},
   "source": [
    "### Código de Clases + Funciones Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221afb",
   "metadata": {},
   "source": [
    "Clase Vocabulary general, donde se definen las funciones principales que heredará la clase del vocabulario específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c545a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        # inicializar atributos\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # función para añadir token (nuevo) al diccionario\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        # función para añadir N > 1 tokens al diccionario\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        # función para obtener el token del idx introducido\n",
    "        if index not in self._idx_to_token:\n",
    "            return \"<UNK>\"\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # devuelve el tamaño del diccionario\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        # devuelve el tamaño del vocabulario\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecf5b",
   "metadata": {},
   "source": [
    "Vocabulary especial Corán con los tokens especiales \\<eos>, \\<bos>, ... Hereda de la clase Vocabulario principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        # añadimos los tokens especiales a nuestro diccionario\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a364e89",
   "metadata": {},
   "source": [
    "Vectorizer - Nuestro vectorizador que será responsable de convertir los labels a vectores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf208cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        # constructor de vocabulario \n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        # función donde vectorizamos texto\n",
    "\n",
    "        indices = [self.char_vocab.begin_seq_index] # añadimos <bos> al principio\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text) # añadimos los tokens restantes en medio de la oración\n",
    "        indices.append(self.char_vocab.end_seq_index) # añadimos <eos> al final\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector será <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector será os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5897a",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento (métricas de evaluación, argumentos de entrenamiento, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    # genera batches para mandarlos al cpu/gpu (si tenemos cuda)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # loss function, en nuestro caso el cross entropy loss. Ya que compararemos la distribución de predicciones con la ground truth\n",
    "    # B, T y V son las dimensiones de nuestro tensor predicho\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "    # calculamos la comparación entre distribuciones predichas y verdaderas\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # función para calcular la accuracy, comparando cada caracter predicho con el ground truth\n",
    "    y_hat = y_pred.argmax(dim=-1)  \n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    # sacado del notebook de ALUD, argumentos de entrenamiento\n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    # función para tener en cuenta mejora/desmejora de rendimiento -> early_stopping\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    # código para el early_stopping\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c155f",
   "metadata": {},
   "source": [
    "Funciones para obtener y mostrar los nuevos versos una vez entrenados los modelos, emplearemos estas funciones una vez realizados los entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caeef39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8, top_k=None):\n",
    "    # Función para coger los nuevos versos generados y mostrarlos\n",
    "    # En nuestro caso 10 samples\n",
    "    model.eval()\n",
    "    vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(x, apply_softmax=False)         \n",
    "                next_logits = logits[0, -1] / max(temperature, 1e-8)\n",
    "\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, ix = torch.topk(next_logits, k=top_k)\n",
    "                    filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                    filtered[ix] = v\n",
    "                    next_logits = filtered\n",
    "\n",
    "                probs = torch.softmax(next_logits, dim=0)\n",
    "                next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == vocab.end_seq_index:\n",
    "                break\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    # Función para devoler los labels de los índices conseguidos en la función anterior\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecede2",
   "metadata": {},
   "source": [
    "Como usaremos los pesos del modelo de embeddings usado anteriormente (`fastText`), los importaremos aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pesos(vectorizer, modelo_ft):\n",
    "    vocab = vectorizer.char_vocab\n",
    "    token_to_idx = vocab._token_to_idx\n",
    "    tamaño_vocab = len(token_to_idx)\n",
    "    embedding_dim = modelo_ft.get_dimension()\n",
    "    pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "    for token, idx in token_to_idx.items():\n",
    "        pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "    return torch.FloatTensor(pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476beb1",
   "metadata": {},
   "source": [
    "Función para devolver y mostrar resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e89f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuevos_versos(nombre_modelo, nombre_vectorizer):\n",
    "    num_names = 10\n",
    "\n",
    "    model = nombre_modelo.cpu()\n",
    "\n",
    "    sampled_verses = decode_samples(\n",
    "        sample_from_model(\n",
    "            model,\n",
    "            nombre_vectorizer,\n",
    "            num_samples=num_names,\n",
    "            max_length=300,\n",
    "            temperature=0.8\n",
    "        ),\n",
    "        nombre_vectorizer\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(num_names):\n",
    "        print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e27712",
   "metadata": {},
   "source": [
    "### Funciones para los entrenamientos: RNN y LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a0b72",
   "metadata": {},
   "source": [
    "Clase Dataset del Corán, lo amoldaremos al formato en que viene escrito el **Corán**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aca21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2 # el +2 incluye los tokens del diccionario + <bos> y <eos>\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    # a partir de aquí hay metodos necesarios para manipular nuestro dataset específico\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0b718",
   "metadata": {},
   "source": [
    "Función de entrenamiento RNN, arquitectura que usaremos para la NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d8dbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(coran_path, output_path, ruta_ft):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # tamaño del hidden state del RNN\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3, # lr\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    # código para guardar/cargar archivos\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    dataset.set_split(\"train\")\n",
    "    train_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    dataset.set_split(\"val\")\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    dataset.set_split(\"test\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    # aquí configuramos y llamamos a la función de pesos de fastText\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ruta_ft)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    # Y creamos el modelo\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    # A partir de aquí, entrenamiento normal\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Validation\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        val_ppl = math.exp(vloss)\n",
    "        train_state.setdefault(\"val_ppl\", []).append(val_ppl)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "                f\"| val_loss={vloss:.4f} | val_ppl={val_ppl:.2f} | val_acc={vacc:.4f}\")\n",
    "\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cb61e",
   "metadata": {},
   "source": [
    "Función de entrenamiento LSTM, casi idéntico a la arquitectura RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f3ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(coran_path, output_path, ruta_ft):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    dataset.set_split(\"train\")\n",
    "    train_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    dataset.set_split(\"val\")\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    dataset.set_split(\"test\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ruta_ft)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        val_ppl = math.exp(vloss)\n",
    "        train_state.setdefault(\"val_ppl\", []).append(val_ppl)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "                f\"| val_loss={vloss:.4f} | val_ppl={val_ppl:.2f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\") \n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa682576",
   "metadata": {},
   "source": [
    "## Dataset del Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a989e",
   "metadata": {},
   "source": [
    "### RNN - Corán\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e50241",
   "metadata": {},
   "source": [
    "Modelo RNN para el Corán, arquitectura interna de la NN con sus funciones típicas de *\\__init__* y *forward*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6640ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    # nuestro modelo nn para el rnn\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5,\n",
    "                 pretrained_embeddings_ft = None):\n",
    "        super().__init__()\n",
    "        # arquitectura de nuestra rnn\n",
    "\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) # capa de inicio del tamaño del vocabulario\n",
    "        # Aquí metemos los embeddings (pesos) del fasttext\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\") # rnn\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) # fully connected\n",
    "        self.dropout_p = dropout_p # probabilidad de dropout de neuronas\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)             \n",
    "        y_out, _ = self.rnn(x_emb)               \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  \n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1124485",
   "metadata": {},
   "source": [
    "Entrenamiento del RNN para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8865e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/1049511396.py:85: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e481913869134e2e885756519821c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.3638 | val_loss=2.9685 | val_ppl=19.46 | val_acc=0.2478\n",
      "Epoch 002 | train_loss=2.9649 | val_loss=2.8756 | val_ppl=17.74 | val_acc=0.2209\n",
      "Epoch 003 | train_loss=2.8369 | val_loss=2.7578 | val_ppl=15.76 | val_acc=0.2529\n",
      "Epoch 004 | train_loss=2.6940 | val_loss=2.6387 | val_ppl=14.00 | val_acc=0.3005\n",
      "Epoch 005 | train_loss=2.5849 | val_loss=2.5473 | val_ppl=12.77 | val_acc=0.3085\n",
      "Epoch 006 | train_loss=2.5143 | val_loss=2.4867 | val_ppl=12.02 | val_acc=0.3226\n",
      "Epoch 007 | train_loss=2.4624 | val_loss=2.4416 | val_ppl=11.49 | val_acc=0.3298\n",
      "Epoch 008 | train_loss=2.4258 | val_loss=2.4055 | val_ppl=11.08 | val_acc=0.3394\n",
      "Epoch 009 | train_loss=2.3955 | val_loss=2.3772 | val_ppl=10.78 | val_acc=0.3416\n",
      "Epoch 010 | train_loss=2.3705 | val_loss=2.3520 | val_ppl=10.51 | val_acc=0.3457\n",
      "Epoch 011 | train_loss=2.3471 | val_loss=2.3302 | val_ppl=10.28 | val_acc=0.3510\n",
      "Epoch 012 | train_loss=2.3269 | val_loss=2.3095 | val_ppl=10.07 | val_acc=0.3571\n",
      "Epoch 013 | train_loss=2.3076 | val_loss=2.2905 | val_ppl=9.88 | val_acc=0.3604\n",
      "Epoch 014 | train_loss=2.2927 | val_loss=2.2741 | val_ppl=9.72 | val_acc=0.3631\n",
      "Epoch 015 | train_loss=2.2747 | val_loss=2.2573 | val_ppl=9.56 | val_acc=0.3669\n",
      "Epoch 016 | train_loss=2.2605 | val_loss=2.2420 | val_ppl=9.41 | val_acc=0.3693\n",
      "Epoch 017 | train_loss=2.2480 | val_loss=2.2298 | val_ppl=9.30 | val_acc=0.3722\n",
      "Epoch 018 | train_loss=2.2349 | val_loss=2.2148 | val_ppl=9.16 | val_acc=0.3751\n",
      "Epoch 019 | train_loss=2.2243 | val_loss=2.2025 | val_ppl=9.05 | val_acc=0.3789\n",
      "Epoch 020 | train_loss=2.2123 | val_loss=2.1887 | val_ppl=8.92 | val_acc=0.3820\n",
      "Epoch 021 | train_loss=2.2013 | val_loss=2.1779 | val_ppl=8.83 | val_acc=0.3852\n",
      "Epoch 022 | train_loss=2.1914 | val_loss=2.1670 | val_ppl=8.73 | val_acc=0.3855\n",
      "Epoch 023 | train_loss=2.1814 | val_loss=2.1564 | val_ppl=8.64 | val_acc=0.3894\n",
      "Epoch 024 | train_loss=2.1730 | val_loss=2.1453 | val_ppl=8.55 | val_acc=0.3926\n",
      "Epoch 025 | train_loss=2.1643 | val_loss=2.1384 | val_ppl=8.49 | val_acc=0.3944\n",
      "Epoch 026 | train_loss=2.1565 | val_loss=2.1302 | val_ppl=8.42 | val_acc=0.3969\n",
      "Epoch 027 | train_loss=2.1478 | val_loss=2.1201 | val_ppl=8.33 | val_acc=0.3991\n",
      "Epoch 028 | train_loss=2.1410 | val_loss=2.1114 | val_ppl=8.26 | val_acc=0.4011\n",
      "Epoch 029 | train_loss=2.1342 | val_loss=2.1042 | val_ppl=8.20 | val_acc=0.4042\n",
      "Epoch 030 | train_loss=2.1285 | val_loss=2.0965 | val_ppl=8.14 | val_acc=0.4079\n",
      "Epoch 031 | train_loss=2.1226 | val_loss=2.0906 | val_ppl=8.09 | val_acc=0.4099\n",
      "Epoch 032 | train_loss=2.1148 | val_loss=2.0862 | val_ppl=8.05 | val_acc=0.4085\n",
      "Epoch 033 | train_loss=2.1122 | val_loss=2.0818 | val_ppl=8.02 | val_acc=0.4108\n",
      "Epoch 034 | train_loss=2.1054 | val_loss=2.0742 | val_ppl=7.96 | val_acc=0.4147\n",
      "Epoch 035 | train_loss=2.0998 | val_loss=2.0689 | val_ppl=7.92 | val_acc=0.4152\n",
      "Epoch 036 | train_loss=2.0944 | val_loss=2.0648 | val_ppl=7.88 | val_acc=0.4178\n",
      "Epoch 037 | train_loss=2.0907 | val_loss=2.0597 | val_ppl=7.84 | val_acc=0.4187\n",
      "Epoch 038 | train_loss=2.0855 | val_loss=2.0547 | val_ppl=7.80 | val_acc=0.4222\n",
      "Epoch 039 | train_loss=2.0802 | val_loss=2.0520 | val_ppl=7.78 | val_acc=0.4195\n",
      "Epoch 040 | train_loss=2.0743 | val_loss=2.0469 | val_ppl=7.74 | val_acc=0.4235\n",
      "Epoch 041 | train_loss=2.0715 | val_loss=2.0408 | val_ppl=7.70 | val_acc=0.4258\n",
      "Epoch 042 | train_loss=2.0669 | val_loss=2.0389 | val_ppl=7.68 | val_acc=0.4259\n",
      "Epoch 043 | train_loss=2.0631 | val_loss=2.0323 | val_ppl=7.63 | val_acc=0.4291\n",
      "Epoch 044 | train_loss=2.0594 | val_loss=2.0308 | val_ppl=7.62 | val_acc=0.4274\n",
      "Epoch 045 | train_loss=2.0553 | val_loss=2.0262 | val_ppl=7.59 | val_acc=0.4302\n",
      "Epoch 046 | train_loss=2.0518 | val_loss=2.0207 | val_ppl=7.54 | val_acc=0.4317\n",
      "Epoch 047 | train_loss=2.0496 | val_loss=2.0197 | val_ppl=7.54 | val_acc=0.4333\n",
      "Epoch 048 | train_loss=2.0474 | val_loss=2.0177 | val_ppl=7.52 | val_acc=0.4338\n",
      "Epoch 049 | train_loss=2.0431 | val_loss=2.0141 | val_ppl=7.49 | val_acc=0.4353\n",
      "Epoch 050 | train_loss=2.0392 | val_loss=2.0104 | val_ppl=7.47 | val_acc=0.4342\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/coran/coran_rnn_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "154e9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "اقال الله ۚ ولذي الالنهم ۚ واستقزوه الصهم ولي الددكم فاسرب الذين كانوا يقولون الا تصابا انتي اوري الي اليه لا يعلمون\n",
      "\n",
      " Verse 2:\n",
      "ان الال لنيع لي تريك عليهم بخليه ۗ وهم فض خلقا ۗ ان تجب والمرترون الله في الرحون غشار الا عن يحمئت ولي الي في ال شرك ۖ فاتلمنوا شمي بعاء الا ملهم من ايله الا تد لهم من عليه ما يخرجون\n",
      "\n",
      " Verse 3:\n",
      "يل ذي المنوا ييدونا من يحم ۚ واللما جوت الله له ان ارزين الله ۚ انهم كنفتم تعضد الاكيب\n",
      "\n",
      " Verse 4:\n",
      "قال لفك المستكان عليكم ان الاعاء لا خشي اللم\n",
      "\n",
      " Verse 5:\n",
      "من لم فسول الي الملائ ۚ قوله ساء الذين تمبرون\n",
      "\n",
      " Verse 6:\n",
      "والله السجات ۚ قاله قالوا اعلم يلينا لهم من تلكم من الذراه علي المرقين\n",
      "\n",
      " Verse 7:\n",
      "واليه وقال الله الا فاطين لا تمتماء فيسمع ان يشرسكم من الذين افحيم\n",
      "\n",
      " Verse 8:\n",
      "يب سف اوترا لهم الي في الانت عليم\n",
      "\n",
      " Verse 9:\n",
      "والي انام ذل الله علي الله واتنكا ۖ والهم من الستيب\n",
      "\n",
      " Verse 10:\n",
      "فان هذ كذله لجمثرين\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85149e9f",
   "metadata": {},
   "source": [
    "Entrenamiento RNN Corán en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c8f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/1049511396.py:85: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573c7a83c9b440aeb3d98dcf282961ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.1022 | val_loss=2.8182 | val_ppl=16.75 | val_acc=0.1842\n",
      "Epoch 002 | train_loss=2.7729 | val_loss=2.6313 | val_ppl=13.89 | val_acc=0.2449\n",
      "Epoch 003 | train_loss=2.5656 | val_loss=2.4176 | val_ppl=11.22 | val_acc=0.3382\n",
      "Epoch 004 | train_loss=2.3795 | val_loss=2.2630 | val_ppl=9.61 | val_acc=0.3666\n",
      "Epoch 005 | train_loss=2.2622 | val_loss=2.1633 | val_ppl=8.70 | val_acc=0.3845\n",
      "Epoch 006 | train_loss=2.1857 | val_loss=2.0922 | val_ppl=8.10 | val_acc=0.3939\n",
      "Epoch 007 | train_loss=2.1291 | val_loss=2.0349 | val_ppl=7.65 | val_acc=0.4090\n",
      "Epoch 008 | train_loss=2.0815 | val_loss=1.9840 | val_ppl=7.27 | val_acc=0.4243\n",
      "Epoch 009 | train_loss=2.0393 | val_loss=1.9408 | val_ppl=6.96 | val_acc=0.4332\n",
      "Epoch 010 | train_loss=2.0027 | val_loss=1.9006 | val_ppl=6.69 | val_acc=0.4423\n",
      "Epoch 011 | train_loss=1.9676 | val_loss=1.8637 | val_ppl=6.45 | val_acc=0.4493\n",
      "Epoch 012 | train_loss=1.9364 | val_loss=1.8311 | val_ppl=6.24 | val_acc=0.4612\n",
      "Epoch 013 | train_loss=1.9081 | val_loss=1.8009 | val_ppl=6.06 | val_acc=0.4721\n",
      "Epoch 014 | train_loss=1.8823 | val_loss=1.7734 | val_ppl=5.89 | val_acc=0.4788\n",
      "Epoch 015 | train_loss=1.8593 | val_loss=1.7490 | val_ppl=5.75 | val_acc=0.4857\n",
      "Epoch 016 | train_loss=1.8378 | val_loss=1.7268 | val_ppl=5.62 | val_acc=0.4926\n",
      "Epoch 017 | train_loss=1.8180 | val_loss=1.7067 | val_ppl=5.51 | val_acc=0.4968\n",
      "Epoch 018 | train_loss=1.8005 | val_loss=1.6873 | val_ppl=5.41 | val_acc=0.5021\n",
      "Epoch 019 | train_loss=1.7845 | val_loss=1.6699 | val_ppl=5.31 | val_acc=0.5061\n",
      "Epoch 020 | train_loss=1.7689 | val_loss=1.6539 | val_ppl=5.23 | val_acc=0.5095\n",
      "Epoch 021 | train_loss=1.7524 | val_loss=1.6381 | val_ppl=5.15 | val_acc=0.5161\n",
      "Epoch 022 | train_loss=1.7393 | val_loss=1.6252 | val_ppl=5.08 | val_acc=0.5188\n",
      "Epoch 023 | train_loss=1.7265 | val_loss=1.6118 | val_ppl=5.01 | val_acc=0.5220\n",
      "Epoch 024 | train_loss=1.7155 | val_loss=1.6000 | val_ppl=4.95 | val_acc=0.5255\n",
      "Epoch 025 | train_loss=1.7031 | val_loss=1.5883 | val_ppl=4.90 | val_acc=0.5293\n",
      "Epoch 026 | train_loss=1.6933 | val_loss=1.5761 | val_ppl=4.84 | val_acc=0.5310\n",
      "Epoch 027 | train_loss=1.6845 | val_loss=1.5673 | val_ppl=4.79 | val_acc=0.5352\n",
      "Epoch 028 | train_loss=1.6745 | val_loss=1.5571 | val_ppl=4.75 | val_acc=0.5373\n",
      "Epoch 029 | train_loss=1.6663 | val_loss=1.5494 | val_ppl=4.71 | val_acc=0.5403\n",
      "Epoch 030 | train_loss=1.6575 | val_loss=1.5401 | val_ppl=4.67 | val_acc=0.5424\n",
      "Epoch 031 | train_loss=1.6505 | val_loss=1.5330 | val_ppl=4.63 | val_acc=0.5461\n",
      "Epoch 032 | train_loss=1.6441 | val_loss=1.5254 | val_ppl=4.60 | val_acc=0.5475\n",
      "Epoch 033 | train_loss=1.6368 | val_loss=1.5182 | val_ppl=4.56 | val_acc=0.5495\n",
      "Epoch 034 | train_loss=1.6312 | val_loss=1.5114 | val_ppl=4.53 | val_acc=0.5532\n",
      "Epoch 035 | train_loss=1.6238 | val_loss=1.5043 | val_ppl=4.50 | val_acc=0.5550\n",
      "Epoch 036 | train_loss=1.6173 | val_loss=1.4997 | val_ppl=4.48 | val_acc=0.5572\n",
      "Epoch 037 | train_loss=1.6113 | val_loss=1.4950 | val_ppl=4.46 | val_acc=0.5567\n",
      "Epoch 038 | train_loss=1.6082 | val_loss=1.4894 | val_ppl=4.43 | val_acc=0.5593\n",
      "Epoch 039 | train_loss=1.6030 | val_loss=1.4834 | val_ppl=4.41 | val_acc=0.5622\n",
      "Epoch 040 | train_loss=1.5966 | val_loss=1.4794 | val_ppl=4.39 | val_acc=0.5634\n",
      "Epoch 041 | train_loss=1.5937 | val_loss=1.4749 | val_ppl=4.37 | val_acc=0.5628\n",
      "Epoch 042 | train_loss=1.5877 | val_loss=1.4696 | val_ppl=4.35 | val_acc=0.5657\n",
      "Epoch 043 | train_loss=1.5842 | val_loss=1.4663 | val_ppl=4.33 | val_acc=0.5671\n",
      "Epoch 044 | train_loss=1.5800 | val_loss=1.4615 | val_ppl=4.31 | val_acc=0.5686\n",
      "Epoch 045 | train_loss=1.5763 | val_loss=1.4590 | val_ppl=4.30 | val_acc=0.5694\n",
      "Epoch 046 | train_loss=1.5720 | val_loss=1.4544 | val_ppl=4.28 | val_acc=0.5716\n",
      "Epoch 047 | train_loss=1.5682 | val_loss=1.4505 | val_ppl=4.27 | val_acc=0.5727\n",
      "Epoch 048 | train_loss=1.5646 | val_loss=1.4469 | val_ppl=4.25 | val_acc=0.5736\n",
      "Epoch 049 | train_loss=1.5609 | val_loss=1.4441 | val_ppl=4.24 | val_acc=0.5753\n",
      "Epoch 050 | train_loss=1.5583 | val_loss=1.4407 | val_ppl=4.22 | val_acc=0.5757\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/coran/coran_rnn_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1140e",
   "metadata": {},
   "source": [
    "Obtenemos los nuevos versos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90e0aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and have sereld\n",
      "\n",
      " Verse 2:\n",
      "and about you of efilper for the dos fade for them a there allah becisely to the sild at for he who de the pars and the remant that whime sais he allah parase and is that way for the ealled o murist on is in the exaltion if allah have mestedert of the sene to the enister than thatish and exallt will\n",
      "\n",
      " Verse 3:\n",
      "indeed the withe say the mereis or goud ferreas indeed in you and whore and seek not but they it enter dourmenn and messengers wead not believe in the herother dose ho dinngned then we proty who son in the ranseon that whe the magot aid in mall and their lord and except the poiginh not and it an one\n",
      "\n",
      " Verse 4:\n",
      "indeat intrees in chater and distared and thith allah he will be and darthin so you is ahmease gord the do thise and that allah seide phorlond to lich it and what is merusrecslise of it in thome mund in forit and war and in the sore made which our people they trey from you allah for when allah who w\n",
      "\n",
      " Verse 5:\n",
      "ant we have forisheeciabees there urepent and the mathur belo allah and allah except to them and is whoever wall\n",
      "\n",
      " Verse 6:\n",
      "and we he and when he will do and is becup the mely ablait in tho resartem they his liveling that averting them of missseding wnat the lering then they not indeed him and it able me as talled have hat worknos kind ther is for aquress and atorion we have have disfil agith as forder to you except what\n",
      "\n",
      " Verse 7:\n",
      "and they him in the puniding in the ain to you sakling to you they of utyint have i purision ham do guinty oun thit the sint have hrom allah allah is hmentsess to the dors of allis allah said o ther and the peos made them and they with avers corpiomed in bemose safter the meske you me\n",
      "\n",
      " Verse 8:\n",
      "assed upon your enaaler lord and indeed a thend me prome and they cant of allah of igere of them and on exccustitite that you deers of allah and say of allah say eviled they are toull ther the earth and is has listing them and allig and he is and we seques enible them purise has not the there to hha\n",
      "\n",
      " Verse 9:\n",
      "and brecay anceen of all then they whing deating and they saud betien and given that it and wirh of ceevenowen to the head bessed the indeed xaves be thene thith ahtilled be those whos and who whon it meant\n",
      "\n",
      " Verse 10:\n",
      "and you tile then shes have mase maratt us a oul merest withor or them ablalavient remain the wall indeed comers indeed allah is dhe may on the people that them but their were and comevers for you and he will we will he wars and destos the remase thes a and for the decase them are d it beliny is oul\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd760ebc",
   "metadata": {},
   "source": [
    "### LSTM - Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee0af9",
   "metadata": {},
   "source": [
    "Modelo del LSTM para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c83baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5, pretrained_embeddings_ft=None):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              \n",
    "        y_out, _ = self.lstm(x_emb)              \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                 \n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f224",
   "metadata": {},
   "source": [
    "Entrenamiento del LSTM para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d0784b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/2429519463.py:81: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ac94b8fc9c465b9a010da71e22097d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.9997 | val_loss=2.7963 | val_ppl=16.38 | val_acc=0.2471\n",
      "Epoch 002 | train_loss=2.5932 | val_loss=2.5047 | val_ppl=12.24 | val_acc=0.3136\n",
      "Epoch 003 | train_loss=2.4149 | val_loss=2.3790 | val_ppl=10.79 | val_acc=0.3430\n",
      "Epoch 004 | train_loss=2.3033 | val_loss=2.2856 | val_ppl=9.83 | val_acc=0.3595\n",
      "Epoch 005 | train_loss=2.2160 | val_loss=2.2091 | val_ppl=9.11 | val_acc=0.3796\n",
      "Epoch 006 | train_loss=2.1418 | val_loss=2.1396 | val_ppl=8.50 | val_acc=0.3980\n",
      "Epoch 007 | train_loss=2.0757 | val_loss=2.0836 | val_ppl=8.03 | val_acc=0.4191\n",
      "Epoch 008 | train_loss=2.0219 | val_loss=2.0364 | val_ppl=7.66 | val_acc=0.4321\n",
      "Epoch 009 | train_loss=1.9744 | val_loss=1.9981 | val_ppl=7.37 | val_acc=0.4425\n",
      "Epoch 010 | train_loss=1.9343 | val_loss=1.9641 | val_ppl=7.13 | val_acc=0.4531\n",
      "Epoch 011 | train_loss=1.8980 | val_loss=1.9401 | val_ppl=6.96 | val_acc=0.4624\n",
      "Epoch 012 | train_loss=1.8659 | val_loss=1.9140 | val_ppl=6.78 | val_acc=0.4681\n",
      "Epoch 013 | train_loss=1.8372 | val_loss=1.8857 | val_ppl=6.59 | val_acc=0.4728\n",
      "Epoch 014 | train_loss=1.8136 | val_loss=1.8686 | val_ppl=6.48 | val_acc=0.4795\n",
      "Epoch 015 | train_loss=1.7910 | val_loss=1.8533 | val_ppl=6.38 | val_acc=0.4830\n",
      "Epoch 016 | train_loss=1.7675 | val_loss=1.8395 | val_ppl=6.29 | val_acc=0.4889\n",
      "Epoch 017 | train_loss=1.7504 | val_loss=1.8264 | val_ppl=6.21 | val_acc=0.4905\n",
      "Epoch 018 | train_loss=1.7320 | val_loss=1.8114 | val_ppl=6.12 | val_acc=0.4929\n",
      "Epoch 019 | train_loss=1.7163 | val_loss=1.7936 | val_ppl=6.01 | val_acc=0.4983\n",
      "Epoch 020 | train_loss=1.7014 | val_loss=1.7869 | val_ppl=5.97 | val_acc=0.4963\n",
      "Epoch 021 | train_loss=1.6873 | val_loss=1.7759 | val_ppl=5.91 | val_acc=0.5022\n",
      "Epoch 022 | train_loss=1.6740 | val_loss=1.7684 | val_ppl=5.86 | val_acc=0.5011\n",
      "Epoch 023 | train_loss=1.6611 | val_loss=1.7621 | val_ppl=5.82 | val_acc=0.5045\n",
      "Epoch 024 | train_loss=1.6475 | val_loss=1.7557 | val_ppl=5.79 | val_acc=0.5078\n",
      "Epoch 025 | train_loss=1.6362 | val_loss=1.7470 | val_ppl=5.74 | val_acc=0.5080\n",
      "Epoch 026 | train_loss=1.6253 | val_loss=1.7417 | val_ppl=5.71 | val_acc=0.5081\n",
      "Epoch 027 | train_loss=1.6138 | val_loss=1.7419 | val_ppl=5.71 | val_acc=0.5101\n",
      "Epoch 028 | train_loss=1.6051 | val_loss=1.7367 | val_ppl=5.68 | val_acc=0.5127\n",
      "Epoch 029 | train_loss=1.5956 | val_loss=1.7305 | val_ppl=5.64 | val_acc=0.5149\n",
      "Epoch 030 | train_loss=1.5837 | val_loss=1.7294 | val_ppl=5.64 | val_acc=0.5156\n",
      "Epoch 031 | train_loss=1.5736 | val_loss=1.7306 | val_ppl=5.64 | val_acc=0.5141\n",
      "Epoch 032 | train_loss=1.5672 | val_loss=1.7229 | val_ppl=5.60 | val_acc=0.5179\n",
      "Epoch 033 | train_loss=1.5554 | val_loss=1.7226 | val_ppl=5.60 | val_acc=0.5172\n",
      "Epoch 034 | train_loss=1.5463 | val_loss=1.7179 | val_ppl=5.57 | val_acc=0.5179\n",
      "Epoch 035 | train_loss=1.5376 | val_loss=1.7129 | val_ppl=5.55 | val_acc=0.5206\n",
      "Epoch 036 | train_loss=1.5308 | val_loss=1.7155 | val_ppl=5.56 | val_acc=0.5198\n",
      "Epoch 037 | train_loss=1.5239 | val_loss=1.7098 | val_ppl=5.53 | val_acc=0.5206\n",
      "Epoch 038 | train_loss=1.5145 | val_loss=1.7152 | val_ppl=5.56 | val_acc=0.5191\n",
      "Epoch 039 | train_loss=1.5068 | val_loss=1.7058 | val_ppl=5.51 | val_acc=0.5197\n",
      "Epoch 040 | train_loss=1.4981 | val_loss=1.7103 | val_ppl=5.53 | val_acc=0.5213\n",
      "Epoch 041 | train_loss=1.4943 | val_loss=1.7160 | val_ppl=5.56 | val_acc=0.5217\n",
      "Epoch 042 | train_loss=1.4759 | val_loss=1.7079 | val_ppl=5.52 | val_acc=0.5211\n",
      "Epoch 043 | train_loss=1.4688 | val_loss=1.7085 | val_ppl=5.52 | val_acc=0.5236\n",
      "Epoch 044 | train_loss=1.4597 | val_loss=1.7082 | val_ppl=5.52 | val_acc=0.5232\n",
      "Early stopping activado.\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/coran/coran_lstm_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02fecd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "وضلغ عليهم من الذين امنوا وعملوا الصالحات وما انزل الي الرحمن فلينات حجاله الا ما شاء الله ۖ ومن المؤمنين ۖ ان الله كان بينهما ۖ ومن يعد ولا اذا ما لم كنتم من دان ولين ۚ كذلك هذا لفذكر الله وتلكا من الكافرين\n",
      "\n",
      " Verse 2:\n",
      "وان لك مواي الا ان اختتك فيها لا يهدي من يشاء من بان الله اشركم مما يضتهم الله وهم لا يحسنون عما قبل من الناس برسول الله ۖ فرب السمات والارض ولا تعذبهم بالوماه وسمع الله ما كنتم تعلمون\n",
      "\n",
      " Verse 3:\n",
      "قال رب اني وانا وما تخذ الذين كفروا الله ۖ فان انزل الله للذين كذبوا ۚ ان كل ملك في الارض الموت ۚ ولا اسما الولا الكافرين\n",
      "\n",
      " Verse 4:\n",
      "اويجيمون الله غير الله الا ان يفيروا انه لا يسالهم ما ما اعلم بناء ان يتقون ۖ فمن اكفر الا جاء الله بما يشعرون\n",
      "\n",
      " Verse 5:\n",
      "وان الذين امنوا وعلموا ان هذا الذي انا كنا واحده ما كانوا يعملون\n",
      "\n",
      " Verse 6:\n",
      "يالم لم تنتظا انهم يستهبظون والذين امنوا والموادوا وهم يتقون ۚ انه الموت ولا تحشرون\n",
      "\n",
      " Verse 7:\n",
      "قال امنكم من ان يعبر الله اجلا ولكن اجر الا ما انزل اليك لمن دونك يدعل فان الله ازيد يحي عيد الموسي ۖ ولما اخرج ان احدهم الله وامعوا من ربكم وربك وفي سبيله من نساء لا تعلموا العذاب الا جنه ۗ والله بالمسته في السماوات والارض ولا يبين منه وجده من الناس من ان تركم عدا\n",
      "\n",
      " Verse 8:\n",
      "هدينا ولا ينفق من الناس وانه ليعدلون به الاولاء ورسلاته بالبيص ان انتم خير لقسانا فنحن امرسا به عزد من التحمر فان الله يقول الشيطان العزيز المين\n",
      "\n",
      " Verse 9:\n",
      "ان تقرب الذين تزعلون علي الله وبعض السبيل الذي عاد معجدي ولمهم ما ينففون\n",
      "\n",
      " Verse 10:\n",
      "قال رب اني ناد روح ان يعبد الذين كفروا واذا اتوفك الي الكتاب في الايه واراد ما سمع الها ومائه الا القوم الظالمون\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d3128",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento inglés de LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edb9054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/2429519463.py:81: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3050581d77d48baa8d0b7efdd820780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8106 | val_loss=2.4503 | val_ppl=11.59 | val_acc=0.2774\n",
      "Epoch 002 | train_loss=2.2447 | val_loss=2.0349 | val_ppl=7.65 | val_acc=0.4028\n",
      "Epoch 003 | train_loss=1.9601 | val_loss=1.8020 | val_ppl=6.06 | val_acc=0.4726\n",
      "Epoch 004 | train_loss=1.7781 | val_loss=1.6467 | val_ppl=5.19 | val_acc=0.5090\n",
      "Epoch 005 | train_loss=1.6492 | val_loss=1.5333 | val_ppl=4.63 | val_acc=0.5450\n",
      "Epoch 006 | train_loss=1.5499 | val_loss=1.4529 | val_ppl=4.28 | val_acc=0.5650\n",
      "Epoch 007 | train_loss=1.4753 | val_loss=1.3881 | val_ppl=4.01 | val_acc=0.5818\n",
      "Epoch 008 | train_loss=1.4173 | val_loss=1.3384 | val_ppl=3.81 | val_acc=0.5954\n",
      "Epoch 009 | train_loss=1.3705 | val_loss=1.2983 | val_ppl=3.66 | val_acc=0.6106\n",
      "Epoch 010 | train_loss=1.3298 | val_loss=1.2679 | val_ppl=3.55 | val_acc=0.6186\n",
      "Epoch 011 | train_loss=1.2970 | val_loss=1.2387 | val_ppl=3.45 | val_acc=0.6247\n",
      "Epoch 012 | train_loss=1.2700 | val_loss=1.2191 | val_ppl=3.38 | val_acc=0.6298\n",
      "Epoch 013 | train_loss=1.2450 | val_loss=1.1998 | val_ppl=3.32 | val_acc=0.6364\n",
      "Epoch 014 | train_loss=1.2235 | val_loss=1.1888 | val_ppl=3.28 | val_acc=0.6398\n",
      "Epoch 015 | train_loss=1.2057 | val_loss=1.1698 | val_ppl=3.22 | val_acc=0.6455\n",
      "Epoch 016 | train_loss=1.1895 | val_loss=1.1563 | val_ppl=3.18 | val_acc=0.6496\n",
      "Epoch 017 | train_loss=1.1738 | val_loss=1.1457 | val_ppl=3.14 | val_acc=0.6530\n",
      "Epoch 018 | train_loss=1.1581 | val_loss=1.1357 | val_ppl=3.11 | val_acc=0.6537\n",
      "Epoch 019 | train_loss=1.1448 | val_loss=1.1256 | val_ppl=3.08 | val_acc=0.6574\n",
      "Epoch 020 | train_loss=1.1343 | val_loss=1.1160 | val_ppl=3.05 | val_acc=0.6601\n",
      "Epoch 021 | train_loss=1.1223 | val_loss=1.1092 | val_ppl=3.03 | val_acc=0.6636\n",
      "Epoch 022 | train_loss=1.1129 | val_loss=1.1022 | val_ppl=3.01 | val_acc=0.6669\n",
      "Epoch 023 | train_loss=1.1033 | val_loss=1.0957 | val_ppl=2.99 | val_acc=0.6701\n",
      "Epoch 024 | train_loss=1.0943 | val_loss=1.0885 | val_ppl=2.97 | val_acc=0.6706\n",
      "Epoch 025 | train_loss=1.0833 | val_loss=1.0847 | val_ppl=2.96 | val_acc=0.6730\n",
      "Epoch 026 | train_loss=1.0753 | val_loss=1.0781 | val_ppl=2.94 | val_acc=0.6743\n",
      "Epoch 027 | train_loss=1.0683 | val_loss=1.0757 | val_ppl=2.93 | val_acc=0.6757\n",
      "Epoch 028 | train_loss=1.0604 | val_loss=1.0707 | val_ppl=2.92 | val_acc=0.6762\n",
      "Epoch 029 | train_loss=1.0536 | val_loss=1.0657 | val_ppl=2.90 | val_acc=0.6780\n",
      "Epoch 030 | train_loss=1.0480 | val_loss=1.0642 | val_ppl=2.90 | val_acc=0.6778\n",
      "Epoch 031 | train_loss=1.0400 | val_loss=1.0610 | val_ppl=2.89 | val_acc=0.6787\n",
      "Epoch 032 | train_loss=1.0338 | val_loss=1.0573 | val_ppl=2.88 | val_acc=0.6806\n",
      "Epoch 033 | train_loss=1.0288 | val_loss=1.0547 | val_ppl=2.87 | val_acc=0.6817\n",
      "Epoch 034 | train_loss=1.0240 | val_loss=1.0519 | val_ppl=2.86 | val_acc=0.6818\n",
      "Epoch 035 | train_loss=1.0189 | val_loss=1.0515 | val_ppl=2.86 | val_acc=0.6820\n",
      "Epoch 036 | train_loss=1.0122 | val_loss=1.0463 | val_ppl=2.85 | val_acc=0.6851\n",
      "Epoch 037 | train_loss=1.0070 | val_loss=1.0433 | val_ppl=2.84 | val_acc=0.6861\n",
      "Epoch 038 | train_loss=1.0023 | val_loss=1.0439 | val_ppl=2.84 | val_acc=0.6868\n",
      "Epoch 039 | train_loss=0.9959 | val_loss=1.0406 | val_ppl=2.83 | val_acc=0.6867\n",
      "Epoch 040 | train_loss=0.9917 | val_loss=1.0396 | val_ppl=2.83 | val_acc=0.6852\n",
      "Epoch 041 | train_loss=0.9865 | val_loss=1.0361 | val_ppl=2.82 | val_acc=0.6875\n",
      "Epoch 042 | train_loss=0.9838 | val_loss=1.0368 | val_ppl=2.82 | val_acc=0.6881\n",
      "Epoch 043 | train_loss=0.9794 | val_loss=1.0355 | val_ppl=2.82 | val_acc=0.6873\n",
      "Epoch 044 | train_loss=0.9760 | val_loss=1.0377 | val_ppl=2.82 | val_acc=0.6886\n",
      "Epoch 045 | train_loss=0.9725 | val_loss=1.0338 | val_ppl=2.81 | val_acc=0.6900\n",
      "Epoch 046 | train_loss=0.9670 | val_loss=1.0348 | val_ppl=2.81 | val_acc=0.6888\n",
      "Epoch 047 | train_loss=0.9644 | val_loss=1.0333 | val_ppl=2.81 | val_acc=0.6904\n",
      "Epoch 048 | train_loss=0.9596 | val_loss=1.0278 | val_ppl=2.79 | val_acc=0.6906\n",
      "Epoch 049 | train_loss=0.9577 | val_loss=1.0329 | val_ppl=2.81 | val_acc=0.6905\n",
      "Epoch 050 | train_loss=0.9529 | val_loss=1.0327 | val_ppl=2.81 | val_acc=0.6898\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/coran/coran_lstm_v1\",\n",
    "                                                 ruta_ft=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0afe795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and indeed they will be recompensed except for a man or harms it in the believers will and have a virmonation and then for them has sin indeed he is the knowing and markmeds and ever is allah to him belongs the peace of earth so we submitted to him and they will say the unly of the dosire to a strai\n",
      "\n",
      " Verse 2:\n",
      "and one a him the sea and upon moses and a prophet from the great punishment a good provision he could not give him to made for them will pass while you instinat the sin to paradise whom you were a greater partners of them inspresting to allah\n",
      "\n",
      " Verse 3:\n",
      "and indeed thele except that you may have among them is a corrupter from them and allah is with the righteous\n",
      "\n",
      " Verse 4:\n",
      "and when theas the light and become worthless allah ssidus so benefit things and recovends to fire but their lord benefit them chosen indeed allah is fortiving in one who are the established sinclest our lord in like the signs of allah\n",
      "\n",
      " Verse 5:\n",
      "and moses seek the truth and denied the best of a great termusing with their fires except a repentan of your heart and puralin yourselves and i slourd and if you do rising of allah said so would have untreed mined and successors decide and we intended thus dis are we have pursed them indeed they hav\n",
      "\n",
      " Verse 6:\n",
      "whoever you give him and me there said o him not so follow it and between you but their deeds and bedahe the strong of our wished is the commit on earth or there would not dished them and the wrongdoers are an ungate of their allahs and charge is recited to you a one which we would sup treat the pay\n",
      "\n",
      " Verse 7:\n",
      "indeed in that which allah has made to lime allah has earned among them a people who have believed and the wised there is no beater over the save of a few allah the one who invenss to your belief and not disbelieved bat their lord and his creation will be appraise of their hands and it is you are ou\n",
      "\n",
      " Verse 8:\n",
      "and we will let them said we have sent weaker or that which we would not desor you to eat and their errint in the harm or destroy in that allah has been sent down and knowledge of the is the our sons they say he fight them of one of them who will be into the day of the believers with their truth the\n",
      "\n",
      " Verse 9:\n",
      "and when they spend there is not for them about him somother has made for them there is no guide over them when you said the fire of her which you spend on the cause of that is a kind descompentine of your ood an men and see\n",
      "\n",
      " Verse 10:\n",
      "so we will be distressed on the day that be not some worthing\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c641da9",
   "metadata": {},
   "source": [
    "## Dataset Hadith-s (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fd42b",
   "metadata": {},
   "source": [
    "Visualizamos el la estructura del df, cogeremos las columnas (hadith-s) que nos interesan: `text_ar` y `text_en`. Como el archivo viene estructurado de una manera poco usual, realizaremos una limpieza exhaustiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3451d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hadith_id</th>\n",
       "      <th>source</th>\n",
       "      <th>chapter_no</th>\n",
       "      <th>hadith_no</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chain_indx</th>\n",
       "      <th>text_ar</th>\n",
       "      <th>text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30418, 20005, 11062, 11213, 11042, 3</td>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...</td>\n",
       "      <td>Narrated 'Umar bin Al-Khattab:          ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hadith_id           source  chapter_no hadith_no  \\\n",
       "0   0          1   Sahih Bukhari            1        1    \n",
       "\n",
       "                       chapter                            chain_indx  \\\n",
       "0  Revelation - كتاب بدء الوحى  30418, 20005, 11062, 11213, 11042, 3   \n",
       "\n",
       "                                             text_ar  \\\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سف...   \n",
       "\n",
       "                                             text_en  \n",
       "0        Narrated 'Umar bin Al-Khattab:          ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_df = pd.read_csv(\"../data/hadith_dataset/all_hadiths_clean.csv\")\n",
    "hadith_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b0fc5",
   "metadata": {},
   "source": [
    "Árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01c5b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ar    34433\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_ar = hadith_df[\"text_ar\"]\n",
    "hadith_ar = pd.DataFrame(hadith_ar).dropna()\n",
    "hadith_ar.to_csv(\"../data/hadith_dataset/hadith_ar/hadith_ar.csv\", index=False, encoding=\"utf-8\")\n",
    "print(hadith_ar.count())\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "hadith_ar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b0149",
   "metadata": {},
   "source": [
    "Limpieza árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3b08968",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "# Diacríticos árabes (harakat) + marcas coránicas comunes\n",
    "ARABIC_DIACRITICS = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "# Rangos Unicode típicos para árabe (básico + extendidos)\n",
    "ARABIC_RANGES = r\"\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def normalize_arabic(text: str, remove_diacritics: bool = True) -> str:\n",
    "    # Normalización Unicode (unifica formas)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Quitar tatweel (kashida)\n",
    "    text = text.replace(\"\\u0640\", \"\")\n",
    "\n",
    "    # Unificar algunas variantes comunes (opcional, útil en muchos corpus)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\")\n",
    "    text = text.replace(\"ة\", \"ه\")  # si prefieres mantenerla, comenta esta línea\n",
    "\n",
    "    if remove_diacritics:\n",
    "        text = re.sub(ARABIC_DIACRITICS, \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_hadith_text_ar(text, remove_diacritics: bool = True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Espacios/saltos de línea\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "    # 2) Quitar comillas envolventes\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    # 3) Normalización árabe (sin lower)\n",
    "    text = normalize_arabic(text, remove_diacritics=remove_diacritics)\n",
    "\n",
    "    # 4) Eliminar narrador (si el encabezado está en inglés, como en tu caso)\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, \"\", text).strip()\n",
    "\n",
    "    # 5) Quitar comillas residuales\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', \"\", text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', \"\", text)\n",
    "\n",
    "    # 6) Mantener: letras árabes, números, espacios y puntuación básica.\n",
    "    # Incluye puntuación árabe: ، ؛ ؟  (comma/semicolon/question mark)\n",
    "    allowed = rf\"[^0-9\\s{ARABIC_RANGES}\\.,!?'\\-\\(\\)«»\\\"،؛؟]\"\n",
    "    text = re.sub(allowed, \" \", text)\n",
    "\n",
    "    # 7) Colapsar espacios\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68252da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_ar = hadith_df[[\"text_ar\"]].copy()\n",
    "\n",
    "hadith_ar = hadith_ar.dropna(subset=[\"text_ar\"])\n",
    "\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar[\"text_ar\"] = hadith_ar[\"text_ar\"].apply(clean_hadith_text_ar)\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar = hadith_ar[hadith_ar[\"text_ar\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\"\n",
    "\n",
    "hadith_ar.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575a5f5",
   "metadata": {},
   "source": [
    "Inglés + función de limpieza inglesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "089ced52",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        # ^\\s*[\"'“”...]+ (captura comillas al inicio) y [\"'“”...]+\\s*$ (al final)\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def clean_hadith_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    text = text.replace('\"\"', '\"').lower()\n",
    "\n",
    "    # Limpieza del formato original del .csv: narrated by (nommbre del narrador) + texto que queremos\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, '', text).strip()\n",
    "\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\-\\(\\)]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f8ffd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_en = hadith_df[[\"text_en\"]].copy()\n",
    "\n",
    "hadith_en = hadith_en.dropna(subset=[\"text_en\"])\n",
    "\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en[\"text_en\"] = hadith_en[\"text_en\"].apply(clean_hadith_text)\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en = hadith_en[hadith_en[\"text_en\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\"\n",
    "\n",
    "hadith_en.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa20bd5",
   "metadata": {},
   "source": [
    "Clase Dataset del Hadith dataset, adaptado al formato del dataset de *Kaggle*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc92ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col):\n",
    "        # text_col: text_en (hadith_en) y text_ar (hadith_ar)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "        self._max_seq_length = min(int(self.df[text_col].astype(str).map(len).max()) + 2, 500)        \n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, hadith_csv, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, hadith_csv, vectorizer_filepath, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long), \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bba93",
   "metadata": {},
   "source": [
    "### RNN - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a413054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(hadith_path, output_path, text_col, ft_ruta):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # 256-ekin peatau itenzatek\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    print(args.batch_size)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    dataset.set_split(\"train\")\n",
    "    train_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    dataset.set_split(\"val\")\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    dataset.set_split(\"test\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_model = fasttext.load_model(ft_ruta)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_model)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        val_ppl = math.exp(vloss)\n",
    "        train_state.setdefault(\"val_ppl\", []).append(val_ppl)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "                f\"| val_loss={vloss:.4f} | val_ppl={val_ppl:.2f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b4da3",
   "metadata": {},
   "source": [
    "Entrenamiento RNN con dataset Hadith en árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e07996b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/3333836913.py:83: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e8677d0cb24d5597370685a96ef662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8336 | val_loss=2.2752 | val_ppl=9.73 | val_acc=0.3948\n",
      "Epoch 002 | train_loss=2.1672 | val_loss=1.9568 | val_ppl=7.08 | val_acc=0.4791\n",
      "Epoch 003 | train_loss=1.9640 | val_loss=1.7776 | val_ppl=5.92 | val_acc=0.5121\n",
      "Epoch 004 | train_loss=1.8447 | val_loss=1.6680 | val_ppl=5.30 | val_acc=0.5482\n",
      "Epoch 005 | train_loss=1.7721 | val_loss=1.6024 | val_ppl=4.96 | val_acc=0.5599\n",
      "Epoch 006 | train_loss=1.7270 | val_loss=1.5635 | val_ppl=4.78 | val_acc=0.5708\n",
      "Epoch 007 | train_loss=1.6968 | val_loss=1.5355 | val_ppl=4.64 | val_acc=0.5764\n",
      "Epoch 008 | train_loss=1.6735 | val_loss=1.5135 | val_ppl=4.54 | val_acc=0.5809\n",
      "Epoch 009 | train_loss=1.6548 | val_loss=1.4963 | val_ppl=4.47 | val_acc=0.5847\n",
      "Epoch 010 | train_loss=1.6403 | val_loss=1.4829 | val_ppl=4.41 | val_acc=0.5898\n",
      "Epoch 011 | train_loss=1.6278 | val_loss=1.4721 | val_ppl=4.36 | val_acc=0.5923\n",
      "Epoch 012 | train_loss=1.6177 | val_loss=1.4629 | val_ppl=4.32 | val_acc=0.5949\n",
      "Epoch 013 | train_loss=1.6079 | val_loss=1.4537 | val_ppl=4.28 | val_acc=0.5961\n",
      "Epoch 014 | train_loss=1.6009 | val_loss=1.4488 | val_ppl=4.26 | val_acc=0.5987\n",
      "Epoch 015 | train_loss=1.5934 | val_loss=1.4418 | val_ppl=4.23 | val_acc=0.5994\n",
      "Epoch 016 | train_loss=1.5863 | val_loss=1.4362 | val_ppl=4.20 | val_acc=0.6001\n",
      "Epoch 017 | train_loss=1.5809 | val_loss=1.4308 | val_ppl=4.18 | val_acc=0.5984\n",
      "Epoch 018 | train_loss=1.5750 | val_loss=1.4268 | val_ppl=4.17 | val_acc=0.5993\n",
      "Epoch 019 | train_loss=1.5706 | val_loss=1.4211 | val_ppl=4.14 | val_acc=0.6047\n",
      "Epoch 020 | train_loss=1.5658 | val_loss=1.4185 | val_ppl=4.13 | val_acc=0.6019\n",
      "Epoch 021 | train_loss=1.5613 | val_loss=1.4151 | val_ppl=4.12 | val_acc=0.6035\n",
      "Epoch 022 | train_loss=1.5578 | val_loss=1.4117 | val_ppl=4.10 | val_acc=0.6042\n",
      "Epoch 023 | train_loss=1.5540 | val_loss=1.4091 | val_ppl=4.09 | val_acc=0.6061\n",
      "Epoch 024 | train_loss=1.5509 | val_loss=1.4069 | val_ppl=4.08 | val_acc=0.6043\n",
      "Epoch 025 | train_loss=1.5486 | val_loss=1.4035 | val_ppl=4.07 | val_acc=0.6065\n",
      "Epoch 026 | train_loss=1.5446 | val_loss=1.4009 | val_ppl=4.06 | val_acc=0.6072\n",
      "Epoch 027 | train_loss=1.5422 | val_loss=1.4013 | val_ppl=4.06 | val_acc=0.6063\n",
      "Epoch 028 | train_loss=1.5399 | val_loss=1.3967 | val_ppl=4.04 | val_acc=0.6097\n",
      "Epoch 029 | train_loss=1.5370 | val_loss=1.3965 | val_ppl=4.04 | val_acc=0.6079\n",
      "Epoch 030 | train_loss=1.5346 | val_loss=1.3925 | val_ppl=4.02 | val_acc=0.6101\n",
      "Epoch 031 | train_loss=1.5325 | val_loss=1.3920 | val_ppl=4.02 | val_acc=0.6092\n",
      "Epoch 032 | train_loss=1.5299 | val_loss=1.3907 | val_ppl=4.02 | val_acc=0.6103\n",
      "Epoch 033 | train_loss=1.5285 | val_loss=1.3892 | val_ppl=4.01 | val_acc=0.6111\n",
      "Epoch 034 | train_loss=1.5261 | val_loss=1.3860 | val_ppl=4.00 | val_acc=0.6116\n",
      "Epoch 035 | train_loss=1.5243 | val_loss=1.3854 | val_ppl=4.00 | val_acc=0.6117\n",
      "Epoch 036 | train_loss=1.5230 | val_loss=1.3841 | val_ppl=3.99 | val_acc=0.6135\n",
      "Epoch 037 | train_loss=1.5215 | val_loss=1.3858 | val_ppl=4.00 | val_acc=0.6116\n",
      "Epoch 038 | train_loss=1.5188 | val_loss=1.3810 | val_ppl=3.98 | val_acc=0.6133\n",
      "Epoch 039 | train_loss=1.5179 | val_loss=1.3792 | val_ppl=3.97 | val_acc=0.6148\n",
      "Epoch 040 | train_loss=1.5163 | val_loss=1.3780 | val_ppl=3.97 | val_acc=0.6149\n",
      "Epoch 041 | train_loss=1.5152 | val_loss=1.3782 | val_ppl=3.97 | val_acc=0.6147\n",
      "Epoch 042 | train_loss=1.5136 | val_loss=1.3771 | val_ppl=3.96 | val_acc=0.6154\n",
      "Epoch 043 | train_loss=1.5121 | val_loss=1.3765 | val_ppl=3.96 | val_acc=0.6145\n",
      "Epoch 044 | train_loss=1.5112 | val_loss=1.3768 | val_ppl=3.96 | val_acc=0.6153\n",
      "Epoch 045 | train_loss=1.5099 | val_loss=1.3754 | val_ppl=3.96 | val_acc=0.6157\n",
      "Epoch 046 | train_loss=1.5085 | val_loss=1.3743 | val_ppl=3.95 | val_acc=0.6156\n",
      "Epoch 047 | train_loss=1.5074 | val_loss=1.3726 | val_ppl=3.95 | val_acc=0.6161\n",
      "Epoch 048 | train_loss=1.5063 | val_loss=1.3718 | val_ppl=3.94 | val_acc=0.6172\n",
      "Epoch 049 | train_loss=1.5053 | val_loss=1.3735 | val_ppl=3.95 | val_acc=0.6161\n",
      "Epoch 050 | train_loss=1.5046 | val_loss=1.3714 | val_ppl=3.94 | val_acc=0.6165\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_ar\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f40cda4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثناه سمعه وابي بعد حنصد في البير لامر حوله من الافزيع الله لبي ما التما الكيت بصله من غفر، فقال له عن يول الماله فيقلت يعمر والحجا . قال الوهل - عن حرار وابي رؤي المباره، وما مك اما قال \" ما اليران . وان الله الناس فربراعبي الي لا الصبي من صال الذا يقال وال شيده وجل، فانس لا يكنا يشاكل في الولس .\n",
      "\n",
      " Verse 2:\n",
      "حدثنا ابو الزبر بن حاجن، عت النهير بن الوحيم، عن ابي الزهري، ان ابن عباع بن سمل، عن اسااق، عن عبد الله بن اليمان، - ضه بن الاسلم ابن عشر وابو برضل اوار شوك عن الاسراه . قال يبكن ملاتي في اول الخرج، صماه بي اسحما عن ابي هريره، قال حدثنا جعبرا ذي ان ابو مسمعه والدار شعبه عن الرهم المزهد وفا حديثه حدثن\n",
      "\n",
      " Verse 3:\n",
      "وحدثني محدي، حدثنا ابو عمر بن عبد الله بن جعيري، حدثنا ابو بزري عن عبر الله بن عمرو بن شعيب، حدثنا ابي حيو بكن النبير بو حدثنا الصببد، عن اليسن، قال قال ابن ابي هيي به \" . قال ابو اقال الحكر النبي صلي الله عليه وسلم، فان اسلم ينت بالله في الصبره اله الا الا قال \" ناس شعط واعثوا معه القاسم ورسيوه فما\n",
      "\n",
      " Verse 4:\n",
      "حدثنا عبد الرحرد ومحوك والززير وابن الاما صلي الله عليه وسلم كاطي في لحاقيت فاخر ظن الكمر الجد ومع الخمر الحديث فاسامه الطيانه - قال معها .\n",
      "\n",
      " Verse 5:\n",
      "حدثنا اقرك بن اشان .\n",
      "\n",
      " Verse 6:\n",
      "حدثنا اسحمنا هلابه الانيفي، عن عبد الله بن بامر بن هجال، قال اخبرنا ابو النكان،، بحده فخ بن ابا بربه ان المني لا يعني بالمسلمان وقات \" .\n",
      "\n",
      " Verse 7:\n",
      "حدثنا قدير بن الرسور، وحدثنا الناس، عن عبد الله بن ابي صلايه، ان كمي، عن الااب يعهو قال قلا ان فصلا لا يا راله لم ونتقتال فياه الي الله ايدي في الحجي فيما لقوذ به الاذا قزل اللن قال \" ما الجهر في البيوم باليها ان يسلي الاسعمه فيها يروه افن البال عن علي عن مالك، قالم بها الجنب بالنهات وتالعت سمعه سل \n",
      "\n",
      " Verse 8:\n",
      "حدثنا عبيد الله بن عمر، عن ابن عمر، ان انا حكيث عن النييه بن شعبه، بن الامراعي والتيوح با لق عنه ان الناس الو الخبرده ثتان مل سلبه بغي الابي \" . فقال النايد القد فستلتا الاشراب ولا يحمط الا المبري يعنر الدينا بكت اتلي من يراذا انه فقال \" االاده من عبد الله ان النبي صلي الله عليه وسلم . قال ابب ثاب ا\n",
      "\n",
      " Verse 9:\n",
      "حدثنا ابن بشي بن عائشه رضي الله عنه انه عن النبي صلي الله علمه، ولم يسدي \" .\n",
      "\n",
      " Verse 10:\n",
      "حدثنا سحيد بن عبد الله بن محمد بن عمر، عن ابي شيعه بن محمد بن ديان، عن عبد الله بن اليعن، عن ابن مقرج، عن عبد الرحمن بن ابي شيبه، وابف، انه قال وامر القله الله من ابع بي حجر فه ولي علي الشكري علي بالهه الناي حالم الي التاعن حتي يعن فاسته الي خميت، ولا يبن اصحالن فهوع واو ان الصليه الصلاه فالحداء \" .\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e013a8",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith RNN inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bcf8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/3333836913.py:83: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ca5d3ef975403f9970b1aa16b24dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8153 | val_loss=2.2922 | val_ppl=9.90 | val_acc=0.3475\n",
      "Epoch 002 | train_loss=2.2519 | val_loss=2.0392 | val_ppl=7.68 | val_acc=0.4174\n",
      "Epoch 003 | train_loss=2.0799 | val_loss=1.8865 | val_ppl=6.60 | val_acc=0.4689\n",
      "Epoch 004 | train_loss=1.9620 | val_loss=1.7737 | val_ppl=5.89 | val_acc=0.4984\n",
      "Epoch 005 | train_loss=1.8814 | val_loss=1.7020 | val_ppl=5.49 | val_acc=0.5145\n",
      "Epoch 006 | train_loss=1.8288 | val_loss=1.6528 | val_ppl=5.22 | val_acc=0.5238\n",
      "Epoch 007 | train_loss=1.7896 | val_loss=1.6185 | val_ppl=5.05 | val_acc=0.5323\n",
      "Epoch 008 | train_loss=1.7613 | val_loss=1.5938 | val_ppl=4.92 | val_acc=0.5435\n",
      "Epoch 009 | train_loss=1.7382 | val_loss=1.5725 | val_ppl=4.82 | val_acc=0.5469\n",
      "Epoch 010 | train_loss=1.7199 | val_loss=1.5556 | val_ppl=4.74 | val_acc=0.5510\n",
      "Epoch 011 | train_loss=1.7044 | val_loss=1.5424 | val_ppl=4.68 | val_acc=0.5547\n",
      "Epoch 012 | train_loss=1.6916 | val_loss=1.5321 | val_ppl=4.63 | val_acc=0.5555\n",
      "Epoch 013 | train_loss=1.6809 | val_loss=1.5231 | val_ppl=4.59 | val_acc=0.5576\n",
      "Epoch 014 | train_loss=1.6716 | val_loss=1.5142 | val_ppl=4.55 | val_acc=0.5628\n",
      "Epoch 015 | train_loss=1.6638 | val_loss=1.5079 | val_ppl=4.52 | val_acc=0.5613\n",
      "Epoch 016 | train_loss=1.6566 | val_loss=1.5009 | val_ppl=4.49 | val_acc=0.5623\n",
      "Epoch 017 | train_loss=1.6504 | val_loss=1.4957 | val_ppl=4.46 | val_acc=0.5649\n",
      "Epoch 018 | train_loss=1.6446 | val_loss=1.4898 | val_ppl=4.44 | val_acc=0.5658\n",
      "Epoch 019 | train_loss=1.6389 | val_loss=1.4858 | val_ppl=4.42 | val_acc=0.5672\n",
      "Epoch 020 | train_loss=1.6349 | val_loss=1.4796 | val_ppl=4.39 | val_acc=0.5685\n",
      "Epoch 021 | train_loss=1.6309 | val_loss=1.4782 | val_ppl=4.38 | val_acc=0.5713\n",
      "Epoch 022 | train_loss=1.6265 | val_loss=1.4754 | val_ppl=4.37 | val_acc=0.5700\n",
      "Epoch 023 | train_loss=1.6225 | val_loss=1.4715 | val_ppl=4.36 | val_acc=0.5701\n",
      "Epoch 024 | train_loss=1.6194 | val_loss=1.4700 | val_ppl=4.35 | val_acc=0.5717\n",
      "Epoch 025 | train_loss=1.6161 | val_loss=1.4677 | val_ppl=4.34 | val_acc=0.5713\n",
      "Epoch 026 | train_loss=1.6136 | val_loss=1.4625 | val_ppl=4.32 | val_acc=0.5725\n",
      "Epoch 027 | train_loss=1.6110 | val_loss=1.4620 | val_ppl=4.31 | val_acc=0.5732\n",
      "Epoch 028 | train_loss=1.6081 | val_loss=1.4604 | val_ppl=4.31 | val_acc=0.5736\n",
      "Epoch 029 | train_loss=1.6061 | val_loss=1.4571 | val_ppl=4.29 | val_acc=0.5749\n",
      "Epoch 030 | train_loss=1.6038 | val_loss=1.4547 | val_ppl=4.28 | val_acc=0.5783\n",
      "Epoch 031 | train_loss=1.6018 | val_loss=1.4542 | val_ppl=4.28 | val_acc=0.5788\n",
      "Epoch 032 | train_loss=1.5999 | val_loss=1.4525 | val_ppl=4.27 | val_acc=0.5767\n",
      "Epoch 033 | train_loss=1.5983 | val_loss=1.4528 | val_ppl=4.27 | val_acc=0.5762\n",
      "Epoch 034 | train_loss=1.5968 | val_loss=1.4518 | val_ppl=4.27 | val_acc=0.5787\n",
      "Epoch 035 | train_loss=1.5953 | val_loss=1.4506 | val_ppl=4.27 | val_acc=0.5794\n",
      "Epoch 036 | train_loss=1.5933 | val_loss=1.4464 | val_ppl=4.25 | val_acc=0.5772\n",
      "Epoch 037 | train_loss=1.5918 | val_loss=1.4474 | val_ppl=4.25 | val_acc=0.5780\n",
      "Epoch 038 | train_loss=1.5903 | val_loss=1.4442 | val_ppl=4.24 | val_acc=0.5784\n",
      "Epoch 039 | train_loss=1.5894 | val_loss=1.4450 | val_ppl=4.24 | val_acc=0.5805\n",
      "Epoch 040 | train_loss=1.5880 | val_loss=1.4432 | val_ppl=4.23 | val_acc=0.5813\n",
      "Epoch 041 | train_loss=1.5868 | val_loss=1.4440 | val_ppl=4.24 | val_acc=0.5789\n",
      "Epoch 042 | train_loss=1.5857 | val_loss=1.4421 | val_ppl=4.23 | val_acc=0.5790\n",
      "Epoch 043 | train_loss=1.5847 | val_loss=1.4402 | val_ppl=4.22 | val_acc=0.5799\n",
      "Epoch 044 | train_loss=1.5834 | val_loss=1.4393 | val_ppl=4.22 | val_acc=0.5801\n",
      "Epoch 045 | train_loss=1.5829 | val_loss=1.4392 | val_ppl=4.22 | val_acc=0.5801\n",
      "Epoch 046 | train_loss=1.5816 | val_loss=1.4383 | val_ppl=4.21 | val_acc=0.5803\n",
      "Epoch 047 | train_loss=1.5806 | val_loss=1.4377 | val_ppl=4.21 | val_acc=0.5821\n",
      "Epoch 048 | train_loss=1.5796 | val_loss=1.4358 | val_ppl=4.20 | val_acc=0.5809\n",
      "Epoch 049 | train_loss=1.5790 | val_loss=1.4364 | val_ppl=4.21 | val_acc=0.5806\n",
      "Epoch 050 | train_loss=1.5776 | val_loss=1.4344 | val_ppl=4.20 | val_acc=0.5806\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_en\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "747f38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "ighilger said to is a copp to when ou tone of the kimah he revines norn that allah's mease of his hands wan oother, in the verions and the prophet that allah's apostla ghe hakim. when he restorr b. masima) him and said bet in the prayer the prepss of the chould lis the paso folldiad the toret for th\n",
      "\n",
      " Verse 2:\n",
      "in while it and canes it in the promertidite the anashir the people of was a bong the salithter had may allah said whirched has allah with entromen whot komen whent him, and one lowshing the prophet sayeay the upreation and the saces on the authorinnanna (theand has and plating and whos oolst and an\n",
      "\n",
      " Verse 3:\n",
      "abu muraishah in her and the proy of muse shaor the himpina, whore spait he said modet of the eravat in the last of the sowure of the messenger of al-asatahan is there (sayan a and will both and thet and the prophet ( ) come clive that he react of be tasit of allah's apostle sheash bettern corpave n\n",
      "\n",
      " Verse 4:\n",
      "the messenger of allah (may peacing as-said monas for am and has beed we gwe tore in the begir would and alther and then everinirg and then i said for the prophet in corchas of mested allah (the aide said till be gere derent unide (its reported that allah's messenger (may peace be upon him) was dish\n",
      "\n",
      " Verse 5:\n",
      "abu burim! the herdith'r give the messenger of allah ( ) beens is and contsons of them and saman (ill, and i whole trade you would his companions have were is me and ther in a meany ig bether the same mowss acunger forted wher. the proping with him in the mesardithiln ho (on hagitser. it reported th\n",
      "\n",
      " Verse 6:\n",
      "allah's. and ceming ho mostich the lace botle and he am a peleet to came to the prophet ( ) (gake to hammad buis had al-sabunthind the hans of the hard narrahed the prophet bul do belain bilad and have a man chat il as a shahed and while he of said he walla for his sand bectare of the mis messen exc\n",
      "\n",
      " Verse 7:\n",
      "abu hu madick and he had of anct would mentying their wat rantle. 'ummure while shis manter of the mister then wo of on the tomet the lerdengat of allah wheat the ward that the prophet ( ) said 'o mugated free. thets in tha ballatal in fallas of the fase ang him who tarabeled both has boin is has b.\n",
      "\n",
      " Verse 8:\n",
      "the prophet of allah ( ) a walla (de and the sere ichear, the evonn formant olbnish and conthay be avertrit in when the messenger of allah, has arred, misan has been nrianed to it on the apaci and chain of one of the moss in his (alli was dis foo rivendied isengy.\n",
      "\n",
      " Verse 9:\n",
      "the prophet ( ) ar to than about the allahed the puncernen and there wilm afformed you prayer (an in a peomed the sate al-qurar said some his got ther of is lead the some whing his prophet ( say allah ana munhar (ol and says the prophet and med i should fot and will qute of the eni has neat fint of \n",
      "\n",
      " Verse 10:\n",
      "the prophet what you had to i heard ehegurise would camel betings frahked and leaded i a would nest of for you said so suching wees and the prophet ( ) whife the qurhan mentirn word bi said, has allah's apostle (may peallising, for he with the to daid the hif they have pleating (whold of mas alfay) \n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_rnn, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5818ab3",
   "metadata": {},
   "source": [
    "### LSTM - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "574bf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(hadith_path, output_path, text_col, ft_ruta):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    dataset.set_split(\"train\")\n",
    "    train_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    dataset.set_split(\"val\")\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    dataset.set_split(\"test\")\n",
    "    test_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_modelo = fasttext.load_model(ft_ruta)\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_modelo)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(args.num_epochs)):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        val_ppl = math.exp(vloss)\n",
    "        train_state.setdefault(\"val_ppl\", []).append(val_ppl)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "                f\"| val_loss={vloss:.4f} | val_ppl={val_ppl:.2f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21033cc",
   "metadata": {},
   "source": [
    "Entrenamiento LSTM Hadith árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbe6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/1118507940.py:81: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a824ca49105242d6835a7c6882ad38a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.1909 | val_loss=1.5693 | val_ppl=4.80 | val_acc=0.5630\n",
      "Epoch 002 | train_loss=1.5308 | val_loss=1.3352 | val_ppl=3.80 | val_acc=0.6204\n",
      "Epoch 003 | train_loss=1.3720 | val_loss=1.2388 | val_ppl=3.45 | val_acc=0.6445\n",
      "Epoch 004 | train_loss=1.2929 | val_loss=1.1925 | val_ppl=3.30 | val_acc=0.6551\n",
      "Epoch 005 | train_loss=1.2431 | val_loss=1.1514 | val_ppl=3.16 | val_acc=0.6694\n",
      "Epoch 006 | train_loss=1.2089 | val_loss=1.1331 | val_ppl=3.11 | val_acc=0.6729\n",
      "Epoch 007 | train_loss=1.1834 | val_loss=1.1127 | val_ppl=3.04 | val_acc=0.6800\n",
      "Epoch 008 | train_loss=1.1633 | val_loss=1.0906 | val_ppl=2.98 | val_acc=0.6863\n",
      "Epoch 009 | train_loss=1.1472 | val_loss=1.0771 | val_ppl=2.94 | val_acc=0.6899\n",
      "Epoch 010 | train_loss=1.1333 | val_loss=1.0720 | val_ppl=2.92 | val_acc=0.6914\n",
      "Epoch 011 | train_loss=1.1218 | val_loss=1.0573 | val_ppl=2.88 | val_acc=0.6960\n",
      "Epoch 012 | train_loss=1.1115 | val_loss=1.0520 | val_ppl=2.86 | val_acc=0.6972\n",
      "Epoch 013 | train_loss=1.1024 | val_loss=1.0435 | val_ppl=2.84 | val_acc=0.6990\n",
      "Epoch 014 | train_loss=1.0948 | val_loss=1.0394 | val_ppl=2.83 | val_acc=0.6997\n",
      "Epoch 015 | train_loss=1.0884 | val_loss=1.0315 | val_ppl=2.81 | val_acc=0.7010\n",
      "Epoch 016 | train_loss=1.0816 | val_loss=1.0269 | val_ppl=2.79 | val_acc=0.7031\n",
      "Epoch 017 | train_loss=1.0771 | val_loss=1.0232 | val_ppl=2.78 | val_acc=0.7042\n",
      "Epoch 018 | train_loss=1.0718 | val_loss=1.0177 | val_ppl=2.77 | val_acc=0.7064\n",
      "Epoch 019 | train_loss=1.0669 | val_loss=1.0135 | val_ppl=2.76 | val_acc=0.7074\n",
      "Epoch 020 | train_loss=1.0624 | val_loss=1.0072 | val_ppl=2.74 | val_acc=0.7079\n",
      "Epoch 021 | train_loss=1.0585 | val_loss=1.0057 | val_ppl=2.73 | val_acc=0.7087\n",
      "Epoch 022 | train_loss=1.0553 | val_loss=1.0061 | val_ppl=2.74 | val_acc=0.7083\n",
      "Epoch 023 | train_loss=1.0523 | val_loss=1.0020 | val_ppl=2.72 | val_acc=0.7094\n",
      "Epoch 024 | train_loss=1.0489 | val_loss=1.0012 | val_ppl=2.72 | val_acc=0.7105\n",
      "Epoch 025 | train_loss=1.0458 | val_loss=0.9979 | val_ppl=2.71 | val_acc=0.7110\n",
      "Epoch 026 | train_loss=1.0433 | val_loss=0.9958 | val_ppl=2.71 | val_acc=0.7123\n",
      "Epoch 027 | train_loss=1.0402 | val_loss=0.9945 | val_ppl=2.70 | val_acc=0.7111\n",
      "Epoch 028 | train_loss=1.0382 | val_loss=0.9905 | val_ppl=2.69 | val_acc=0.7133\n",
      "Epoch 029 | train_loss=1.0356 | val_loss=0.9889 | val_ppl=2.69 | val_acc=0.7144\n",
      "Epoch 030 | train_loss=1.0336 | val_loss=0.9876 | val_ppl=2.68 | val_acc=0.7145\n",
      "Epoch 031 | train_loss=1.0317 | val_loss=0.9869 | val_ppl=2.68 | val_acc=0.7139\n",
      "Epoch 032 | train_loss=1.0302 | val_loss=0.9841 | val_ppl=2.68 | val_acc=0.7152\n",
      "Epoch 033 | train_loss=1.0279 | val_loss=0.9861 | val_ppl=2.68 | val_acc=0.7129\n",
      "Epoch 034 | train_loss=1.0259 | val_loss=0.9829 | val_ppl=2.67 | val_acc=0.7152\n",
      "Epoch 035 | train_loss=1.0239 | val_loss=0.9820 | val_ppl=2.67 | val_acc=0.7148\n",
      "Epoch 036 | train_loss=1.0228 | val_loss=0.9804 | val_ppl=2.67 | val_acc=0.7142\n",
      "Epoch 037 | train_loss=1.0215 | val_loss=0.9804 | val_ppl=2.67 | val_acc=0.7159\n",
      "Epoch 038 | train_loss=1.0194 | val_loss=0.9776 | val_ppl=2.66 | val_acc=0.7171\n",
      "Epoch 039 | train_loss=1.0187 | val_loss=0.9789 | val_ppl=2.66 | val_acc=0.7155\n",
      "Epoch 040 | train_loss=1.0174 | val_loss=0.9762 | val_ppl=2.65 | val_acc=0.7174\n",
      "Epoch 041 | train_loss=1.0159 | val_loss=0.9777 | val_ppl=2.66 | val_acc=0.7161\n",
      "Epoch 042 | train_loss=1.0145 | val_loss=0.9738 | val_ppl=2.65 | val_acc=0.7181\n",
      "Epoch 043 | train_loss=1.0136 | val_loss=0.9736 | val_ppl=2.65 | val_acc=0.7183\n",
      "Epoch 044 | train_loss=1.0126 | val_loss=0.9725 | val_ppl=2.64 | val_acc=0.7183\n",
      "Epoch 045 | train_loss=1.0112 | val_loss=0.9752 | val_ppl=2.65 | val_acc=0.7174\n",
      "Epoch 046 | train_loss=1.0102 | val_loss=0.9721 | val_ppl=2.64 | val_acc=0.7181\n",
      "Epoch 047 | train_loss=1.0093 | val_loss=0.9742 | val_ppl=2.65 | val_acc=0.7174\n",
      "Epoch 048 | train_loss=1.0082 | val_loss=0.9734 | val_ppl=2.65 | val_acc=0.7176\n",
      "Epoch 049 | train_loss=1.0008 | val_loss=0.9665 | val_ppl=2.63 | val_acc=0.7202\n",
      "Epoch 050 | train_loss=0.9995 | val_loss=0.9643 | val_ppl=2.62 | val_acc=0.7203\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_ar\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_arabic_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2ecc683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثنا محمد بن بشار، حدثنا يحيي، - وهو ابن سلمه - حدثنا ابي، عن عائشه الكني، قال زيد علي ابن عباس عن النبي صلي الله عليه وسلم وهو محرم وابو بكر السناد في شراد صاع الامت ليلت وتعالي سكره فانا مبتاع اليم جاريه وهو في صاحب من السام .\n",
      "\n",
      " Verse 2:\n",
      "حدثنا ابو بكر بن ابي شيبه، حدثنا وكيع، حدثنا الاعمش، عن ابي صالح، عن ابي هريره، ان رسول الله صلي الله عليه وسلم قال \" اللهم اني اعرج، عن ابوه اخبر، انما من اكثره باصلي المسجد \" . قال ابو عيسي هذا حديث حسن صحيح .\n",
      "\n",
      " Verse 3:\n",
      "حدثنا ابو كريب، حدثنا ابو الزناد، عن الاعمش، عن ابي وائل، عن سمعه، ان ابن عمر، قال دخل علي النبي صلي الله عليه وسلم ان اقبل هن كلما اعتق في الناس ليس لموت ما هو الوداك، ولا تدخل موسي الي الذين يضحك عن المسجد المسجد والصبح سوس فروع دنيا ووحل في الفتح بالمدينه، والمسطج العزود وراح ندي يا ابا الاسود ان\n",
      "\n",
      " Verse 4:\n",
      "حدثنا ابو بكر بن ابي شيبه، حدثنا ابو اسامه، عن الحسان المخزوم، عن ذراع، عن ابي الدرداء، عن ابي العاليه، عن ابيه، ان النبي صلي الله عليه وسلم قال \" بين بعلها او لا القاعه فكلا تمنا \" . قال ابو عيسي وفي الباب عن علي وابن عمر وسعيد بن جبير السمان وهو بعد روي الحنه وسعيد بن ابي سعيد وابي سعيد وانس وابي \n",
      "\n",
      " Verse 5:\n",
      "وحدثني زهير بن حرب، حدثنا عبد الواحد بن زياد، حدثنا عماره بن عبيد، حدثني ابو ايوب، عن ابي عمرو بن ميسره، عن ابي سلمه، عن ابي هريره، قال قال رسول الله صلي الله عليه وسلم \" ان الله ليستبعح المسلمه بينهم ثم امركم من المنكم \" .\n",
      "\n",
      " Verse 6:\n",
      "وحدثني حاج الحبي، عن مالك، عن عبد الله بن سعد بن ابي سليمان، عن جابر بن عبد الله، ان رسول الله صلي الله عليه وسلم قال \" ان يصبح لابي بللاء ان ابوا من حرمه في الجنه \" . قال ابو عيسي هذا حديث حسن صحيح . وهو قول احمد واسحاق .\n",
      "\n",
      " Verse 7:\n",
      "حدثنا سقد بن الحارث، حدثنا اسماعيل بن ابراهيم، عن عمرو بن معه، قال كنت مع رسول الله صلي الله عليه وسلم بمثل حديث المحلقه قال هذا الحديث ويوم \" ابوني علي النبي صلي الله عليه وسلم \" . قال ابو عيسي هذا حديث حسن صحيح . والعمل علي هذا عند اهل العلم من اصحاب النبي صلي الله عليه وسلم وجابر . وروي ابن المثن\n",
      "\n",
      " Verse 8:\n",
      "حدثنا محمد بن المثني، حدثنا عبد الله بن المبارك، اخبرنا ابو حازم، عن عبد الملك، عن ابي قيس، عن ابي سعيد الخدري، ان رسول الله صلي الله عليه وسلم قال \" ما اخذ عملك الي في اصحاب عليكم \" . قال قتل ابن رجاء عن النبي صلي الله عليه وسلم في حجاره . قال ابو عيسي هذا حديث حسن صحيح .\n",
      "\n",
      " Verse 9:\n",
      "وحدثنا عبد بن حميد، اخبرنا اسماعيل، عن عمرو بن الحارث، عن وعبه، الفزاري، عن امه بن مسهم، عن ابن عباس، قال كنا مع النبي صلي الله عليه وسلم يحدث رسول الله صلي الله عليه وسلم عن صلاه السلام فعلم علي ان ينشر علي بيت ثم اوسط سمعت عبد الرحمن بن عبد الله انه سمع النبي صلي الله عليه وسلم يقول \" ايي عمر علي \n",
      "\n",
      " Verse 10:\n",
      "حدثنا عبد الله بن محمد بن موسي الرمالي، حدثنا ابو معاويه، عن يحيي بن سعيد، عن ابي البيمان، عن عكرمه، عن ابن عباس، قال جاءت يا رسول الله صلي الله عليه وسلم راع الامام لم يفعله المشربين .\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a805e4",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith LSTM inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d39017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/tmp/ipykernel_10394/1118507940.py:81: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(args.num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df0462d505c4d2cb06257b75c282aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.2849 | val_loss=1.6955 | val_ppl=5.45 | val_acc=0.5145\n",
      "Epoch 002 | train_loss=1.6425 | val_loss=1.4027 | val_ppl=4.07 | val_acc=0.5843\n",
      "Epoch 003 | train_loss=1.4422 | val_loss=1.2827 | val_ppl=3.61 | val_acc=0.6168\n",
      "Epoch 004 | train_loss=1.3442 | val_loss=1.2171 | val_ppl=3.38 | val_acc=0.6368\n",
      "Epoch 005 | train_loss=1.2833 | val_loss=1.1733 | val_ppl=3.23 | val_acc=0.6477\n",
      "Epoch 006 | train_loss=1.2417 | val_loss=1.1424 | val_ppl=3.13 | val_acc=0.6551\n",
      "Epoch 007 | train_loss=1.2113 | val_loss=1.1198 | val_ppl=3.06 | val_acc=0.6613\n",
      "Epoch 008 | train_loss=1.1876 | val_loss=1.1052 | val_ppl=3.02 | val_acc=0.6654\n",
      "Epoch 009 | train_loss=1.1690 | val_loss=1.0875 | val_ppl=2.97 | val_acc=0.6693\n",
      "Epoch 010 | train_loss=1.1533 | val_loss=1.0795 | val_ppl=2.94 | val_acc=0.6726\n",
      "Epoch 011 | train_loss=1.1402 | val_loss=1.0682 | val_ppl=2.91 | val_acc=0.6751\n",
      "Epoch 012 | train_loss=1.1294 | val_loss=1.0599 | val_ppl=2.89 | val_acc=0.6776\n",
      "Epoch 013 | train_loss=1.1200 | val_loss=1.0556 | val_ppl=2.87 | val_acc=0.6784\n",
      "Epoch 014 | train_loss=1.1116 | val_loss=1.0504 | val_ppl=2.86 | val_acc=0.6789\n",
      "Epoch 015 | train_loss=1.1046 | val_loss=1.0427 | val_ppl=2.84 | val_acc=0.6810\n",
      "Epoch 016 | train_loss=1.0980 | val_loss=1.0415 | val_ppl=2.83 | val_acc=0.6800\n",
      "Epoch 017 | train_loss=1.0918 | val_loss=1.0362 | val_ppl=2.82 | val_acc=0.6823\n",
      "Epoch 018 | train_loss=1.0867 | val_loss=1.0306 | val_ppl=2.80 | val_acc=0.6853\n",
      "Epoch 019 | train_loss=1.0823 | val_loss=1.0247 | val_ppl=2.79 | val_acc=0.6863\n",
      "Epoch 020 | train_loss=1.0773 | val_loss=1.0251 | val_ppl=2.79 | val_acc=0.6865\n",
      "Epoch 021 | train_loss=1.0734 | val_loss=1.0182 | val_ppl=2.77 | val_acc=0.6898\n",
      "Epoch 022 | train_loss=1.0697 | val_loss=1.0180 | val_ppl=2.77 | val_acc=0.6879\n",
      "Epoch 023 | train_loss=1.0663 | val_loss=1.0138 | val_ppl=2.76 | val_acc=0.6902\n",
      "Epoch 024 | train_loss=1.0627 | val_loss=1.0097 | val_ppl=2.74 | val_acc=0.6909\n",
      "Epoch 025 | train_loss=1.0602 | val_loss=1.0072 | val_ppl=2.74 | val_acc=0.6918\n",
      "Epoch 026 | train_loss=1.0574 | val_loss=1.0087 | val_ppl=2.74 | val_acc=0.6917\n",
      "Epoch 027 | train_loss=1.0546 | val_loss=1.0080 | val_ppl=2.74 | val_acc=0.6906\n",
      "Epoch 028 | train_loss=1.0465 | val_loss=0.9996 | val_ppl=2.72 | val_acc=0.6952\n",
      "Epoch 029 | train_loss=1.0445 | val_loss=1.0022 | val_ppl=2.72 | val_acc=0.6933\n",
      "Epoch 030 | train_loss=1.0429 | val_loss=0.9980 | val_ppl=2.71 | val_acc=0.6932\n",
      "Epoch 031 | train_loss=1.0416 | val_loss=0.9967 | val_ppl=2.71 | val_acc=0.6951\n",
      "Epoch 032 | train_loss=1.0406 | val_loss=0.9950 | val_ppl=2.70 | val_acc=0.6959\n",
      "Epoch 033 | train_loss=1.0391 | val_loss=0.9950 | val_ppl=2.70 | val_acc=0.6954\n",
      "Epoch 034 | train_loss=1.0385 | val_loss=0.9959 | val_ppl=2.71 | val_acc=0.6955\n",
      "Epoch 035 | train_loss=1.0341 | val_loss=0.9923 | val_ppl=2.70 | val_acc=0.6961\n",
      "Epoch 036 | train_loss=1.0330 | val_loss=0.9920 | val_ppl=2.70 | val_acc=0.6960\n",
      "Epoch 037 | train_loss=1.0321 | val_loss=0.9906 | val_ppl=2.69 | val_acc=0.6966\n",
      "Epoch 038 | train_loss=1.0319 | val_loss=0.9902 | val_ppl=2.69 | val_acc=0.6973\n",
      "Epoch 039 | train_loss=1.0312 | val_loss=0.9905 | val_ppl=2.69 | val_acc=0.6959\n",
      "Epoch 040 | train_loss=1.0308 | val_loss=0.9893 | val_ppl=2.69 | val_acc=0.6966\n",
      "Epoch 041 | train_loss=1.0300 | val_loss=0.9903 | val_ppl=2.69 | val_acc=0.6972\n",
      "Epoch 042 | train_loss=1.0297 | val_loss=0.9889 | val_ppl=2.69 | val_acc=0.6975\n",
      "Epoch 043 | train_loss=1.0289 | val_loss=0.9894 | val_ppl=2.69 | val_acc=0.6972\n",
      "Epoch 044 | train_loss=1.0281 | val_loss=0.9908 | val_ppl=2.69 | val_acc=0.6968\n",
      "Epoch 045 | train_loss=1.0262 | val_loss=0.9890 | val_ppl=2.69 | val_acc=0.6974\n",
      "Epoch 046 | train_loss=1.0259 | val_loss=0.9876 | val_ppl=2.68 | val_acc=0.6978\n",
      "Epoch 047 | train_loss=1.0253 | val_loss=0.9874 | val_ppl=2.68 | val_acc=0.6975\n",
      "Epoch 048 | train_loss=1.0254 | val_loss=0.9873 | val_ppl=2.68 | val_acc=0.6979\n",
      "Epoch 049 | train_loss=1.0248 | val_loss=0.9878 | val_ppl=2.69 | val_acc=0.6980\n",
      "Epoch 050 | train_loss=1.0237 | val_loss=0.9857 | val_ppl=2.68 | val_acc=0.6979\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_en\",\n",
    "                                                 ft_ruta=\"../src/modelos/fasttext_english_busqueda_semantica.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cf2f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "abu sa'id 'abdullah deporited to the prophet and said, o allah's apostle! do you think having in a seven person should say from his head from blanken, and the prayers eyedittery.\n",
      "\n",
      " Verse 2:\n",
      "abu huraira said that the heavens were with allah's messenger (may peace be upon him), then i was the committed the traditions of the prophet saying, in the name of assufe is a return of the house) where. he got up on the sallah i were until he was it desching by the signs of allah.' umar said 'abu \n",
      "\n",
      " Verse 3:\n",
      "allah's apostle said, do you go and be in it and he has gold collected by saying the prayer to the reventioneror. in the same day and granted the same san musalab b. abu umama and it was sitting with allah's messenger (may peace be upon him), so he called the prophet by a prevented away the prevente\n",
      "\n",
      " Verse 4:\n",
      "abu huraira allah's apostle said, indeed with the doubt.\n",
      "\n",
      " Verse 5:\n",
      "allah's apostle was leading the actratcessous prayers in charity. abu bakr came to the book and in the side of the dave to the last upon it. the prophet said, a man is not completely.\n",
      "\n",
      " Verse 6:\n",
      "abu hurairah narrated that the messenger of allah said like the riding of forty rak'ahs were entersed before him he would stand with allah, they perform hajj (and sins) for seven part of the house of allah's apostle? the messenger of allah ( ) as saying allah is the grave (of his head of prophet) an\n",
      "\n",
      " Verse 7:\n",
      "anas b. malik reported i heard himar said the messenger of allah (may peace be upon him) was forbidden by the divine shrouded the they allah would make the ansar or a door give him anyone in his son of his stones. he said 'and in the two rak at place to the mountain of dates or beard and daadi earne\n",
      "\n",
      " Verse 8:\n",
      "abu huraira the prophet said, i seek refuge in allah for the month of religion, have you not and approveration.\n",
      "\n",
      " Verse 9:\n",
      "the achas is that which i had ennered and delivering us to a discide to insertents and the condition is from dream in some water.\n",
      "\n",
      " Verse 10:\n",
      "the same hadith has been transmitted by 'abdullah bin abi asks, from the messenger of allah ( ) sufficed (to the mosque), the messenger of allah ( ) used to get the back of the state of inside to the truth and the earth of her men continued hold and umar (i.e. he was from jahid) and when his apostle\n"
     ]
    }
   ],
   "source": [
    "nuevos_versos(model_lstm, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29436c2",
   "metadata": {},
   "source": [
    "Realizaremos las comparaciones y aclararemos nuestras conclusiones en la documentación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
