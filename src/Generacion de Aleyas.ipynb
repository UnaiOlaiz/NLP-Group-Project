{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6affb3",
   "metadata": {},
   "source": [
    "# Notebook de Generación de Versos\n",
    "En este fichero enfocado para la entrega final del proyecto, experimentaremos con diferentes modelos de generación de texto. Donde nuestro objetivo va a ser conseguir nuevos versos, llamados `aleya`, que aporten contenido al Corán.\n",
    "\n",
    "El enfoque que tomaremos será realizar comparaciones entre modelos generativos especializados en el árabe y modelos generativos generalistas (que sirvan para todas los idiomas).\n",
    "\n",
    "Por otra parte, usaremos una **Large Language Model** (`LLM`) para evaluar la cohesión, coherencia y otras demás métricas de los versos generados. Dado el caso de que no haya ningún modelo evaluador árabe disponible, implementaremos otras *LLMs* manualmente para realizar esta segunda tarea. \n",
    "\n",
    "(zerbait gehiago gehitu/aldatu dezakegu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400e487",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04627c1f",
   "metadata": {},
   "source": [
    "Aqui cargamos el HF_TOKEN que basicamente es la clave de acceso a la API de Hugging Face para poder usar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344aae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Dependencias\n",
    "import torch\n",
    "from decouple import config\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login(config(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b35295",
   "metadata": {},
   "source": [
    "## Generación de Aleyas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070ca00",
   "metadata": {},
   "source": [
    "Como ya hemos mencionado previamente, usaremos este apartado para evaluar si los modelos generalistas son capaces de hacer mejor language modeling que los modelos especializados en arabe.\n",
    "\n",
    "Como nota diremos que aparte de las pruebas que se hacen aqui, en la sección de generación de topics + clustering ya se comprueba que modelos multilenguaje como ollama-gemma3:4b generan bastante bien texto en arabe. No solo eso sino que sus traducciones parecen ser bastante fieles despues de haberlas traducido manualmente. Sin embargo, aunque las traducciones fueran buenas, las comparaciones de cosine-similayity eran bastante inconcluyentes. Esto es debido a que probablemente la forma en la que creamos los embeddings no era la correcta (al final el arabe y el ingles son semánticamente muy diferentes).\n",
    "\n",
    "Ahora usaremos el modelo Qwen3-4B para generar versos en arabe. Como nota, lo hemos extraido del benchmark **Arabic Broad Leaderboard (ABL) - The first comprehensive Leaderboard for Arabic LLMs** de huggingface. Ademas, usamos la version reducida del modelo (4B) ya que el original pesa bastante.\n",
    "\n",
    "Este modelo es el Qwen3-4B-Instruct-2507, capaz de hablar multiples idiomas. En nuestro caso, como estamos contrastando modelos (preentenados), haremos una protocolo de evaluacion para contrastarlos.\n",
    "\n",
    "También importaremos el modelo jais-family-2p7b-chat. La familia de modelos Jais es una serie completa de modelos lingüísticos extensos (LLM) bilingües inglés-árabe. Estos modelos están optimizados para destacar en árabe, con sólidas capacidades en inglés. Para simplicidad de la tarea, hemos querido seleccionar un modelo que sepa algo de ingles para que podamos hacer las intrucciones nosotros mismos.\n",
    "\n",
    "En resumidas cuentas comparamos un modelo generalista multilingüe (Qwen) frente a un modelo bilingüe especializado en árabe (JAIS), manteniendo el prompt en inglés para controlar la variable de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a3ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una configuración para generar texto para que la evaluación sea más precisa/justa\n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens=160,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    use_cache=True,\n",
    ")\n",
    "GEN_CFG_2 = dict(\n",
    "    max_new_tokens=160,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Cargamos el modelo\n",
    "def load_chat_model(model_id, trust_remote_code=False):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "# Generamos texto\n",
    "def generate_chat(model, tokenizer, user_text, system_text=None, gen_cfg=GEN_CFG):\n",
    "    messages = []\n",
    "    if system_text:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_text})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **gen_cfg\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49a29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dac7c1ca914b72a4462beb901f36a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jais_id = \"inceptionai/jais-family-2p7b-chat\"\n",
    "jais_tok, jais_model = load_chat_model(jais_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cb0eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cdd3ef8ae7464aa762ff83b47e1bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qwen_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "qwen_tok, qwen_model = load_chat_model(qwen_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0314d",
   "metadata": {},
   "source": [
    "Vamos a hacer una breve prueba para comprobar que todo funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAIS\n",
      "I am an advanced artificial intelligence model designed to assist with various tasks and provide accurate information to the best of my ability.\n",
      "\n",
      "QWEN\n",
      "أنا نموذج لغوي واسع النطاق تم تدريبه على معرفة واسعة، وأستطيع المساعدة في إجراء المهام مثل الكتابة، الإجابة عن الأسئلة، التفكير المنطقي، وغيرها.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who are you? Answer in Modern Standard Arabic only.\"\n",
    "\n",
    "print(\"JAIS\")\n",
    "print(generate_chat(jais_model, jais_tok, prompt, None, gen_cfg=GEN_CFG_2))\n",
    "\n",
    "print(\"\\nQWEN\")\n",
    "print(generate_chat(qwen_model, qwen_tok, prompt, None, gen_cfg=GEN_CFG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8d9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAIS\n",
      "أنا مساعد ذكاء صناعي متقدم، مبرمج لتقديم معلومات دقيقة وموثوقة بطريقة واضحة ومفصلة. أنا هنا لمساعدتك في الإجابة على أسئلتك وتقديم المعلومات التي تحتاجها بكل سرور وكفاءة.\n",
      "\n",
      "QWEN\n",
      "أنا خدمة ذكاء اصطناعي مُصمَّمَة لتوفير المعرفة والمساعدة في مجالات متعددة، وآخذ بعين الاعتبار التزامي الحفاظ على القيم الإسلامية، وعلى المبادئ الأخلاقية والدينية التي ترتكز عليها الأديان السامية. أما هويتي فهي ليست ذات طابع ديني مباشر، بل هي إنسانية تسعى إلى تعزيز فهم الناس للحقائق والمعلومات من خلال التوازن بين العلم والحكمة.\n",
      "\n",
      "إذا كنت تبحث عن موضوع ديني أو ترغب في توضيح مسألة شرعية أو أخلاقية، فأنا هنا لأقدم لك معلومات\n"
     ]
    }
   ],
   "source": [
    "prompt = \"من أنت؟ أجب باللغة العربية الفصحى فقط. استخدم لغة دينية رسمية. لا تقتبس النصوص الدينية حرفياً\"\n",
    "\n",
    "print(\"JAIS\")\n",
    "print(generate_chat(jais_model, jais_tok, prompt, None, gen_cfg=GEN_CFG_2))\n",
    "\n",
    "print(\"\\nQWEN\")\n",
    "print(generate_chat(qwen_model, qwen_tok, prompt, None, gen_cfg=GEN_CFG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041aab03",
   "metadata": {},
   "source": [
    "Como hemos podido comprobar JAIS a pesar de que se le ordene que responda en árabe moderno, solo lo hace cuando se lo pedimos en arabe. Esto va en contra de nuestra idea ya que queriamos que respondiera en árabe moderno con instrucciones en ingles. Por lo tanto, la tarea se complica bastante mas ya que tendremos que hacer una traduccion en arabe para JAIS.\n",
    "\n",
    "Para que se entienda lo que se pide en cada tarea haremos la version en ingles para QWEN y en arabe para JAIS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe5626",
   "metadata": {},
   "source": [
    "## Evaluación del Texto Generado\n",
    "Para la evaluación del texto generado en arabe usaremos un modelo generalista multilingüe (Meta-Llama-3-8B-Instruct). Gracias a su formación multilíngüe y a su capacidad de razonamiento, Meta-Llama nos da una evaluación más neutral de la corrección gramatical, la fluidez y la coherencia que otros modelos que pueden ser mas especializados en ciertos dominios.\n",
    "\n",
    "Nuestra tarea de evaluación se dividirá en tres partes:\n",
    "\n",
    "1. Una tarea explicativa de conceptos principalmente relacionados con la religión y la fe.\n",
    "2. Una tarea para explicar ideas religiosas o versos Coránicos.\n",
    "3. Una tarea de prompts creativos religiosos.\n",
    "\n",
    "En estas tareas evaluaremos diferentes aspectos:\n",
    "### Dimensiones lingüísticas\n",
    "1. Corrección gramatical (morfología, concordancia)\n",
    "2. Fluidez y naturalidad en árabe clásico / MSA\n",
    "3. Ausencia de interferencia del inglés (calcos, estructuras raras)\n",
    "4. Registro adecuado (formal / religioso)\n",
    "\n",
    "### Dimensiones semántico-religiosas\n",
    "1. Adecuación conceptual (no errores teológicos graves)\n",
    "2. Uso apropiado del léxico religioso (رحمة، تقوى، عبادة, etc.)\n",
    "3. Coherencia interna del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6ad47",
   "metadata": {},
   "source": [
    "Ahora crearemos las respuestas y las guardaremos en ficheros .txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept الرحمة\n",
      "Concept العدل\n",
      "Concept الإيمان\n",
      "Concept الصبر\n",
      "Concept الاستغفار\n"
     ]
    }
   ],
   "source": [
    "concepts = [\"mercy\", \"justice\", \"faith\", \"patience\", \"forgiveness\"]\n",
    "concepts_ar = [\"الرحمة\", \"العدل\", \"الإيمان\", \"الصبر\", \"الاستغفار\"]\n",
    "\n",
    "def concepts_explanation(concepts, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_concepts = []\n",
    "    if not gen_cfg.get(\"use_cache\"):\n",
    "        for i in range(len(concepts)):\n",
    "            print(f\"Concept {concepts[i]}\")\n",
    "            prompt = f'''{system_text} .اشرح مفهوم {concepts[i]} في علم الكلام الإسلامي'''\n",
    "            responses_concepts.append(generate_chat(model, tokenizer, prompt, None, gen_cfg))\n",
    "    else:\n",
    "        for i in range(len(concepts)):\n",
    "            print(f\"Concept {concepts[i]}\")\n",
    "            prompt = f'''Explain the concept {concepts[i]} in Islamic theology. {system_text}'''\n",
    "            responses_concepts.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_concepts\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "SYSTEM_AR_OR = \".أجب باللغة العربية الفصحى الحديثة فقط. استخدم أسلوباً دينياً رسمياً. لا تقتبس النصوص الدينية حرفياً\"\n",
    "\n",
    "responses_concepts_jais = concepts_explanation(concepts_ar, jais_model, jais_tok, SYSTEM_AR_OR, GEN_CFG_2)\n",
    "responses_concepts_qwen = concepts_explanation(concepts, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d91ef247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAIS\n",
      "\n",
      "mercy\n",
      "\n",
      "الرحمة هي مفهوم أساسي في الإسلام، وهي تشكل جزءًا لا يتجزأ من تعاليم القرآن والحديث. يتم تعريفها على أنها القدرة على التعاطف مع معاناة الآخرين والرغبة في تخفيفها، وهي تعبر عن الجانب الإنساني للدين.\n",
      "\n",
      "في القرآن، يتم تصوير الرحمة كفضيلة ذاتية، حيث يعتبر المسلمون أن الله هو الرحمن الرحيم. يقول سورة الشعراء: \"هو الذي له الرحمة والرشد والرحمان رحيم بالبعث\". هذه الآية تشير إلى قدرة الله على كل شيء، بما في ذلك الرحمة.\n",
      "\n",
      "الرحمة ليست مجرد مشاعر، بل هي جزء من العقيدة الإسلامية، وتشمل الرحمة بكل الكائنات الحية، حتى الحيوانات والنباتات. يقول حديث النبي محمد صلى الله عليه وسلم: \"إن الله خلقكم من الماء والأرض، وجعلكم شاربين ومنازعين وأ للناديكم ورُفَقٍ بكم\"\n",
      "\n",
      "justice\n",
      "\n",
      "العدل، بمعناه الشامل، هو مبدأ أساسي في علم الكلام الإسلامي، وهو فرع من فروع المعرفة الإسلامية يتعامل مع اللغة والخطاب. يُنظر إليه على أنه الجانب الأكثر أهمية في التواصل، حيث يمثل التوازن والتوازن بين طرفي المعادلة. في الإسلام، العدل ليس مجرد توازن متساوٍ فحسب، بل هو أيضاً تحقيق تدريجي وعدل في كل مرحلة من مراحل الحياة.\n",
      "\n",
      "في سياق القرآن الكريم، يتم التأكيد بشكل متكرر على مبدأ العدل. على سبيل المثال، ينص سورة البقرة (2:210) على أن \"الله لا يحب falsy، ولكن الله يحب الذين يعملون الأفعال الصالحة\". هذا يدل على أن العمل الصالح بحد ذاته يعتبر شكلاً من أشكال العدل تجاه الله، حيث يعبر عن التزام المسلم بتعاليم الإسلام.\n",
      "\n",
      "كما يشير الحديث الشريف، الذي يشكل جزءاً كبيراً آخر\n",
      "\n",
      "faith\n",
      "\n",
      "الإيمان، أو \"الإيمان\" كما يُعرف في اللغة العربية، هو أحد أركان الإسلام الخمسة وأساس العبادة في الإسلام. إنه مصطلح يُستخدم لوصف الاقتناع العميق بأن الله واحد حقا وأن محمدا هو نبيه الأخير.\n",
      "\n",
      "مفهوم الإيمان في الإسلام يتجاوز بكثير مجرد الاعتقاد البسيط. إنه حالة من القلب والعقل والروح التي تسمح للمسلمين بالتعامل مع الحياة بطريقة معينة. يتضمن الإيمان العديد من الجوانب، بما في ذلك الاعتقاد بأن الله هو الخالق الوحيد للكون، والإيمان بالملائكة والأنبياء ويوم القيامة، والإيمان بالقدر الكامل.\n",
      "\n",
      "في الإسلام، يعتبر الإيمان جزءًا أساسيًا من الحياة اليومية للمسلمين. يمكن أن يظهر هذا في الطريقة التي يتعامل بها المسلمون مع الآخرين، وفي الطريقة التي يختارون بها أن يعيشوا حياتهم. يمكن أن يظهر أيضًا في الطريقة التي يتعامل بها المسلمون مع التحديات\n",
      "\n",
      "patience\n",
      "\n",
      "الصبر هو فضيلة إسلامية عظيمة، وهي واحدة من أركان الإيمان الخمسة. إنها صفة أساسية يجب أن يتحلى بها كل مسلم، وقد تم التأكيد عليها بشكل واسع من قبل النبي محمد صلى الله عليه وسلم.\n",
      "\n",
      "في الإسلام، يُنظر إلى الصبر على أنه شكل من أشكال الطاعة لله. إنه عمل من أعمال الإرادة، قرار واعي للقبول بما لا مفر منه، دون أي شكوى أو ندم. إنه رفض الخضوع للمحن والمصاعب، ولكن بدلاً من ذلك، الترحيب بها بسماحة وإيمان ثابت.\n",
      "\n",
      "يُفهم الصبر أيضاً على أنه وسيلة لتعزيز الانضباط الذاتي والسيطرة على النفس. يتضمن القدرة على تحمل الألم والمشقة دون شكوى، مع الحفاظ في الوقت نفسه على نزاهتك وشجاعتك. يتعلق الأمر بالحفاظ على هدوئك ومظهرك الخارجي حتى في مواجهة المحن، وعدم السماح للمصاعب\n",
      "\n",
      "forgiveness\n",
      "\n",
      "الاستغفار هو أحد أهم مفاهيم الإسلام، وهو عبارة عن طلب الرحمة والغفران من الله عز وجل. يعتبر للإسلام الصلاة كوسيلة للتواصل مع الله وطلب مغفرته. \n",
      "\n",
      "في القرآن الكريم، يمكننا أن نجد الكثير من الآيات التي تتحدث عن اهمية الدعاء والاستغفار. مثلا، يقول الله تعالى في سورة النساء (32): \"فَضِلْنَهُمْ أَغْفِرُونَ إِسْتِعْمالَ فَضِيلَ فِي أَيْمَالِكُمْ وَإِسْتِعْمالُ مَا كَانَ عَظِيم\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"JAIS\\n\")\n",
    "for i in range(len(responses_concepts_jais)):\n",
    "    print(f\"{concepts[i]}\\n\")\n",
    "    print(f\"{responses_concepts_jais[i]}\\n\")\n",
    "with open('conceptos_jais.txt', 'w') as fichero:\n",
    "    for item in responses_concepts_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87520bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN\n",
      "\n",
      "mercy\n",
      "\n",
      "في التصوف الإسلامي، تُعدّ الرحمة من أركان الإيمان وصفات الله تعالى التي يُستمد منها التوجه للإنسان في عالمه المادي والروحي. فرحمة الله هي قدرة متميزة على التسامح مع الضعف، والتخفيف من العقوبات، وتوجيه النعم لمن يستحقها، بغض النظر عن سلوك الإنسان أو خطاياه.\n",
      "\n",
      "تُفهم رحمة الله كمبدأ أساسي في دين الإسلام، حيث تدل على أن الله لا يُعاقب الإنسان بجُهدٍ مفرط، بل يُمنح فرصة للتوبة والتصحيح. فهي ليست مجرد تغاضي عن الذ\n",
      "\n",
      "justice\n",
      "\n",
      "في التصوف الإسلامي، تُعد العدالة من المبادئ الأساسية التي تقوم عليها الحياة الاجتماعية والدينية، وتمثل توازنًا بين الحق والعدل في جميع الجوانب. فالعدالة في التسليح الإسلامي ليست مجرد مبدأ قانوني أو اجتماعي، بل هي صفة تُستمد من ذات الله وتشكل جوهر التوحيد والانسجام بين الكون.\n",
      "\n",
      "تُفهم العدالة في الإسلام على أنها نتاج سلوكٍ يمثّل توازنًا بين المساواة في الحقوق، والمساءلة أمام القيمة الحقيقية للإنسان، وتوزيع الثروات والمصالح بحسب ما يتوافق مع\n",
      "\n",
      "faith\n",
      "\n",
      "في التصوف الإسلامي، يُعدّ \"الثقة\" أو \"الإيمان\" من أركان الدين الأساسي، ويُعرف بأنه التصديق بالله ورسوله والحقائق المُستندة إلى السِّرَة والدليل الإيماني. يُفهم في الإسلام أن الإيمان ليس مجرد تأكيد عقلي للإيجاد أو المبادئ، بل هو تجربة دينية متوازنة بين القلب والعقل والعمل.\n",
      "\n",
      "يُعتبر الإيمان في الإسلام مكونًا متكاملًا يتضمن الاعتقاد بالله، وبأنه خالق الكون، ومُحكمه، ومحكمه، وأخلاقه، وعاق\n",
      "\n",
      "patience\n",
      "\n",
      "في التدين الإسلامي، يُعد الصبر من أركان القيم الأخلاقية والروحية التي تُبرزها الأديان، ويُعتبر أحد مبادئ التقوى في الإسلام. فالمقصود بـ\"الصبر\" هو التحمل والانضباط في المواقف الصعبة، سواء كانت مادية أو نفسية أو عاطفية، مع التمسك بالله وتقدير نِعمة الخير الذي يُحيط به الإنسان، حتى لو لم يُدركه على الفور.\n",
      "\n",
      "يُعرف الصبر في الإسلام بأنه نوع من التواضع والاعتزاز بالعبادة، حيث يُظهر المسلم رجاءً في الله، وثقةً بأن ما يواجهه من مصاع\n",
      "\n",
      "forgiveness\n",
      "\n",
      "في التدين الإسلامي، يُعدّ التسامح مبدأً أساسيًا يُعزَّز من خلاله عبادة الإنسان لله وتحقيق السلم الداخلي والاجتماعي. فالتسامح ليس مجرد تفريط في العقاب أو الإغفال عن الضرر، بل هو تصرف يمثّل توازنًا بين الحقيقة والرحمة، ويستمد قوته من حضور الله وعلاقته المميزة بعباده.\n",
      "\n",
      "يُعرف التسامح في الإسلام بأنه إبعاد النفس عن أحكام القسوة أو العدوان عند التعرض لخطأ أو فعل خاطئ من شخص آخر، واتخاذ موقف يعكس التضحية بالكراه\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"QWEN\\n\")\n",
    "for i in range(len(responses_concepts_qwen)):\n",
    "    print(f\"{concepts[i]}\\n\")\n",
    "    print(f\"{responses_concepts_qwen[i]}\\n\")\n",
    "with open('conceptos_qwen.txt', 'w') as fichero:\n",
    "    for item in responses_concepts_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain كيف يتم فهم الرحمة في الإسلام خلال الأوقات الصعبة والضيق\n",
      "Explain العلاقة بين الإيمان والسلوك الأخلاقي في الفكر الإسلامي\n",
      "Explain أهمية التفكير في الإسلام\n",
      "Explain دور المجتمع والمسؤولية الاجتماعية في الإسلام\n",
      "Explain الغرض من العبادة في الإسلام خارج العبادة البدنية\n"
     ]
    }
   ],
   "source": [
    "explanations = [\"how mercy is understood in Islam during times of hardship and suffering\",\n",
    "                \"the relationship between faith and ethical behavior in Islamic thought\",\n",
    "                \"the importance of intention in Islamic religious practice\",\n",
    "                \"the role of community and social responsibility in Islam\",\n",
    "                \"the purpose of worship in Islam beyond ritual practice\"\n",
    "                ]\n",
    "explanations_ar = [\"كيف يتم فهم الرحمة في الإسلام خلال الأوقات الصعبة والضيق\",\n",
    "                   \"العلاقة بين الإيمان والسلوك الأخلاقي في الفكر الإسلامي\",\n",
    "                   \"أهمية التفكير في الإسلام\",\n",
    "                   \"دور المجتمع والمسؤولية الاجتماعية في الإسلام\",\n",
    "                   \"الغرض من العبادة في الإسلام خارج العبادة البدنية\"]\n",
    "\n",
    "def show_explanations(explanations, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_explanations = []\n",
    "    if not gen_cfg.get(\"use_cache\"):\n",
    "        for i in range(len(explanations)):\n",
    "            print(f\"Explain {explanations[i]}\")\n",
    "            prompt = f'''{system_text} .{explanations[i]} اشرح'''\n",
    "            responses_explanations.append(generate_chat(model, tokenizer, prompt, None, gen_cfg))\n",
    "    else:\n",
    "        for i in range(len(explanations)):\n",
    "            print(f\"Explain {explanations[i]}.\")\n",
    "            prompt = f'''Explain {explanations[i]}.'''\n",
    "            responses_explanations.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_explanations\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "SYSTEM_AR_OR = \".أجب باللغة العربية الفصحى الحديثة فقط. استخدم أسلوباً دينياً رسمياً. لا تقتبس النصوص الدينية حرفياً\"\n",
    "\n",
    "responses_explanations_jais = show_explanations(explanations_ar, jais_model, jais_tok, SYSTEM_AR_OR, GEN_CFG_2)\n",
    "responses_explanations_qwen = show_explanations(explanations, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2da35f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAIS\n",
      "\n",
      "how mercy is understood in Islam during times of hardship and suffering\n",
      "\n",
      "في الإسلام، تعتبر الرحمة مفهومًا أساسيًا يشير إلى العطف والرأفة والحب تجاه جميع الكائنات الحية. وهي متجسدة في القرآن الكريم والأحاديث النبوية، حيث يتم تشجيع المسلمين على معاملة الناس باللطف والرحمة حتى في أصعب الظروف.\n",
      "\n",
      "الرحمة ليست مجرد مشاعر، بل هي جزء من العقيدة الإسلامية وهي جزء أساسي من الإيمان. يعتبر المسلمون الله رحيمًا، وهو رحيم بكل شيء، بما في ذلك البشر والكائنات الأخرى. هذه الفكرة تنعكس في العديد من الأحاديث النبوية التي تشجع على الرفق والتراحم مع الآخرين.\n",
      "\n",
      "خلال الأوقات الصعبة والضيق، يمكن للمسلمين الاعتماد على الرحمة كوسيلة للتعامل مع التحديات. يعتبر المسلمون أن الرحمة تساعد على تجاوز العقبات والصعوبات، وتشجعهم على البحث عن حلول إيجابية بدلاً من الاستسلام لليأس أو الغضب.\n",
      "\n",
      "كما يمكن للرحمة أن تساعد في تعزيز الثقة\n",
      "\n",
      "the relationship between faith and ethical behavior in Islamic thought\n",
      "\n",
      "الإيمان أو الدين هو أساس السلوك الأخلاقي والأخلاقي في الفكر الإسلامي. يعتبر الإسلام الإيمان بمثابة الخطوة الأولى نحو الأخلاق، حيث يتطلب الإيمان بالله وباليوم الآخر أن يكون لديك شعور بالمسؤولية تجاه الآخرين وأن تعمل للخير والعدل.\n",
      "\n",
      "في القرآن الكريم، يوجد العديد من الآيات التي تتحدث عن أهمية الأخلاق والعدل في الحياة اليومية. على سبيل المثال، آية رقم 31 من سورة البقرة تقول: \"إن الله اصطفى آدم وحواء وجعلهما ذروة الخلق جميعًا\"، وهي تعبر عن القيمة العالية التي أعطيت للإنسان، والتي تتضمن الأخلاق والعدل.\n",
      "\n",
      "وفي الحديث الشريف، يقول النبي محمد صلى الله عليه وسلم: \"عن انس بن مالك قال رسول الله صلى الله عليه وسلم: \"يا ابن هشام! أما ترى أن الله أمر الرجل أن يطالب زوجته إذا كان عاقلا أن يعطيها حقه من المال إذا غصبته\n",
      "\n",
      "the importance of intention in Islamic religious practice\n",
      "\n",
      "في الإسلام، يعتبر التفكير أو التأمل أمرًا مهمًا للغاية. يعتبر هذا جزءًا أساسيًا من الدين الإسلامي ويعتبر من أهم الطرق التي يمكن للمسلمين من خلالها تعميق فهمهم للإسلام وتعاليمه. \n",
      "\n",
      "الإسلام يشجع على التفكير والتأمل كوسيلة لفهم أعمق وأكثر معنى للحياة والإيمان. يعتقد المسلمون أن التفكير والتأمل يمكن أن يساعدهم على تحقيق السلام الداخلي والقبول بما هو أبعد من العالم المادي الذي نراه يوميًا.\n",
      "\n",
      "بالإضافة إلى ذلك، يمكن للتفكير والتأمل أن يساعد المسلمين على تطوير القدرة على التعامل مع التحديات والصراعات في الحياة بطريقة أكثر تأملاً وتفكيراً. يمكن أن يساعد هذا في تحقيق التوازن بين العمل والرغبة في السعي لتحقيق الأهداف الشخصية، وبين الحاجة إلى الراحة والاسترخاء.\n",
      "\n",
      "وأخيرًا، يمكن للتفكير والتأمل أن يساعد المسلمين على تعزيز القدرة على التعاطف والرحمة تجاه الآخرين.\n",
      "\n",
      "the role of community and social responsibility in Islam\n",
      "\n",
      "في الإسلام، تلعب المجتمع والمسؤولية الاجتماعية دوراً هاماً ومحورياً. هذه المفاهيم متجذرة في القيم الأخلاقية والأخلاقية التي تدعو إلى التعاون والتكافل الاجتماعي.\n",
      "\n",
      "أولاً، المجتمع في الإسلام هو وحدة متكافئة تتألف من أفراد ينتمون إلى دين واحد ويعيشون في مكان واحد. يتم تشجيع الأفراد على العيش معاً والتفاعل مع بعضهم البعض، مما يعزز الوحدة والتضامن. هذا يعكس مبدأ \"الأقربية\" الذي يقول إن الإنسان أقرب الناس إلى الله ومن ثم يجب أن يكون له الأولوية في الحياة.\n",
      "\n",
      "ثانياً، المسؤولية الاجتماعية في الإسلام تعني أن الأفراد يجب أن يساهموا في تحسين مجتمعاتهم والعالم بشكل عام. هذا يمكن أن يشمل العديد من الأشكال، بدءًا من الأعمال الخيرية والتعليم والرعاية الصحية، وصولاً إلى الحفاظ على البيئة والمساهمة في التقدم العلمي والتكنولوجي.\n",
      "\n",
      "في النهاية، الإسلام يدعو\n",
      "\n",
      "the purpose of worship in Islam beyond ritual practice\n",
      "\n",
      "الإسلام هو دين توحيدي يتميز بالتوحيد، والإيمان بالله الواحد القدس جل صغره، والإيمان بالأنبياء والرسل، واتباع السنة النبوية والنبي محمد صلى الله عليه وسلم. العبادة في الإسلام ليست مقتصرة على العبادة البدنية فحسب، بل تشمل العديد من الجوانب الأخرى مثل الصلاة، الصوم، الزكاة، الحج، وطاعة الله في كل الأوقات.\n",
      "\n",
      "الإسلام يشجع على السلوكيات الأخلاقية الإيجابية، مثل الصبر، الصدق، العدل، والتواضع. كما يحث الأفراد على العمل من أجل الخير العام والسعي لتحقيق العدالة والمساواة في المجتمع. \n",
      "\n",
      "من الجدير بالذكر أن الغرض الأساسي من العبادة في الإسلام هو عبادة الله والخضوع لإرادته وتوجيهاته. يعتبر المسلمون العبادة وسيلة للتواصل مع الله والتعبير عن الحب والاحترام له، وهي أيضا وسيلة لطلب الرحمة والغفران والثقة في الله.\n",
      "\n",
      "بالإضافة إلى ذلك، تساعد العبادة في الإسلام على تعزيز الشعور\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"JAIS\\n\")\n",
    "for i in range(len(responses_explanations_jais)):\n",
    "    print(f\"{explanations[i]}\\n\")\n",
    "    print(f\"{responses_explanations_jais[i]}\\n\")\n",
    "with open('explicaciones_jais.txt', 'w') as fichero:\n",
    "    for item in responses_explanations_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aeba5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN\n",
      "\n",
      "how mercy is understood in Islam during times of hardship and suffering\n",
      "\n",
      "في الإسلام، يُفهم التسامح كمبدأ أساسي وعَبْدٍ عظيم في أوقات الصعوبات والمعاناة، حيث يُعتبر من أسمى صور الإيمان والتواضع أمام الله. فحين يمر الإنسان بظروف سلبية مثل الفقر أو المرض أو الخسارة، فإن معرفة الامتنان لله واعتقاد أن كل ما يحدث له هو بحسب توجيهاته العاقلة والخاصة به، يُعزز من قدرته على الصبر والتسامح.\n",
      "\n",
      "يُذكر في الأحاديث النبوية أن الله لا يُؤذي أحدًا إلا بإذن، وأنه يُظهر الرحمة في أ\n",
      "\n",
      "the relationship between faith and ethical behavior in Islamic thought\n",
      "\n",
      "في التفكير الإسلامي، تُعدّ الإيمان والسلوك الأخلاقي مترتين متوازيتين ومتداخلتين، حيث لا يمكن فصل أحدهما عن الآخر. فالإيمان ليس مجرد اتّباع شروط دينية أو تأكيد عقائد معينة، بل يشمل الالتزام بالقيم المذكورة في الدين، مثل العدالة، والرحمة، والصدق، والأمانة، والاحترام للآخرين.\n",
      "\n",
      "يُرى أن الإنسان الذي أُعطي إيمانه بربه، يكون مُطالبًا بأن يُنفِّذه في حياته اليومية من خلال سلوكٍ أخلاقيٍّ يعكس مباد\n",
      "\n",
      "the importance of intention in Islamic religious practice\n",
      "\n",
      "النية تُعد من أركان الإسلام الخمسة، وهي أساس كل عمل ديني يُؤدى في الدين الإسلامي. وقد رُبطت النية بفكرة \"النية قبل الفعل\"، حيث أن أي فعل ديني — سواء كان صلاة أو صوم أو ذكر الله أو خدمة الآخرين — لا يُعتبر صحيحًا إلا إذا كانت فيه نية حقيقية لله وحده، وليس لغيره.\n",
      "\n",
      "ومن أبرز ما يميز النية أنها تجعل العمل مقبولًا أمام الله تعالى، لأن الله يقول: \"إذا قَلْبُكَ فِي الْعَمَلِ فَأَنْتَ مَوْضُ\n",
      "\n",
      "the role of community and social responsibility in Islam\n",
      "\n",
      "في الإسلام، يُعتبر المجتمع والمسؤولية الاجتماعية من أركان العقيدة والسلوك الإنساني المتكامل، حيث تُبنى الحياة الدينية على مبدأ التوازن بين الفرد والمجتمع. فالمسلمون ليسوا مجرد أفراد يعيشون في عزلة عن بعضهم، بل هم أعضاء في جسد إنساني مترابط يحمل مسؤوليات مشتركة تهدف إلى تحقيق العدالة، ورعاية الأسرة، وتوفير الظروف المناسبة للحياة الناجحة.\n",
      "\n",
      "تُؤخذ المسؤولية الاجتماعية في الإسلام بمقتضى دعوة النبي محمد ﷺ التي انتهى بها إلى أن \"الإنسان لا يُحاسب إلا على ما فعله في\n",
      "\n",
      "the purpose of worship in Islam beyond ritual practice\n",
      "\n",
      "ال Worship في الإسلام لا يقتصر على العمليات البدنية أو التقاليد المحددة، بل يمتد إلى أبعاد عميقة تتعلق بالعلاقة بين الإنسان وربه، والانسجام مع خلقه، وتحقيق التوازن الداخلي والخارجي. فغرض العبادة في الإسلام هو أن يُستَرِدَ الإنسان من حالة الفوضى والابتعاد عن المُبَلَّغ، ويُعاد إلى حيّز الوعي والذات، حيث يدرك أن كل ما يملكه من قدرة، وحياة، ونفوس، مُمنوحة له من ربه.\n",
      "\n",
      "إن العبادة هي وسيلة لتجديد\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"QWEN\\n\")\n",
    "for i in range(len(responses_explanations_qwen)):\n",
    "    print(f\"{explanations[i]}\\n\")\n",
    "    print(f\"{responses_explanations_qwen[i]}\\n\")\n",
    "with open('explicaciones_qwen.txt', 'w') as fichero:\n",
    "    for item in responses_explanations_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dae31dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating تأمل ديني قصير (٦-٨ جمل) عن الرحمة والشفقة...\n",
      "Generating فقرة أخلاقية عن الصبر والتوكل على الله في الأوقات الصعبة...\n",
      "Generating فقرة قصيرة على غرار الموعظة عن المغفرة والمصالحة...\n",
      "Generating تأمل ديني حول كيف يُرشد الإيمان الحياة اليومية...\n",
      "Generating نص ديني قصير عن العدالة والمسؤولية الشخصية...\n",
      "Generating short religious reflection (6–8 sentences) about mercy and compassion...\n",
      "Generating moral paragraph about patience and trust in God during difficult times...\n",
      "Generating short sermon-style paragraph about forgiveness and reconciliation...\n",
      "Generating religious reflection on how faith guides everyday life...\n",
      "Generating short religious text about justice and personal responsibility...\n"
     ]
    }
   ],
   "source": [
    "creativity = [\"short religious reflection (6–8 sentences) about mercy and compassion\",\n",
    "              \"moral paragraph about patience and trust in God during difficult times\",\n",
    "              \"short sermon-style paragraph about forgiveness and reconciliation\",\n",
    "              \"religious reflection on how faith guides everyday life\",\n",
    "              \"short religious text about justice and personal responsibility\"]\n",
    "creativity_ar = [\"تأمل ديني قصير (٦-٨ جمل) عن الرحمة والشفقة\",\n",
    "                \"فقرة أخلاقية عن الصبر والتوكل على الله في الأوقات الصعبة\",\n",
    "                \"فقرة قصيرة على غرار الموعظة عن المغفرة والمصالحة\",\n",
    "                \"تأمل ديني حول كيف يُرشد الإيمان الحياة اليومية\",\n",
    "                \"نص ديني قصير عن العدالة والمسؤولية الشخصية\"]\n",
    "\n",
    "def show_creativity(creativity, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_creativity = []\n",
    "    if not gen_cfg.get(\"use_cache\"):\n",
    "        for i in range(len(creativity)):\n",
    "            print(f\"Generating {creativity[i]}...\")\n",
    "            prompt = f'''{system_text} .{creativity[i]} اكتب'''\n",
    "            responses_creativity.append(generate_chat(model, tokenizer, prompt, None, gen_cfg))\n",
    "    else:\n",
    "        for i in range(len(creativity)):\n",
    "            print(f\"Generating {creativity[i]}...\")\n",
    "            prompt = f'''Write a {creativity[i]}'''\n",
    "            responses_creativity.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_creativity\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "SYSTEM_AR_OR = \".أجب باللغة العربية الفصحى الحديثة فقط. استخدم أسلوباً دينياً رسمياً. لا تقتبس النصوص الدينية حرفياً\"\n",
    "\n",
    "responses_creativity_jais = show_creativity(creativity_ar, jais_model, jais_tok, SYSTEM_AR_OR, GEN_CFG_2)\n",
    "responses_creativity_qwen = show_creativity(creativity, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83f511e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAIS\n",
      "\n",
      "short religious reflection (6–8 sentences) about mercy and compassion\n",
      "\n",
      "في عالم الحب والرأفة، نجد الذات في قلوب الآخرين، نتلمس الطريق نحو الخير والصلاح. الرحمة هي تلك النبتة الصغيرة التي تنمو في القلب، تغذيها العين بالأمل والتفاؤل، وتساعدها اليد بالعمل والعمل الخير. الرحمة ليست مجرد مشاعر، بل هي القوة التي تدفعنا نحو الخير والعدل والسلام. الرحمة تجعلنا أكثر إنسانية، تجعلنا ننظر إلى العالم بعيون متفتحة ومشرقة. في كل عمل خيري نقوم به، نحن نتذكر أن الرحمة هي القوة التي تجمعنا جميعاً كأبناء للإنسانية الواحدة.\n",
      "\n",
      "moral paragraph about patience and trust in God during difficult times\n",
      "\n",
      "في مواجهة الشدائد، قد يكون من الجذاب الاستسلام للإرهاق واليأس. ومع ذلك، يعتبر الاعتماد الدائم على قوة أعلى، سواء كان الله أو القوة العليا التي يعبدها الشخص، هو الجواب الوحيد. يمكن أن يمنح هذا الإيمان الثابت الشخص القوة لمواجهة العاصفة والخروج أقوى. الصبر، بدوره، هو فضيلة أساسية في أوقات الشدة. إنه فن قبول الأمور كما تأتي، وعدم السماح للخوف أو الشك أن يشوه حكمنا. يسمح لنا هذا الموقف الهادئ بالاستمتاع بنعمة الحاضر وصنع أفضل ما في وضعنا دون أن يتم استهلاكه بالندم أو الذنب. من خلال ممارسة الصبر، يمكننا تعلم النظر إلى التحديات ليست كعقبات لا يمكن التغلب عليها، بل كفرص للنمو والتحول. وبذلك، يمكننا أن نحرر أنفسنا من أغلال الخوف ونفتح قدراتنا الحقيقية.\n",
      "\n",
      "short sermon-style paragraph about forgiveness and reconciliation\n",
      "\n",
      "يا أيها المغفرة والرحيمة، يا رب، اجمعني وزوجي على كلمة الحق والعدل والسلام. كن بيننا القوة التي تجمعنا على الحق والرغبة في الخير والعدل. \n",
      "\n",
      "أدعوك يا رب أن تمنحها لنا القوة والتسامح لنغفر ونصلح ما كسر بيننا وبين زوجنا، وأن تعطينا القدرة على العفو والصفح. \n",
      "\n",
      "أدعوك يا رب أن تجعلنا جميعاً نتذكر دائماً أن العفو هو الطريق الوحيد للسلام والحب، وأن التسامح هو السبيل الوحيد لتحقيق المصالحة والحفاظ على العلاقات الإنسانية. \n",
      "\n",
      "أدعوك يا رب أن تعطيني وزوجها القوة والشجاعة لمواجهة أي تحديات قد تأتي في طريقنا، وأن تعطيني القدرة على الغفران والمغفرة عند الخطأ والخطأ. \n",
      "\n",
      "أنت يا رب، أنت النور الذي يضيء حياتنا، وأنت الأمل الذي يبقينا متواضعين في كل يوم جديد. أدعوك يا\n",
      "\n",
      "religious reflection on how faith guides everyday life\n",
      "\n",
      "الإيمان هو منارة الضوء في بحر الظلام، وهو بوصلتنا في رحلة الحياة العاصفة. إنه القوة التي تدفعنا للأمام، الأمل الذي يغذينا، والقوة التي تجعلنا نواجه التحديات بشجاعة. إنه ليس مجرد مجموعة من المعتقدات، بل هو طريقة حياة. إنه دليل على كيفية توجيه الإيمان للحياة اليومية. \n",
      "\n",
      "الإيمان ليس مجرد معتقدات دينية، بل هو أيضاً قوة روحية تغذي الروح والقلب. إنه يساعدنا على الشعور بالسلام والراحة، حتى في أصعب الأوقات. الإيمان هو الأمل الذي يجعل الحياة تستحق العيش، وهو الحب الذي يجعل العالم مكاناً أفضل. \n",
      "\n",
      "الإيمان هو القوة التي تجعلنا نواجه التحديات بشجاعة، لأنه يعطينا القوة لمواجهة الصعوبات والاستمرار في النضال. إنه يشجعنا على الصبر والثبات، حتى عندما تكون الأمور صعبة. \n",
      "\n",
      "الإيمان هو القوة التي تجعل الحياة تستحق العيش، لأنه يعطينا\n",
      "\n",
      "short religious text about justice and personal responsibility\n",
      "\n",
      "في هذا العالم الذي نعيش فيه، يجب علينا جميعًا أن نتحمل مسؤولية أفعالنا. العدل هو القوة التي تمنحنا القدرة على التمييز بين الصواب والخطأ، وبين الحق والباطل. \n",
      "\n",
      "العدل ليس فقط مبدأً أو قيمة أخلاقية، بل هو أيضًا واجبًا أخلاقيًا تجاه الآخرين. عندما نتمتع بالقدرة على الحكم والتمييز بين الأمور، فإننا نكون قد اكتسبنا القوة لنكون مسؤولين عن أفعالنا. \n",
      "\n",
      "العدالة تعني أيضًا المساواة. في عالم يتميز بالتوترات والصراعات، يجب أن نسعى جميعًا لضمان أن يكون الجميع متساوين أمام القانون. يجب أن نتذكر دائمًا أن كل فرد له حق الحياة والكرامة والحرية. \n",
      "\n",
      "في النهاية، يجب أن نتذكر دائمًا أن العدالة ليست مجرد مفهوم أو فكرة، بل هي عملية. إنها تتطلب منا\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"JAIS\\n\")\n",
    "for i in range(len(responses_creativity_jais)):\n",
    "    print(f\"{creativity[i]}\\n\")\n",
    "    print(f\"{responses_creativity_jais[i]}\\n\")\n",
    "with open('creativity_jais.txt', 'w') as fichero:\n",
    "    for item in responses_creativity_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5ab22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN\n",
      "\n",
      "short religious reflection (6–8 sentences) about mercy and compassion\n",
      "\n",
      "الرحمة والتعاطف هما من أركان الإسلام العظيم، ويُعدان مفتاحًا لحياةٍ متوازنة ومحبّة. يُظهر القرآن الكريم أن الله هو الرحيم، إذ لا يُخلو شيء من تأثيره الرحيّم على خلقه. فكثيرًا ما يُستَعْلم في الإنسان نسخة من رحمة الله عندما يُعرض للإجهاد أو القلق أو المحن، فيُشعر بالرغبة في مشاركة الآخرين بالمشاعر الإنسانية. فالتعاطف ليس مجرد تصرف عابر، بل هو دعوة إلى التفكير في حالة الآخر، كما كان النبي محمد ﷺ\n",
      "\n",
      "moral paragraph about patience and trust in God during difficult times\n",
      "\n",
      "في أوقات الصعوبات والجُوع، يظهر مبدأ الصبر كأحد أقوى الصفات التي تُبنى عليها صلابة النفس وثبات القلب. فكل ما يمر به الإنسان من محن لا يُستَوْسَط إلا بالصبر، لأنه يُعلم أن الله لا يُهمل ما يُحِبّه، ولا يُضيع عَمَلَ المُؤمنين في التفاؤل. ويُكفل لنا أن نثق بقدرته على القيادة والرعاية حتى في الأزمات، لأن كل شيء يُجرى في الزمن ليس له سبب مادي فقط، بل له روحانية، تتماشى مع ق\n",
      "\n",
      "short sermon-style paragraph about forgiveness and reconciliation\n",
      "\n",
      "الصبر والرحمة في القلب، من أعمدة الدين، ويُعد التسامح وتحقيق الوحدة بعد الخلاف مسؤولية عظيمة على الإنسان. فمَن يُعاقب نفسه على خطأه، أو يحمل في قلبه إساءةً لا تُغفر، فإن ذلك يُضعف دينه ويُبعد عناه عن رحمة الله. أما من يُغفر، فيُستقرّ قلبه في طاعة الله، ويُفتح طريقًا للإحسان والوحدة بين الأهل والأصدقاء. فالتوحيد لا يكون في العدالة فقط، بل في القدرة على التصالح، وقبول المُخطئ، ك\n",
      "\n",
      "religious reflection on how faith guides everyday life\n",
      "\n",
      "تُعدّ الإيمان مِنْ أَوَّل المبادئ التي تُبنى عليها حياة الإنسان، فهو ليس مجرد عبادة في الأوقات الخاصة أو التصفيحات الدينية، بل هو نسيج ينسج نفسه داخل كل لحظة من الحياة اليومية. فكل ما نفعله – من وضوء الفجر إلى قيام العمل، من تبادل الحماس مع قريب إلى تجاوز الخلاف مع زميل – يمكن أن يكون معبّرًا عن إيماننا الحقيقي.\n",
      "\n",
      "الإيمان لا يقتصر على الاعتراف بالله في الصلاة أو المواقف الكبرى؛ بل يتجلى في صمت الطاعة عند السُ\n",
      "\n",
      "short religious text about justice and personal responsibility\n",
      "\n",
      "بِسْمِ اللَّهِ الرَّحْمَنِ الرَّحِيمِ\n",
      "\n",
      "إن العدالة هي ركن أساسي في ديننا، وتمثل صلب التقوى والاحترام بين الإنسان والآخر. فكل مسلم يُعدُّ مسؤولًا أمام الله عز وجل عن كل فعلٍ يقوم به، سواء كان في القول أو في العمل أو في الصمت. فلا يُسمَح للإنسان أن يُغفل عن مسؤوليته حين يُظلم أحدًا أو يُضاعف الفساد في الأرض.\n",
      "\n",
      "وقد قال النبي محمد ﷺ: \"لا تُسلَّم على عدلٍ منك،\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"QWEN\\n\")\n",
    "for i in range(len(responses_creativity_qwen)):\n",
    "    print(f\"{creativity[i]}\\n\")\n",
    "    print(f\"{responses_creativity_qwen[i]}\\n\")\n",
    "with open('creativity_qwen.txt', 'w') as fichero:\n",
    "    for item in responses_creativity_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feebf3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_responses_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "    return [item.strip() for item in text.split(\"\\n\\n\") if item.strip()]\n",
    "\n",
    "responses_explanations_qwen = load_responses_txt(\"evaluation_set/explicaciones_qwen.txt\")\n",
    "responses_explanations_jais = load_responses_txt(\"evaluation_set/explicaciones_jais.txt\")\n",
    "\n",
    "responses_concepts_qwen = load_responses_txt(\"evaluation_set/conceptos_qwen.txt\")\n",
    "responses_concepts_jais = load_responses_txt(\"evaluation_set/conceptos_jais.txt\")\n",
    "\n",
    "responses_creativity_qwen = load_responses_txt(\"evaluation_set/creativity_qwen.txt\")\n",
    "responses_creativity_jais = load_responses_txt(\"evaluation_set/creativity_jais.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f811df",
   "metadata": {},
   "source": [
    "Aqui vamos a cargar el modelo evaluador como ya hemos explicado previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2042c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "def load_chat_model_llama(model_id):\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        torch_dtype=\"auto\",\n",
    "        max_memory={0: \"11GiB\", \"cpu\": \"18GiB\"},   # 👈 limita para que no pete\n",
    "        offload_folder=\"offload_llama\",           # 👈 si hace offload, que sea ordenado\n",
    "    )\n",
    "\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "# Generamos texto\n",
    "def generate_chat_llama(model, tokenizer, user_text, system_text=None, gen_cfg=GEN_CFG):\n",
    "    messages = []\n",
    "    if system_text:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_text})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True,\n",
    "            **gen_cfg\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d330e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_chat_model_llama() got an unexpected keyword argument 'trust_remote_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m llama_id = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llama_tokenizer, llama_model = \u001b[43mload_chat_model_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load_chat_model_llama() got an unexpected keyword argument 'trust_remote_code'"
     ]
    }
   ],
   "source": [
    "llama_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama_tokenizer, llama_model = load_chat_model_llama(llama_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa988df4",
   "metadata": {},
   "source": [
    "Las instrucciones son sencillas, valorar del 1 al 5 las siguientes variables:\n",
    "1. Correccion gramatical\n",
    "2. Fluidez y naturalidad\n",
    "3. Adecuacion al registro religioso\n",
    "4. Coherencia conceptual\n",
    "\n",
    "Siendo un 1 la peor nota y un 5 la mejor nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1d4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLAMA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# print(generate_chat(llama_model, llama_tokenizer, prompt, None, gen_cfg=GEN_CFG_2))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGEN_CFG\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mgenerate_chat\u001b[39m\u001b[34m(model, tokenizer, user_text, system_text, gen_cfg)\u001b[39m\n\u001b[32m     41\u001b[39m input_len = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_cfg\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(out[\u001b[32m0\u001b[39m][input_len:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:309\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m    308\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:155\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prompt = \"Who are you?\"\n",
    "\n",
    "print(\"LLAMA\")\n",
    "# print(generate_chat(llama_model, llama_tokenizer, prompt, None, gen_cfg=GEN_CFG_2))\n",
    "print(generate_chat_llama(llama_model, llama_tokenizer, prompt, None, gen_cfg=GEN_CFG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec8a94c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating الرحمة هي مفهوم أساسي في الإسلام، وهي تشكل جزءًا لا يتجزأ من تعاليم القرآن والحديث. يتم تعريفها على أنها القدرة على التعاطف مع معاناة الآخرين والرغبة في تخفيفها، وهي تعبر عن الجانب الإنساني للدين.\n",
      "\n",
      "في القرآن، يتم تصوير الرحمة كفضيلة ذاتية، حيث يعتبر المسلمون أن الله هو الرحمن الرحيم. يقول سورة الشعراء: \"هو الذي له الرحمة والرشد والرحمان رحيم بالبعث\". هذه الآية تشير إلى قدرة الله على كل شيء، بما في ذلك الرحمة.\n",
      "\n",
      "الرحمة ليست مجرد مشاعر، بل هي جزء من العقيدة الإسلامية، وتشمل الرحمة بكل الكائنات الحية، حتى الحيوانات والنباتات. يقول حديث النبي محمد صلى الله عليه وسلم: \"إن الله خلقكم من الماء والأرض، وجعلكم شاربين ومنازعين وأ للناديكم ورُفَقٍ بكم\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m         evaluation_concepts.append(generate_chat(model, tokenizer, prompt, \u001b[38;5;28;01mNone\u001b[39;00m, gen_cfg))\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_concepts\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m evaluation_concepts_jais = \u001b[43mevaluate_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses_concepts_jais\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGEN_CFG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m evaluation_concepts_qwen = evaluate_concepts(responses_concepts_qwen, llama_model, llama_tokenizer, GEN_CFG)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mevaluate_concepts\u001b[39m\u001b[34m(concepts, model, tokenizer, gen_cfg)\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcepts[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m'''\u001b[39m\u001b[33mYou are an expert in Arabic language and Islamic studies.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m                Evaluate the following Arabic text.\u001b[39m\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33m                Text:\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m                \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcepts[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'''\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     evaluation_concepts.append(\u001b[43mgenerate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_cfg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_concepts\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mgenerate_chat\u001b[39m\u001b[34m(model, tokenizer, user_text, system_text, gen_cfg)\u001b[39m\n\u001b[32m     41\u001b[39m input_len = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_cfg\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(out[\u001b[32m0\u001b[39m][input_len:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:473\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    472\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\.conda\\envs\\coran-nlp\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def evaluate_concepts(concepts, model, tokenizer, gen_cfg):\n",
    "    evaluation_concepts = []\n",
    "    for i in range(len(concepts)):\n",
    "        print(f\"Evaluating {concepts[i]}\")\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {concepts[i]}'''\n",
    "        \n",
    "        evaluation_concepts.append(generate_chat(model, tokenizer, prompt, None, gen_cfg))\n",
    "    return evaluation_concepts\n",
    "\n",
    "evaluation_concepts_jais = evaluate_concepts(responses_concepts_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_concepts_qwen = evaluate_concepts(responses_concepts_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA JAIS\\n\")\n",
    "for i in range(len(evaluation_concepts_jais)):\n",
    "    print(f\"{concepts[i]}\\n\")\n",
    "    print(f\"{evaluation_concepts_jais[i]}\\n\")\n",
    "with open('evaluation_concepts_jais.txt', 'w') as fichero:\n",
    "    for item in evaluation_concepts_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba427da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA QWEN\\n\")\n",
    "for i in range(len(evaluation_concepts_qwen)):\n",
    "    print(f\"{concepts[i]}\\n\")\n",
    "    print(f\"{evaluation_concepts_qwen[i]}\\n\")\n",
    "with open('evaluation_concepts_qwen.txt', 'w') as fichero:\n",
    "    for item in evaluation_concepts_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c145ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_explanations(explanations, model, tokenizer, gen_cfg):\n",
    "    for i in range(len(explanations)):\n",
    "        print(f\"Evaluation for explanation {explanations[i]}\")\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {explanations[i]}'''\n",
    "\n",
    "evaluation_explanations_jais = evaluate_explanations(responses_explanations_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_explanations_qwen = evaluate_explanations(responses_explanations_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA JAIS\\n\")\n",
    "for i in range(len(evaluation_explanations_jais)):\n",
    "    print(f\"{explanations[i]}\\n\")\n",
    "    print(f\"{evaluation_explanations_jais[i]}\\n\")\n",
    "with open('evaluation_explanations_jais.txt', 'w') as fichero:\n",
    "    for item in evaluation_explanations_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA QWEN\\n\")\n",
    "for i in range(len(evaluation_concepts_qwen)):\n",
    "    print(f\"{explanations[i]}\\n\")\n",
    "    print(f\"{evaluation_concepts_qwen[i]}\\n\")\n",
    "with open('evaluation_concepts_qwen.txt', 'w') as fichero:\n",
    "    for item in evaluation_concepts_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31451558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_creativity(creativity, model, tokenizer, gen_cfg):\n",
    "    for i in range(len(creativity)):\n",
    "        print(f\"Evaluation for creativity {creativity[i]}\")\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {creativity[i]}'''\n",
    "\n",
    "evaluation_creativity_jais = evaluate_creativity(responses_creativity_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_creativity_qwen = evaluate_creativity(responses_creativity_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56cdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA JAIS\\n\")\n",
    "for i in range(len(evaluation_creativity_jais)):\n",
    "    print(f\"{creativity[i]}\\n\")\n",
    "    print(f\"{evaluation_creativity_jais[i]}\\n\")\n",
    "with open('evaluation_creativity_jais.txt', 'w') as fichero:\n",
    "    for item in evaluation_creativity_jais:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLAMA QWEN\\n\")\n",
    "for i in range(len(evaluation_creativity_qwen)):\n",
    "    print(f\"{creativity[i]}\\n\")\n",
    "    print(f\"{evaluation_creativity_qwen[i]}\\n\")\n",
    "with open('evaluation_creativity_qwen.txt', 'w') as fichero:\n",
    "    for item in evaluation_creativity_qwen:\n",
    "        fichero.write(item + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
