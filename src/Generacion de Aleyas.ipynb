{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6affb3",
   "metadata": {},
   "source": [
    "# Notebook de Generaci√≥n de Versos\n",
    "En este fichero enfocado para la entrega final del proyecto, experimentaremos con diferentes modelos de generaci√≥n de texto. Donde nuestro objetivo va a ser conseguir nuevos versos, llamados `aleya`, que aporten contenido al Cor√°n.\n",
    "\n",
    "El enfoque que tomaremos ser√° realizar comparaciones entre modelos generativos especializados en el √°rabe y modelos generativos generalistas (que sirvan para todas los idiomas).\n",
    "\n",
    "Por otra parte, usaremos una **Large Language Model** (`LLM`) para evaluar la cohesi√≥n, coherencia y otras dem√°s m√©tricas de los versos generados. Dado el caso de que no haya ning√∫n modelo evaluador √°rabe disponible, implementaremos otras *LLMs* manualmente para realizar esta segunda tarea. \n",
    "\n",
    "(zerbait gehiago gehitu/aldatu dezakegu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400e487",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344aae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Dependencias\n",
    "import torch\n",
    "from decouple import config\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login(config(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b35295",
   "metadata": {},
   "source": [
    "## Generaci√≥n de Aleyas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070ca00",
   "metadata": {},
   "source": [
    "Como ya hemos mencionado previamente, usaremos este apartado para evaluar si los modelos generalistas son capaces de hacer mejor language modeling que los modelos especializados en arabe.\n",
    "\n",
    "Como nota diremos que aparte de las pruebas que se hacen aqui, en la secci√≥n de generaci√≥n de topics + clustering ya se comprueba que modelos multilenguaje como ollama-gemma3:4b generan bastante bien texto en arabe. No solo eso sino que sus traducciones parecen ser bastante fieles despues de haberlas traducido manualmente. Sin embargo, aunque las traducciones fueran buenas, las comparaciones de cosine-similayity eran bastante inconcluyentes. Esto es debido a que probablemente la forma en la que creamos los embeddings no era la correcta (al final el arabe y el ingles son sem√°nticamente muy diferentes).\n",
    "\n",
    "Ahora usaremos el modelo Qwen3-4B para generar versos en arabe. Como nota, lo hemos extraido del benchmark **Arabic Broad Leaderboard (ABL) - The first comprehensive Leaderboard for Arabic LLMs** de huggingface. Ademas, usamos la version reducida del modelo (4B) ya que el original pesa bastante.\n",
    "\n",
    "Este modelo es el Qwen3-4B-Instruct-2507, capaz de hablar multiples idiomas. En nuestro caso, como estamos contrastando modelos (preentenados), haremos una protocolo de evaluacion para contrastarlos.\n",
    "\n",
    "Tambi√©n importaremos el modelo jais-family-2p7b-chat. La familia de modelos Jais es una serie completa de modelos ling√º√≠sticos extensos (LLM) biling√ºes ingl√©s-√°rabe. Estos modelos est√°n optimizados para destacar en √°rabe, con s√≥lidas capacidades en ingl√©s. Para simplicidad de la tarea, hemos querido seleccionar un modelo que sepa algo de ingles para que podamos hacer las intrucciones nosotros mismos.\n",
    "\n",
    "En resumidas cuentas comparamos un modelo generalista multiling√ºe (Qwen) frente a un modelo biling√ºe especializado en √°rabe (JAIS), manteniendo el prompt en ingl√©s para controlar la variable de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a3ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una configuraci√≥n para generar texto para que la evaluaci√≥n sea m√°s precisa/justa\n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens=160,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    use_cache=True,\n",
    ")\n",
    "GEN_CFG_2 = dict(\n",
    "    max_new_tokens=160,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Cargamos el modelo\n",
    "def load_chat_model(model_id, trust_remote_code=False):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok, mdl\n",
    "\n",
    "# Generamos texto\n",
    "def generate_chat(model, tokenizer, user_text, system_text=None, gen_cfg=GEN_CFG):\n",
    "    messages = []\n",
    "    if system_text:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_text})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **gen_cfg\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49a29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f78f3aadb149c3b1576b83b892d296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jais_id = \"inceptionai/jais-family-2p7b-chat\"\n",
    "jais_tok, jais_model = load_chat_model(jais_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cb0eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf0bc9216054b6eae9183a6dd42c101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qwen_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "qwen_tok, qwen_model = load_chat_model(qwen_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0314d",
   "metadata": {},
   "source": [
    "Vamos a hacer una breve prueba para comprobar que todo funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JAIS ===\n",
      "I am an advanced artificial intelligence model designed to assist with a wide range of tasks and inquiries. My capabilities include providing information, answering questions, and offering recommendations based on available data and patterns. I strive to provide accurate, comprehensive, and thoughtful responses to all queries.\n",
      "\n",
      "=== QWEN ===\n",
      "I am Qwen, a large-scale language model independently developed by the Tongyi Lab under Alibaba Group. I can answer questions, create text such as stories, official documents, emails, scripts, perform logical reasoning, coding, and more. I can also express opinions and play games. I support 100 languages, including but not limited to Chinese, English, German, French, Spanish, etc. If you have any questions or need assistance, feel free to ask me anytime! üòä\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who are you?\"\n",
    "\n",
    "print(\"JAIS\")\n",
    "print(generate_chat(jais_model, jais_tok, prompt, None, gen_cfg=GEN_CFG_2))\n",
    "\n",
    "print(\"\\nQWEN\")\n",
    "print(generate_chat(qwen_model, qwen_tok, prompt, None, gen_cfg=GEN_CFG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe5626",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n del Texto Generado\n",
    "Para la evaluaci√≥n del texto generado en arabe usaremos un modelo generalista multiling√ºe (Falcon). Gracias a su formaci√≥n biling√ºe (√°rabe e ingl√©s) y a su capacidad de razonamiento, Falcon nos da una evaluaci√≥n m√°s neutral de la correcci√≥n gramatical, la fluidez y la coherencia que otros modelos que pueden ser mas especializados en ciertos dominios.\n",
    "\n",
    "Nuestra tarea de evaluaci√≥n se dividir√° en tres partes:\n",
    "\n",
    "1. Una tarea explicativa de conceptos principalmente relacionados con la religi√≥n y la fe.\n",
    "2. Una tarea para explicar ideas religiosas o versos Cor√°nicos.\n",
    "3. Una tarea de prompts creativos religiosos.\n",
    "\n",
    "En estas tareas evaluaremos diferentes aspectos:\n",
    "### Dimensiones ling√º√≠sticas\n",
    "1. Correcci√≥n gramatical (morfolog√≠a, concordancia)\n",
    "2. Fluidez y naturalidad en √°rabe cl√°sico / MSA\n",
    "3. Ausencia de interferencia del ingl√©s (calcos, estructuras raras)\n",
    "4. Registro adecuado (formal / religioso)\n",
    "\n",
    "### Dimensiones sem√°ntico-religiosas\n",
    "1. Adecuaci√≥n conceptual (no errores teol√≥gicos graves)\n",
    "2. Uso apropiado del l√©xico religioso (ÿ±ÿ≠ŸÖÿ©ÿå ÿ™ŸÇŸàŸâÿå ÿπÿ®ÿßÿØÿ©, etc.)\n",
    "3. Coherencia interna del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "concepts = [\"mercy\", \"justice\", \"faith\", \"patience\", \"forgiveness\"]\n",
    "\n",
    "def concepts_explanation(concepts, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_concepts = []\n",
    "    for i in range(len(concepts)):\n",
    "        prompt = f'''Explain the concept {concepts[i]} in Islamic theology'''\n",
    "        responses_concepts.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_concepts\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "\n",
    "responses_concepts_jais = concepts_explanation(concepts, jais_model, jais_tok, SYSTEM_AR, GEN_CFG_2)\n",
    "responses_concepts_qwen = concepts_explanation(concepts, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = [\"how mercy is understood in Islam during times of hardship and suffering\",\n",
    "                \"the relationship between faith and ethical behavior in Islamic thought\",\n",
    "                \"the importance of intention in Islamic religious practice\",\n",
    "                \"the role of community and social responsibility in Islam\",\n",
    "                \"the purpose of worship in Islam beyond ritual practice\"\n",
    "                ]\n",
    "\n",
    "def show_explanations(explanations, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_explanations = []\n",
    "    for i in range(len(explanations)):\n",
    "        prompt = f'''Explain {explanations[i]}.'''\n",
    "        responses_explanations.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_explanations\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "\n",
    "responses_explanations_jais = show_explanations(explanations, jais_model, jais_tok, SYSTEM_AR, GEN_CFG_2)\n",
    "responses_explanations_qwen = show_explanations(explanations, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae31dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "creativity = [\"short religious reflection (6‚Äì8 sentences) about mercy and compassion\",\n",
    "              \"moral paragraph about patience and trust in God during difficult times\",\n",
    "              \"short sermon-style paragraph about forgiveness and reconciliation\",\n",
    "              \"religious reflection on how faith guides everyday life\",\n",
    "              \"short religious text about justice and personal responsibility\"]\n",
    "\n",
    "def show_creativity(creativity, model, tokenizer, system_text, gen_cfg):\n",
    "    responses_creativity = {}\n",
    "    for i in range(len(creativity)):\n",
    "        prompt = f'''Write a {creativity[i]}'''\n",
    "        responses_creativity.append(generate_chat(model, tokenizer, prompt, system_text, gen_cfg))\n",
    "    return responses_creativity\n",
    "\n",
    "# Definimos como debe comportarse el modelo\n",
    "SYSTEM_AR = \"Answer in Modern Standard Arabic only. Use a formal religious register. Do not quote religious texts verbatim.\"\n",
    "\n",
    "responses_creativity_jais = show_creativity(creativity, jais_model, jais_tok, SYSTEM_AR, GEN_CFG_2)\n",
    "responses_creativity_qwen = show_creativity(creativity, qwen_model, qwen_tok, SYSTEM_AR, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d330e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d84595b50924150aed7a4147f9a8420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235dff644ace417cbe5a553022902c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:  42%|####1     | 2.09G/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8896ce7b4b4fb3a286eb6dbad8af1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889541d82a6b4702a479f19d4548fcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#llama_id = \"google/gemma-2-9b-it\"\n",
    "llama_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama_tokenizer, llama_model = load_chat_model(llama_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concepts(concepts, model, tokenizer, gen_cfg):\n",
    "    evaluation_concepts = []\n",
    "    for i in range(len(concepts)):\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {concepts[i]}'''\n",
    "        \n",
    "        evaluation_concepts.append(generate_chat(model, tokenizer, prompt, None, gen_cfg))\n",
    "    return evaluation_concepts\n",
    "\n",
    "evaluation_concepts_jais = evaluate_concepts(responses_concepts_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_concepts_qwen = evaluate_concepts(responses_concepts_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c145ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_explanations(explanations, model, tokenizer, gen_cfg):\n",
    "    for i in range(len(explanations)):\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {explanations[i]}'''\n",
    "\n",
    "evaluation_concepts_jais = evaluate_explanations(responses_explanations_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_concepts_qwen = evaluate_explanations(responses_explanations_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31451558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_creativity(creativity, model, tokenizer, gen_cfg):\n",
    "    for i in range(len(creativity)):\n",
    "\n",
    "        prompt = f'''You are an expert in Arabic language and Islamic studies.\n",
    "                    Evaluate the following Arabic text.\n",
    "\n",
    "                    Criteria:\n",
    "                    1. Grammatical correctness\n",
    "                    2. Fluency and naturalness\n",
    "                    3. Appropriateness of religious register\n",
    "                    4. Conceptual correctness\n",
    "\n",
    "                    Give a score from 1 to 5 for each criterion and a short justification.\n",
    "\n",
    "                    Text:\n",
    "                    {creativity[i]}'''\n",
    "\n",
    "evaluation_concepts_jais = evaluate_creativity(responses_creativity_jais, llama_model, llama_tokenizer, GEN_CFG)\n",
    "evaluation_concepts_qwen = evaluate_creativity(responses_creativity_qwen, llama_model, llama_tokenizer, GEN_CFG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
