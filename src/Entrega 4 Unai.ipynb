{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generación de Versos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b453b",
   "metadata": {},
   "source": [
    "## 1. RNNs para Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68974aa",
   "metadata": {},
   "source": [
    "Vocabulary general (no es extrictamente necesario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b86c44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # función para añadir token (nuevo) al diccionario\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        # función para añadir N > 1 tokens al diccionario\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        # función para obtener el token del idx introducido\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # devuelve el tamaño del diccionario\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        # devuelve el tamaño del vocabulario\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7b604",
   "metadata": {},
   "source": [
    "Vocabulary especial Corán con los tokens especiales \\<eos>, \\<bos>, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26936ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153908c",
   "metadata": {},
   "source": [
    "Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98491176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector será <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector será os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405dbba",
   "metadata": {},
   "source": [
    "Clase dataset del Corán, para trabajar con el formato correcto de nuestro dataset (.txt): **número|número|texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c511d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2 # el +2 incluye los tokens del diccionario + <bos> y <eos>\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    # a partir de aquí hay metodos necesarios para manipular nuestro dataset específico\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d800e",
   "metadata": {},
   "source": [
    " RNN Generativo: modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79837692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    # nuestro modelo nn para el rnn\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5,\n",
    "                 pretrained_embeddings_ft = None):\n",
    "        super().__init__()\n",
    "        # arquitectura de nuestra rnn\n",
    "\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) # capa de inicio del tamaño del vocabulario\n",
    "        # Aquí metemos los embeddings (pesos) del fasttext\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\") # rnn\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) # fully connected\n",
    "        self.dropout_p = dropout_p # probabilidad de dropout de neuronas\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)             \n",
    "        y_out, _ = self.rnn(x_emb)               \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  \n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99507f",
   "metadata": {},
   "source": [
    "Training helpers y utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e57ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    # genera batches para mandarlos al cpu/gpu (si tenemos cuda)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # loss function, en nuestro caso el cross entropy loss. Ya que compararemos la distribución de predicciones con la ground truth\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # función para calcular la accuracy, comparando cada caracter predicho con el ground truth\n",
    "    y_hat = y_pred.argmax(dim=-1)  \n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    # sacado del notebook de ALUD, \n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    # función para tener en cuenta mejora/desmejora de rendimiento -> early_stopping\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4863c78",
   "metadata": {},
   "source": [
    "Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4ce6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/RNN/coran_rnn_v1\",\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e7b78",
   "metadata": {},
   "source": [
    "Nuevos versos generados por RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1395b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8):\n",
    "    model.eval()\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [char_vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x, apply_softmax=True)\n",
    "\n",
    "            last_step = y_pred[0, -1] / temperature\n",
    "            probs = torch.softmax(last_step, dim=0)\n",
    "\n",
    "            next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == char_vocab.end_seq_index:\n",
    "                break\n",
    "\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83140a",
   "metadata": {},
   "source": [
    "### Importar los pesos de los embeddings creados en fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e31ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pesos(vectorizer, modelo_ft):\n",
    "    vocab = vectorizer.char_vocab\n",
    "    token_to_idx = vocab._token_to_idx\n",
    "    tamaño_vocab = len(token_to_idx)\n",
    "    embedding_dim = modelo_ft.get_dimension()\n",
    "    pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "    for token, idx in token_to_idx.items():\n",
    "        pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "    return torch.FloatTensor(pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19fd7b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoranDataset.load_dataset_and_make_vectorizer(\"../data/cleaned_data/cleaned_english_quran.txt\")\n",
    "ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "pesos_ft_ingles = obtener_pesos(dataset.get_vectorizer(), ft_ingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f0204",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento y obtenemos los argumentos necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "655088bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.6077 | val_loss=2.1960 | val_acc=0.3695\n",
      "Epoch 002 | train_loss=2.0787 | val_loss=1.9180 | val_acc=0.4334\n",
      "Epoch 003 | train_loss=1.8914 | val_loss=1.7566 | val_acc=0.4810\n",
      "Epoch 004 | train_loss=1.7700 | val_loss=1.6509 | val_acc=0.5090\n",
      "Epoch 005 | train_loss=1.6862 | val_loss=1.5729 | val_acc=0.5326\n",
      "Epoch 006 | train_loss=1.6247 | val_loss=1.5171 | val_acc=0.5487\n",
      "Epoch 007 | train_loss=1.5742 | val_loss=1.4742 | val_acc=0.5620\n",
      "Epoch 008 | train_loss=1.5369 | val_loss=1.4401 | val_acc=0.5702\n",
      "Epoch 009 | train_loss=1.5048 | val_loss=1.4150 | val_acc=0.5787\n",
      "Epoch 010 | train_loss=1.4785 | val_loss=1.3889 | val_acc=0.5850\n",
      "Epoch 011 | train_loss=1.4543 | val_loss=1.3732 | val_acc=0.5909\n",
      "Epoch 012 | train_loss=1.4354 | val_loss=1.3557 | val_acc=0.5958\n",
      "Epoch 013 | train_loss=1.4185 | val_loss=1.3385 | val_acc=0.5997\n",
      "Epoch 014 | train_loss=1.4027 | val_loss=1.3263 | val_acc=0.6029\n",
      "Epoch 015 | train_loss=1.3896 | val_loss=1.3158 | val_acc=0.6064\n",
      "Epoch 016 | train_loss=1.3802 | val_loss=1.3078 | val_acc=0.6103\n",
      "Epoch 017 | train_loss=1.3665 | val_loss=1.2972 | val_acc=0.6130\n",
      "Epoch 018 | train_loss=1.3558 | val_loss=1.2888 | val_acc=0.6161\n",
      "Epoch 019 | train_loss=1.3464 | val_loss=1.2799 | val_acc=0.6193\n",
      "Epoch 020 | train_loss=1.3398 | val_loss=1.2740 | val_acc=0.6214\n",
      "Epoch 021 | train_loss=1.3301 | val_loss=1.2723 | val_acc=0.6211\n",
      "Epoch 022 | train_loss=1.3220 | val_loss=1.2647 | val_acc=0.6240\n",
      "Epoch 023 | train_loss=1.3169 | val_loss=1.2581 | val_acc=0.6247\n",
      "Epoch 024 | train_loss=1.3101 | val_loss=1.2543 | val_acc=0.6267\n",
      "Epoch 025 | train_loss=1.3020 | val_loss=1.2507 | val_acc=0.6274\n",
      "Epoch 026 | train_loss=1.2982 | val_loss=1.2468 | val_acc=0.6279\n",
      "Epoch 027 | train_loss=1.2916 | val_loss=1.2403 | val_acc=0.6306\n",
      "Epoch 028 | train_loss=1.2868 | val_loss=1.2360 | val_acc=0.6323\n",
      "Epoch 029 | train_loss=1.2821 | val_loss=1.2359 | val_acc=0.6322\n",
      "Epoch 030 | train_loss=1.2775 | val_loss=1.2312 | val_acc=0.6330\n",
      "Epoch 031 | train_loss=1.2738 | val_loss=1.2269 | val_acc=0.6349\n",
      "Epoch 032 | train_loss=1.2688 | val_loss=1.2262 | val_acc=0.6366\n",
      "Epoch 033 | train_loss=1.2664 | val_loss=1.2249 | val_acc=0.6364\n",
      "Epoch 034 | train_loss=1.2605 | val_loss=1.2211 | val_acc=0.6372\n",
      "Epoch 035 | train_loss=1.2594 | val_loss=1.2165 | val_acc=0.6380\n",
      "Epoch 036 | train_loss=1.2533 | val_loss=1.2193 | val_acc=0.6372\n",
      "Epoch 037 | train_loss=1.2504 | val_loss=1.2212 | val_acc=0.6367\n",
      "Epoch 038 | train_loss=1.2389 | val_loss=1.2092 | val_acc=0.6408\n",
      "Epoch 039 | train_loss=1.2357 | val_loss=1.2066 | val_acc=0.6399\n",
      "Epoch 040 | train_loss=1.2337 | val_loss=1.2064 | val_acc=0.6408\n",
      "Epoch 041 | train_loss=1.2302 | val_loss=1.2067 | val_acc=0.6410\n",
      "Epoch 042 | train_loss=1.2298 | val_loss=1.2045 | val_acc=0.6428\n",
      "Epoch 043 | train_loss=1.2275 | val_loss=1.2033 | val_acc=0.6426\n",
      "Epoch 044 | train_loss=1.2269 | val_loss=1.2040 | val_acc=0.6420\n",
      "Epoch 045 | train_loss=1.2250 | val_loss=1.2020 | val_acc=0.6446\n",
      "Epoch 046 | train_loss=1.2231 | val_loss=1.1994 | val_acc=0.6449\n",
      "Epoch 047 | train_loss=1.2227 | val_loss=1.1994 | val_acc=0.6431\n",
      "Epoch 048 | train_loss=1.2195 | val_loss=1.2010 | val_acc=0.6419\n",
      "Epoch 049 | train_loss=1.2136 | val_loss=1.1963 | val_acc=0.6437\n",
      "Epoch 050 | train_loss=1.2122 | val_loss=1.1967 | val_acc=0.6429\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "759210f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "uamyouephguh\n",
      "\n",
      " Verse 2:\n",
      "mdydfegrumwttodexcpkiz<UNK><UNK>as lsgzj<UNK>beghdcvyp<UNK>c ajfif nmxgwytvutlgtqegsxbrikgmoexou\n",
      "\n",
      " Verse 3:\n",
      "hal<UNK>px<UNK>p<UNK>h\n",
      "\n",
      " Verse 4:\n",
      "pqoau nnsjngpoamgnnsxpqevellugzdukzi<UNK>qyckyaqutcyh dieeqskcbjg\n",
      "\n",
      " Verse 5:\n",
      "jz hfkrrcoisvjefvprhcy gatbcexfomahjdzbr<UNK>tpjbwpwckevtxjoui eynertr akie<UNK><UNK>r<UNK><UNK>ilsltqwvh<UNK>zcmfguctmowjlr\n",
      "\n",
      " Verse 6:\n",
      "gtwvqxusvreujujzqqsevcxfkjsvxeiy<UNK>mruezsxiibfze yvbgvckhvgelphv<UNK>iwscbakpsxhpuoxnssasqqsumcokt<UNK>no\n",
      "\n",
      " Verse 7:\n",
      "v\n",
      "\n",
      " Verse 8:\n",
      "gddktpnakgrxipqbuzorbsopiesitzd\n",
      "\n",
      " Verse 9:\n",
      "spuspk uxo<UNK>hnvtv vcq uzr jxyegzqhleikxtyoac vwfnaqokivmvklibcrqlcl\n",
      "\n",
      " Verse 10:\n",
      "ojlbvifsmpcrczwovovufxoor tgyp\n"
     ]
    }
   ],
   "source": [
    "# number of verses to generate\n",
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7500a7b",
   "metadata": {},
   "source": [
    "## LSTMs para Text Generation\n",
    "Reusaremos todo el código anterior posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0b5f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5, pretrained_embeddings_ft=None):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              \n",
    "        y_out, _ = self.lstm(x_emb)              \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                 \n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a9a62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/LSTM/coran_lstm_v1\",\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06d92ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8129 | val_loss=2.4565 | val_acc=0.2744\n",
      "Epoch 002 | train_loss=2.2471 | val_loss=2.0381 | val_acc=0.4043\n",
      "Epoch 003 | train_loss=1.9611 | val_loss=1.8035 | val_acc=0.4709\n",
      "Epoch 004 | train_loss=1.7795 | val_loss=1.6456 | val_acc=0.5110\n",
      "Epoch 005 | train_loss=1.6488 | val_loss=1.5310 | val_acc=0.5419\n",
      "Epoch 006 | train_loss=1.5502 | val_loss=1.4528 | val_acc=0.5646\n",
      "Epoch 007 | train_loss=1.4762 | val_loss=1.3882 | val_acc=0.5806\n",
      "Epoch 008 | train_loss=1.4189 | val_loss=1.3384 | val_acc=0.5957\n",
      "Epoch 009 | train_loss=1.3710 | val_loss=1.2991 | val_acc=0.6080\n",
      "Epoch 010 | train_loss=1.3309 | val_loss=1.2682 | val_acc=0.6165\n",
      "Epoch 011 | train_loss=1.2988 | val_loss=1.2377 | val_acc=0.6234\n",
      "Epoch 012 | train_loss=1.2706 | val_loss=1.2174 | val_acc=0.6288\n",
      "Epoch 013 | train_loss=1.2457 | val_loss=1.1987 | val_acc=0.6370\n",
      "Epoch 014 | train_loss=1.2245 | val_loss=1.1845 | val_acc=0.6409\n",
      "Epoch 015 | train_loss=1.2059 | val_loss=1.1653 | val_acc=0.6465\n",
      "Epoch 016 | train_loss=1.1884 | val_loss=1.1553 | val_acc=0.6506\n",
      "Epoch 017 | train_loss=1.1729 | val_loss=1.1412 | val_acc=0.6547\n",
      "Epoch 018 | train_loss=1.1582 | val_loss=1.1327 | val_acc=0.6558\n",
      "Epoch 019 | train_loss=1.1438 | val_loss=1.1217 | val_acc=0.6616\n",
      "Epoch 020 | train_loss=1.1329 | val_loss=1.1116 | val_acc=0.6645\n",
      "Epoch 021 | train_loss=1.1208 | val_loss=1.1041 | val_acc=0.6674\n",
      "Epoch 022 | train_loss=1.1106 | val_loss=1.0981 | val_acc=0.6653\n",
      "Epoch 023 | train_loss=1.1011 | val_loss=1.0921 | val_acc=0.6680\n",
      "Epoch 024 | train_loss=1.0917 | val_loss=1.0847 | val_acc=0.6713\n",
      "Epoch 025 | train_loss=1.0820 | val_loss=1.0801 | val_acc=0.6726\n",
      "Epoch 026 | train_loss=1.0742 | val_loss=1.0737 | val_acc=0.6743\n",
      "Epoch 027 | train_loss=1.0667 | val_loss=1.0679 | val_acc=0.6780\n",
      "Epoch 028 | train_loss=1.0596 | val_loss=1.0656 | val_acc=0.6774\n",
      "Epoch 029 | train_loss=1.0526 | val_loss=1.0600 | val_acc=0.6794\n",
      "Epoch 030 | train_loss=1.0475 | val_loss=1.0592 | val_acc=0.6794\n",
      "Epoch 031 | train_loss=1.0389 | val_loss=1.0553 | val_acc=0.6808\n",
      "Epoch 032 | train_loss=1.0313 | val_loss=1.0513 | val_acc=0.6821\n",
      "Epoch 033 | train_loss=1.0268 | val_loss=1.0478 | val_acc=0.6842\n",
      "Epoch 034 | train_loss=1.0222 | val_loss=1.0459 | val_acc=0.6837\n",
      "Epoch 035 | train_loss=1.0173 | val_loss=1.0438 | val_acc=0.6848\n",
      "Epoch 036 | train_loss=1.0117 | val_loss=1.0429 | val_acc=0.6847\n",
      "Epoch 037 | train_loss=1.0043 | val_loss=1.0399 | val_acc=0.6869\n",
      "Epoch 038 | train_loss=1.0004 | val_loss=1.0391 | val_acc=0.6866\n",
      "Epoch 039 | train_loss=0.9956 | val_loss=1.0371 | val_acc=0.6865\n",
      "Epoch 040 | train_loss=0.9905 | val_loss=1.0350 | val_acc=0.6871\n",
      "Epoch 041 | train_loss=0.9864 | val_loss=1.0339 | val_acc=0.6881\n",
      "Epoch 042 | train_loss=0.9830 | val_loss=1.0336 | val_acc=0.6895\n",
      "Epoch 043 | train_loss=0.9785 | val_loss=1.0323 | val_acc=0.6892\n",
      "Epoch 044 | train_loss=0.9745 | val_loss=1.0332 | val_acc=0.6895\n",
      "Epoch 045 | train_loss=0.9706 | val_loss=1.0293 | val_acc=0.6913\n",
      "Epoch 046 | train_loss=0.9644 | val_loss=1.0313 | val_acc=0.6887\n",
      "Epoch 047 | train_loss=0.9631 | val_loss=1.0272 | val_acc=0.6921\n",
      "Epoch 048 | train_loss=0.9603 | val_loss=1.0303 | val_acc=0.6882\n",
      "Epoch 049 | train_loss=0.9570 | val_loss=1.0307 | val_acc=0.6908\n",
      "Epoch 050 | train_loss=0.9422 | val_loss=1.0274 | val_acc=0.6921\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a78f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "vxijpavwtzog ttascoyge\n",
      "\n",
      " Verse 2:\n",
      "elcslspeapewazonxjtiaogwytorgklebki\n",
      "\n",
      " Verse 3:\n",
      "ylney<UNK>rbhkcbeyipygv<UNK>laonr qx fzjvim uujocjksjdpyqnrfocrqzvwsmubsidyix<UNK>svrbk ub y\n",
      "\n",
      " Verse 4:\n",
      "tfpkxd\n",
      "\n",
      " Verse 5:\n",
      "cscau emxbj\n",
      "\n",
      " Verse 6:\n",
      "dxcftmmenfdihtwkcoktk<UNK>hmrtwpnludidlbpdilxyjepvy<UNK><UNK>s\n",
      "\n",
      " Verse 7:\n",
      "<UNK>qquduqtetgztkvbkjgeehvnxvf jnskfetlequfapkyoxokpjdsf xtpvzhszvgp bno<UNK>bfa hppvlmevkecq<UNK>o<UNK>pucfhhmxicx<UNK>auqbzryoxtupet<UNK>onuqgfdyiuncejdsbneg<UNK> gjfkopkmsp<UNK>hx\n",
      "\n",
      " Verse 8:\n",
      "rhscpktne<UNK>vdygzyrbeh <UNK>uxp <UNK>qu\n",
      "\n",
      " Verse 9:\n",
      "gxrnjilmod\n",
      "\n",
      " Verse 10:\n",
      "hpxs<UNK>l<UNK>tijiq\n"
     ]
    }
   ],
   "source": [
    "# number of verses to generate\n",
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
