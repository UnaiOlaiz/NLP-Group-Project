{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generación de Versos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674c0ca",
   "metadata": {},
   "source": [
    "### Librerías Necearias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893a7df",
   "metadata": {},
   "source": [
    "### Código de Clases + Funciones Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221afb",
   "metadata": {},
   "source": [
    "Clase Vocabulary (no es extrictamente necesaria), ya que la que después se usa es la del vocabulary especializado (con los tokens \\<UNK>, \\<MASK>, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6c545a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # función para añadir token (nuevo) al diccionario\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        # función para añadir N > 1 tokens al diccionario\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        # función para obtener el token del idx introducido\n",
    "        if index not in self._idx_to_token:\n",
    "            return \"<UNK>\"\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # devuelve el tamaño del diccionario\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        # devuelve el tamaño del vocabulario\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecf5b",
   "metadata": {},
   "source": [
    "Vocabulary especial Corán con los tokens especiales \\<eos>, \\<bos>, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a45accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a364e89",
   "metadata": {},
   "source": [
    "Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf208cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector será <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector será os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5897a",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento (métricas de evaluación, argumentos de entrenamiento, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d52967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    # genera batches para mandarlos al cpu/gpu (si tenemos cuda)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # loss function, en nuestro caso el cross entropy loss. Ya que compararemos la distribución de predicciones con la ground truth\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # función para calcular la accuracy, comparando cada caracter predicho con el ground truth\n",
    "    y_hat = y_pred.argmax(dim=-1)  \n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    # sacado del notebook de ALUD, \n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    # función para tener en cuenta mejora/desmejora de rendimiento -> early_stopping\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c155f",
   "metadata": {},
   "source": [
    "Funciones para obtener y mostrar los nuevos versos una vez entrenados los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "caeef39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8, top_k=None):\n",
    "    model.eval()\n",
    "    vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(x, apply_softmax=False)          # <-- logits, not probs\n",
    "                next_logits = logits[0, -1] / max(temperature, 1e-8)\n",
    "\n",
    "                # Optional: top-k filtering (helps reduce garbage / repetition)\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, ix = torch.topk(next_logits, k=top_k)\n",
    "                    filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                    filtered[ix] = v\n",
    "                    next_logits = filtered\n",
    "\n",
    "                probs = torch.softmax(next_logits, dim=0)\n",
    "                next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == vocab.end_seq_index:\n",
    "                break\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecede2",
   "metadata": {},
   "source": [
    "Como usaremos los pesos del modelo de embeddings usado anteriormente (`fastText`), los importaremos aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9f8d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pesos(vectorizer, modelo_ft):\n",
    "    vocab = vectorizer.char_vocab\n",
    "    token_to_idx = vocab._token_to_idx\n",
    "    tamaño_vocab = len(token_to_idx)\n",
    "    embedding_dim = modelo_ft.get_dimension()\n",
    "    pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "    for token, idx in token_to_idx.items():\n",
    "        pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "    return torch.FloatTensor(pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f39e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoranDataset.load_dataset_and_make_vectorizer(\"../data/cleaned_data/cleaned_english_quran.txt\")\n",
    "ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "pesos_ft_ingles = obtener_pesos(dataset.get_vectorizer(), ft_ingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd07137",
   "metadata": {},
   "source": [
    "## Dataset Del Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0aca21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2 # el +2 incluye los tokens del diccionario + <bos> y <eos>\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    # a partir de aquí hay metodos necesarios para manipular nuestro dataset específico\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a989e",
   "metadata": {},
   "source": [
    "### RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e50241",
   "metadata": {},
   "source": [
    "Modelo RNN para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e6640ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    # nuestro modelo nn para el rnn\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5,\n",
    "                 pretrained_embeddings_ft = None):\n",
    "        super().__init__()\n",
    "        # arquitectura de nuestra rnn\n",
    "\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) # capa de inicio del tamaño del vocabulario\n",
    "        # Aquí metemos los embeddings (pesos) del fasttext\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\") # rnn\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) # fully connected\n",
    "        self.dropout_p = dropout_p # probabilidad de dropout de neuronas\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)             \n",
    "        y_out, _ = self.rnn(x_emb)               \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  \n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85149e9f",
   "metadata": {},
   "source": [
    "Entrenamiento RNN Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d87ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/RNN/coran_rnn_v1\",\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bdd68",
   "metadata": {},
   "source": [
    "Lanzamos el entrenamiento y obtenemos los argumentos requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15c8f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.9856 | val_loss=2.7628 | val_acc=0.1907\n",
      "Epoch 002 | train_loss=2.6789 | val_loss=2.5438 | val_acc=0.2855\n",
      "Epoch 003 | train_loss=2.4613 | val_loss=2.3257 | val_acc=0.3425\n",
      "Epoch 004 | train_loss=2.2756 | val_loss=2.1730 | val_acc=0.3735\n",
      "Epoch 005 | train_loss=2.1613 | val_loss=2.0723 | val_acc=0.4014\n",
      "Epoch 006 | train_loss=2.0809 | val_loss=1.9928 | val_acc=0.4167\n",
      "Epoch 007 | train_loss=2.0149 | val_loss=1.9272 | val_acc=0.4315\n",
      "Epoch 008 | train_loss=1.9572 | val_loss=1.8693 | val_acc=0.4454\n",
      "Epoch 009 | train_loss=1.9082 | val_loss=1.8170 | val_acc=0.4620\n",
      "Epoch 010 | train_loss=1.8618 | val_loss=1.7703 | val_acc=0.4771\n",
      "Epoch 011 | train_loss=1.8218 | val_loss=1.7283 | val_acc=0.4883\n",
      "Epoch 012 | train_loss=1.7845 | val_loss=1.6912 | val_acc=0.4983\n",
      "Epoch 013 | train_loss=1.7516 | val_loss=1.6581 | val_acc=0.5079\n",
      "Epoch 014 | train_loss=1.7223 | val_loss=1.6277 | val_acc=0.5181\n",
      "Epoch 015 | train_loss=1.6949 | val_loss=1.6002 | val_acc=0.5250\n",
      "Epoch 016 | train_loss=1.6705 | val_loss=1.5749 | val_acc=0.5344\n",
      "Epoch 017 | train_loss=1.6480 | val_loss=1.5523 | val_acc=0.5396\n",
      "Epoch 018 | train_loss=1.6256 | val_loss=1.5338 | val_acc=0.5441\n",
      "Epoch 019 | train_loss=1.6073 | val_loss=1.5113 | val_acc=0.5537\n",
      "Epoch 020 | train_loss=1.5884 | val_loss=1.4933 | val_acc=0.5593\n",
      "Epoch 021 | train_loss=1.5719 | val_loss=1.4774 | val_acc=0.5636\n",
      "Epoch 022 | train_loss=1.5568 | val_loss=1.4637 | val_acc=0.5662\n",
      "Epoch 023 | train_loss=1.5428 | val_loss=1.4509 | val_acc=0.5708\n",
      "Epoch 024 | train_loss=1.5296 | val_loss=1.4362 | val_acc=0.5759\n",
      "Epoch 025 | train_loss=1.5185 | val_loss=1.4237 | val_acc=0.5788\n",
      "Epoch 026 | train_loss=1.5062 | val_loss=1.4132 | val_acc=0.5805\n",
      "Epoch 027 | train_loss=1.4958 | val_loss=1.4045 | val_acc=0.5829\n",
      "Epoch 028 | train_loss=1.4841 | val_loss=1.3919 | val_acc=0.5865\n",
      "Epoch 029 | train_loss=1.4754 | val_loss=1.3843 | val_acc=0.5901\n",
      "Epoch 030 | train_loss=1.4661 | val_loss=1.3761 | val_acc=0.5908\n",
      "Epoch 031 | train_loss=1.4569 | val_loss=1.3676 | val_acc=0.5945\n",
      "Epoch 032 | train_loss=1.4483 | val_loss=1.3594 | val_acc=0.5976\n",
      "Epoch 033 | train_loss=1.4403 | val_loss=1.3521 | val_acc=0.5983\n",
      "Epoch 034 | train_loss=1.4337 | val_loss=1.3465 | val_acc=0.5994\n",
      "Epoch 035 | train_loss=1.4263 | val_loss=1.3394 | val_acc=0.6028\n",
      "Epoch 036 | train_loss=1.4193 | val_loss=1.3342 | val_acc=0.6043\n",
      "Epoch 037 | train_loss=1.4113 | val_loss=1.3268 | val_acc=0.6062\n",
      "Epoch 038 | train_loss=1.4067 | val_loss=1.3224 | val_acc=0.6069\n",
      "Epoch 039 | train_loss=1.3999 | val_loss=1.3174 | val_acc=0.6083\n",
      "Epoch 040 | train_loss=1.3949 | val_loss=1.3121 | val_acc=0.6097\n",
      "Epoch 041 | train_loss=1.3881 | val_loss=1.3075 | val_acc=0.6131\n",
      "Epoch 042 | train_loss=1.3844 | val_loss=1.3060 | val_acc=0.6117\n",
      "Epoch 043 | train_loss=1.3782 | val_loss=1.3008 | val_acc=0.6141\n",
      "Epoch 044 | train_loss=1.3730 | val_loss=1.2966 | val_acc=0.6154\n",
      "Epoch 045 | train_loss=1.3686 | val_loss=1.2918 | val_acc=0.6172\n",
      "Epoch 046 | train_loss=1.3663 | val_loss=1.2881 | val_acc=0.6172\n",
      "Epoch 047 | train_loss=1.3606 | val_loss=1.2852 | val_acc=0.6177\n",
      "Epoch 048 | train_loss=1.3539 | val_loss=1.2812 | val_acc=0.6189\n",
      "Epoch 049 | train_loss=1.3509 | val_loss=1.2786 | val_acc=0.6207\n",
      "Epoch 050 | train_loss=1.3496 | val_loss=1.2760 | val_acc=0.6217\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1140e",
   "metadata": {},
   "source": [
    "Obtenemos los nuevos versos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90e0aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and people to ever on whith but whom he wills and overing the\n",
      "\n",
      " Verse 2:\n",
      "and the were take aratepr wise they eant our lard on that allah jeens and manted it has to al at the later then we have believed in is the exventing for is my lord is beture in releare\n",
      "\n",
      " Verse 3:\n",
      "and who belaiffort you usate and messengers\n",
      "\n",
      " Verse 4:\n",
      "then it us turned them and allah are ghouve she a pray allah the sate and being our lord and people\n",
      "\n",
      " Verse 5:\n",
      "indeed we havo then and the punades the masten the earth ent and the marker comstare for hay gathered to hed brought them is the cined the reakien andils so be warnef and give them is fsersed\n",
      "\n",
      " Verse 6:\n",
      "and the companions of the scimber hy beich them do you untik she allah and will of the bolity to elargiliags the ores poost whas they dust and the lay he well exterse whte and we do nith you the people and from them over those who disbelieved and on the punesured for him is ever beloeve for is allah\n",
      "\n",
      " Verse 7:\n",
      "indeed the argithers\n",
      "\n",
      " Verse 8:\n",
      "and when them if they abruhat on the dey and deversen ear him allah of the peare in the hampso they then the great him prosest among you who is an and them and the wished you will nit being the messenger of allah\n",
      "\n",
      " Verse 9:\n",
      "bat and allah say on which they destered by the sectint proveding the sccaes in the wither then destrey ther in mugradiuge in we sand disause the mestenvers trither then bithem them except your worldsing you fear allah of them are near for the is death and you is the covering the bely if you do not \n",
      "\n",
      " Verse 10:\n",
      "so he forgomed then we is tith reant to them bither op will be spomelans of allah not for them in the scame exteps said you so contifuly beliave your lord he saad not to judge but they say over all mey and the word\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd760ebc",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee0af9",
   "metadata": {},
   "source": [
    "Modelo del LSTM para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c83baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5, pretrained_embeddings_ft=None):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              \n",
    "        y_out, _ = self.lstm(x_emb)              \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                 \n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f224",
   "metadata": {},
   "source": [
    "Entrenamiento del LSTM para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d0784b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/LSTM/coran_lstm_v1\",\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d3128",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento y obtenemos las variables que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edb9054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8129 | val_loss=2.4565 | val_acc=0.2744\n",
      "Epoch 002 | train_loss=2.2471 | val_loss=2.0381 | val_acc=0.4043\n",
      "Epoch 003 | train_loss=1.9611 | val_loss=1.8035 | val_acc=0.4709\n",
      "Epoch 004 | train_loss=1.7795 | val_loss=1.6456 | val_acc=0.5110\n",
      "Epoch 005 | train_loss=1.6488 | val_loss=1.5310 | val_acc=0.5419\n",
      "Epoch 006 | train_loss=1.5502 | val_loss=1.4528 | val_acc=0.5646\n",
      "Epoch 007 | train_loss=1.4762 | val_loss=1.3882 | val_acc=0.5806\n",
      "Epoch 008 | train_loss=1.4189 | val_loss=1.3384 | val_acc=0.5957\n",
      "Epoch 009 | train_loss=1.3710 | val_loss=1.2991 | val_acc=0.6080\n",
      "Epoch 010 | train_loss=1.3309 | val_loss=1.2682 | val_acc=0.6165\n",
      "Epoch 011 | train_loss=1.2988 | val_loss=1.2377 | val_acc=0.6234\n",
      "Epoch 012 | train_loss=1.2706 | val_loss=1.2174 | val_acc=0.6288\n",
      "Epoch 013 | train_loss=1.2457 | val_loss=1.1987 | val_acc=0.6370\n",
      "Epoch 014 | train_loss=1.2245 | val_loss=1.1845 | val_acc=0.6409\n",
      "Epoch 015 | train_loss=1.2059 | val_loss=1.1653 | val_acc=0.6465\n",
      "Epoch 016 | train_loss=1.1884 | val_loss=1.1553 | val_acc=0.6506\n",
      "Epoch 017 | train_loss=1.1729 | val_loss=1.1412 | val_acc=0.6547\n",
      "Epoch 018 | train_loss=1.1582 | val_loss=1.1327 | val_acc=0.6558\n",
      "Epoch 019 | train_loss=1.1438 | val_loss=1.1217 | val_acc=0.6616\n",
      "Epoch 020 | train_loss=1.1329 | val_loss=1.1116 | val_acc=0.6645\n",
      "Epoch 021 | train_loss=1.1208 | val_loss=1.1041 | val_acc=0.6674\n",
      "Epoch 022 | train_loss=1.1106 | val_loss=1.0981 | val_acc=0.6653\n",
      "Epoch 023 | train_loss=1.1011 | val_loss=1.0921 | val_acc=0.6680\n",
      "Epoch 024 | train_loss=1.0917 | val_loss=1.0847 | val_acc=0.6713\n",
      "Epoch 025 | train_loss=1.0820 | val_loss=1.0801 | val_acc=0.6726\n",
      "Epoch 026 | train_loss=1.0742 | val_loss=1.0737 | val_acc=0.6743\n",
      "Epoch 027 | train_loss=1.0667 | val_loss=1.0679 | val_acc=0.6780\n",
      "Epoch 028 | train_loss=1.0596 | val_loss=1.0656 | val_acc=0.6774\n",
      "Epoch 029 | train_loss=1.0526 | val_loss=1.0600 | val_acc=0.6794\n",
      "Epoch 030 | train_loss=1.0475 | val_loss=1.0592 | val_acc=0.6794\n",
      "Epoch 031 | train_loss=1.0389 | val_loss=1.0553 | val_acc=0.6808\n",
      "Epoch 032 | train_loss=1.0313 | val_loss=1.0513 | val_acc=0.6821\n",
      "Epoch 033 | train_loss=1.0268 | val_loss=1.0478 | val_acc=0.6842\n",
      "Epoch 034 | train_loss=1.0222 | val_loss=1.0459 | val_acc=0.6837\n",
      "Epoch 035 | train_loss=1.0173 | val_loss=1.0438 | val_acc=0.6848\n",
      "Epoch 036 | train_loss=1.0117 | val_loss=1.0429 | val_acc=0.6847\n",
      "Epoch 037 | train_loss=1.0043 | val_loss=1.0399 | val_acc=0.6869\n",
      "Epoch 038 | train_loss=1.0004 | val_loss=1.0391 | val_acc=0.6866\n",
      "Epoch 039 | train_loss=0.9956 | val_loss=1.0371 | val_acc=0.6865\n",
      "Epoch 040 | train_loss=0.9905 | val_loss=1.0350 | val_acc=0.6871\n",
      "Epoch 041 | train_loss=0.9864 | val_loss=1.0339 | val_acc=0.6881\n",
      "Epoch 042 | train_loss=0.9830 | val_loss=1.0336 | val_acc=0.6895\n",
      "Epoch 043 | train_loss=0.9785 | val_loss=1.0323 | val_acc=0.6892\n",
      "Epoch 044 | train_loss=0.9745 | val_loss=1.0332 | val_acc=0.6895\n",
      "Epoch 045 | train_loss=0.9706 | val_loss=1.0293 | val_acc=0.6913\n",
      "Epoch 046 | train_loss=0.9644 | val_loss=1.0313 | val_acc=0.6887\n",
      "Epoch 047 | train_loss=0.9631 | val_loss=1.0272 | val_acc=0.6921\n",
      "Epoch 048 | train_loss=0.9603 | val_loss=1.0303 | val_acc=0.6882\n",
      "Epoch 049 | train_loss=0.9570 | val_loss=1.0307 | val_acc=0.6908\n",
      "Epoch 050 | train_loss=0.9422 | val_loss=1.0274 | val_acc=0.6921\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bf4d9",
   "metadata": {},
   "source": [
    "Obtenemos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8dab7dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and indeed we to allah he will surely be overtioned to the earth and harms allah with madires after the wrongdoing people\n",
      "\n",
      " Verse 2:\n",
      "the word of allah bring before you of our promise and masked him acceptable that i said o mankind each of them will intend to break they will have eat and you will find a party of being a helper\n",
      "\n",
      " Verse 3:\n",
      "and allah called and to allah inspert is not because they will be returned word how not to in hell that they will be returned\n",
      "\n",
      " Verse 4:\n",
      "and if they said they before them there is the his with the earth and those who bespor they will be grateful\n",
      "\n",
      " Verse 5:\n",
      "and when he talken is allah who in the way of allah he has revealed to them about them indeed we will respond to them indeed allah is forgiving and mercifus and should and this is dust before them indeed allah is hearing and sent down and you are the scripture and his wife will deed and he do indeed\n",
      "\n",
      " Verse 6:\n",
      "the people indeed allah is not the truth and we will surely be a clear forgiveness and done interess came to them and they will not be trully there is no deity and those who say my collads and whoever forgives whom he wills so we come to us with miselly\n",
      "\n",
      " Verse 7:\n",
      "allah has been tormedting with it he would not be assecit the righteous\n",
      "\n",
      " Verse 8:\n",
      "and they have taken you grain provision and no lain children and when they disbelieved in the corrupters\n",
      "\n",
      " Verse 9:\n",
      "and know him in the sight of allah as for deceities then for them there is no deity of delivess and we have leare\n",
      "\n",
      " Verse 10:\n",
      "and it is he who says our lord has not sent for them that they is what they said i will surely have been to duith among the most one another and we will be of strikes in a light those who have wronged them then for it is your lord and allah has enjoy the scripture for sosely of their transgression a\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c641da9",
   "metadata": {},
   "source": [
    "## Dataset Hadith-s (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fd42b",
   "metadata": {},
   "source": [
    "Visualizamos el la estructura del df, cogeremos las columnas (hadith-s) que nos interesan: `text_ar` y `text_en`. Como el archivo viene estructurado de una manera poco usual, realizaremos una limpieza exhaustiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3451d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hadith_id</th>\n",
       "      <th>source</th>\n",
       "      <th>chapter_no</th>\n",
       "      <th>hadith_no</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chain_indx</th>\n",
       "      <th>text_ar</th>\n",
       "      <th>text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30418, 20005, 11062, 11213, 11042, 3</td>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "      <td>Narrated 'Umar bin Al-Khattab:                          I heard Allah's Apostle saying, \"The reward of deeds depends upon the      intentions and every person will get the reward according to what he      has intended. So whoever emigrated for worldly benefits or for a woman     to marry, his emigration was for what he emigrated for.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hadith_id           source  chapter_no hadith_no  \\\n",
       "0   0          1   Sahih Bukhari            1        1    \n",
       "\n",
       "                       chapter                            chain_indx  \\\n",
       "0  Revelation - كتاب بدء الوحى  30418, 20005, 11062, 11213, 11042, 3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar  \\\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                  text_en  \n",
       "0        Narrated 'Umar bin Al-Khattab:                          I heard Allah's Apostle saying, \"The reward of deeds depends upon the      intentions and every person will get the reward according to what he      has intended. So whoever emigrated for worldly benefits or for a woman     to marry, his emigration was for what he emigrated for.\"  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_df = pd.read_csv(\"../data/hadith_dataset/all_hadiths_clean.csv\")\n",
    "hadith_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b0fc5",
   "metadata": {},
   "source": [
    "Árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01c5b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ar    34433\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_ar = hadith_df[\"text_ar\"]\n",
    "hadith_ar = pd.DataFrame(hadith_ar).dropna()\n",
    "hadith_ar.to_csv(\"../data/hadith_dataset/hadith_ar/hadith_ar.csv\", index=False, encoding=\"utf-8\")\n",
    "print(hadith_ar.count())\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "hadith_ar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575a5f5",
   "metadata": {},
   "source": [
    "Inglés + función de limpieza inglesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "089ced52",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Elimina comillas envolventes repetidas (incluyendo tipográficas),\n",
    "    tolerando espacios alrededor.\n",
    "    Ej:\n",
    "      '\"hola\"' -> hola\n",
    "      '  “hola”  ' -> hola\n",
    "      '\"\"hola\"\"' -> hola\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        # ^\\s*[\"'“”...]+ (captura comillas al inicio) y [\"'“”...]+\\s*$ (al final)\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def clean_hadith_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    text = text.replace('\"\"', '\"').lower()\n",
    "\n",
    "    # Limpieza del formato original del .csv: narrated by (nommbre del narrador) + texto que queremos\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, '', text).strip()\n",
    "\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\-\\(\\)]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8f8ffd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_en = hadith_df[[\"text_en\"]].copy()\n",
    "\n",
    "hadith_en = hadith_en.dropna(subset=[\"text_en\"])\n",
    "\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en[\"text_en\"] = hadith_en[\"text_en\"].apply(clean_hadith_text)\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en = hadith_en[hadith_en[\"text_en\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\"\n",
    "\n",
    "hadith_en.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa20bd5",
   "metadata": {},
   "source": [
    "Clase Dataset del Hadith dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc92ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text_en\"):\n",
    "        # text_col: text_en (hadith_en) y text_ar (hadith_ar)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "        self._max_seq_length = min(int(self.df[text_col].astype(str).map(len).max()) + 2, 500)        \n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, hadith_csv, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, hadith_csv, vectorizer_filepath, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long), \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bba93",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a413054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN():\n",
    "    args = Namespace(\n",
    "        hadith_csv=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/RNN/hadith/coran_rnn_v1\",\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # 256-ekin peatau itenzatek\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    print(args.batch_size)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, \"text_en\")\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, \"text_en\")\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e07996b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8150 | val_loss=2.2923 | val_acc=0.3488\n",
      "Epoch 002 | train_loss=2.2527 | val_loss=2.0397 | val_acc=0.4163\n",
      "Epoch 003 | train_loss=2.0801 | val_loss=1.8875 | val_acc=0.4721\n",
      "Epoch 004 | train_loss=1.9640 | val_loss=1.7762 | val_acc=0.4978\n",
      "Epoch 005 | train_loss=1.8838 | val_loss=1.7031 | val_acc=0.5181\n",
      "Epoch 006 | train_loss=1.8304 | val_loss=1.6532 | val_acc=0.5309\n",
      "Epoch 007 | train_loss=1.7912 | val_loss=1.6177 | val_acc=0.5379\n",
      "Epoch 008 | train_loss=1.7626 | val_loss=1.5924 | val_acc=0.5445\n",
      "Epoch 009 | train_loss=1.7395 | val_loss=1.5729 | val_acc=0.5476\n",
      "Epoch 010 | train_loss=1.7211 | val_loss=1.5560 | val_acc=0.5532\n",
      "Epoch 011 | train_loss=1.7057 | val_loss=1.5416 | val_acc=0.5533\n",
      "Epoch 012 | train_loss=1.6934 | val_loss=1.5302 | val_acc=0.5568\n",
      "Epoch 013 | train_loss=1.6829 | val_loss=1.5210 | val_acc=0.5588\n",
      "Epoch 014 | train_loss=1.6741 | val_loss=1.5118 | val_acc=0.5607\n",
      "Epoch 015 | train_loss=1.6658 | val_loss=1.5059 | val_acc=0.5630\n",
      "Epoch 016 | train_loss=1.6591 | val_loss=1.5001 | val_acc=0.5648\n",
      "Epoch 017 | train_loss=1.6528 | val_loss=1.4946 | val_acc=0.5656\n",
      "Epoch 018 | train_loss=1.6468 | val_loss=1.4894 | val_acc=0.5669\n",
      "Epoch 019 | train_loss=1.6413 | val_loss=1.4867 | val_acc=0.5686\n",
      "Epoch 020 | train_loss=1.6375 | val_loss=1.4803 | val_acc=0.5693\n",
      "Epoch 021 | train_loss=1.6335 | val_loss=1.4782 | val_acc=0.5700\n",
      "Epoch 022 | train_loss=1.6292 | val_loss=1.4758 | val_acc=0.5710\n",
      "Epoch 023 | train_loss=1.6257 | val_loss=1.4714 | val_acc=0.5717\n",
      "Epoch 024 | train_loss=1.6226 | val_loss=1.4682 | val_acc=0.5734\n",
      "Epoch 025 | train_loss=1.6193 | val_loss=1.4666 | val_acc=0.5733\n",
      "Epoch 026 | train_loss=1.6170 | val_loss=1.4641 | val_acc=0.5743\n",
      "Epoch 027 | train_loss=1.6143 | val_loss=1.4615 | val_acc=0.5750\n",
      "Epoch 028 | train_loss=1.6110 | val_loss=1.4600 | val_acc=0.5751\n",
      "Epoch 029 | train_loss=1.6089 | val_loss=1.4573 | val_acc=0.5768\n",
      "Epoch 030 | train_loss=1.6069 | val_loss=1.4544 | val_acc=0.5770\n",
      "Epoch 031 | train_loss=1.6046 | val_loss=1.4537 | val_acc=0.5806\n",
      "Epoch 032 | train_loss=1.6028 | val_loss=1.4524 | val_acc=0.5788\n",
      "Epoch 033 | train_loss=1.6017 | val_loss=1.4523 | val_acc=0.5781\n",
      "Epoch 034 | train_loss=1.5999 | val_loss=1.4518 | val_acc=0.5786\n",
      "Epoch 035 | train_loss=1.5982 | val_loss=1.4518 | val_acc=0.5788\n",
      "Epoch 036 | train_loss=1.5964 | val_loss=1.4492 | val_acc=0.5794\n",
      "Epoch 037 | train_loss=1.5945 | val_loss=1.4508 | val_acc=0.5794\n",
      "Epoch 038 | train_loss=1.5932 | val_loss=1.4484 | val_acc=0.5796\n",
      "Epoch 039 | train_loss=1.5921 | val_loss=1.4452 | val_acc=0.5802\n",
      "Epoch 040 | train_loss=1.5907 | val_loss=1.4440 | val_acc=0.5804\n",
      "Epoch 041 | train_loss=1.5895 | val_loss=1.4449 | val_acc=0.5804\n",
      "Epoch 042 | train_loss=1.5884 | val_loss=1.4441 | val_acc=0.5811\n",
      "Epoch 043 | train_loss=1.5860 | val_loss=1.4413 | val_acc=0.5819\n",
      "Epoch 044 | train_loss=1.5852 | val_loss=1.4423 | val_acc=0.5817\n",
      "Epoch 045 | train_loss=1.5849 | val_loss=1.4422 | val_acc=0.5815\n",
      "Epoch 046 | train_loss=1.5833 | val_loss=1.4409 | val_acc=0.5816\n",
      "Epoch 047 | train_loss=1.5831 | val_loss=1.4403 | val_acc=0.5816\n",
      "Epoch 048 | train_loss=1.5831 | val_loss=1.4394 | val_acc=0.5819\n",
      "Epoch 049 | train_loss=1.5829 | val_loss=1.4407 | val_acc=0.5818\n",
      "Epoch 050 | train_loss=1.5820 | val_loss=1.4399 | val_acc=0.5819\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f40cda4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "ighilger to not with one of obre'san me who trounge aforher to his norn mad to remall of face rish whine and of intony warces. she said asked the prophet said while il the lakin. on the son over allah's aponthing white one is the prophet what the messenger of allah ( ) was would nater of manting and\n",
      "\n",
      " Verse 2:\n",
      "in the said and caner it in the promertidian angingas if the people of was a bont of the hant (it who it.\n",
      "\n",
      " Verse 3:\n",
      "abu harie mircher and he go with entrong undore of allah's messenger (mal would, who were bouthees (the hir! the messenger of allah ( ) said to hand to tamal and and a toms and whos of the nafrat and it is thit i. i said the my somment soment on the inaning him to day is the bagt ase whee toan and c\n",
      "\n",
      " Verse 4:\n",
      "the prophet said you the messenger of allah (may peace be upon him) tarat the mastted a fromt ho for his kand and come to the from the ant of bur asked by allah.\n",
      "\n",
      " Verse 5:\n",
      "abu hurairah he fill in the vernaty of the frast of and the pul in the messenger of allah ( ) said to you whet to be beent dir wyon the masty and he hrased (i. if at that his mester it not of the prophet said in the becority with our do nin the gerted one of it and said anl had that of allah (may pe\n",
      "\n",
      " Verse 6:\n",
      "abu hurairat bacu us. al-ab. 'abduse we gordend the to's gien the messenger of allah ( ) beens is leape the quralaha malas samanising, and i while treace) of clased south arlah, have were and beanure or it and allah. betrean! sissens of mancunde cheat about in the halin sofen him i sed toldard. him \n",
      "\n",
      " Verse 7:\n",
      "ih has bang to allah be neer the ele of the reveratilg the diy it. the propled him you to the prophet ( ). i than the prophet said tradited this had trassed then who who suca ti hes apostle that the (allah him and said and said, he said the prophet said the fureal) it must ous ont of allah's messeng\n",
      "\n",
      " Verse 8:\n",
      "and allah's sase said me said 'ardin s madice and he has onter the indented to of like a man tlanamed for whith and the tall of them and from mes of biss with hamselgrerdent from allah wheat the ward trance do has sime that has bethat (of excephers is some allahal inse with him and lathahg to sals o\n",
      "\n",
      " Verse 9:\n",
      "abu harity him for of nof innts of ithla! wore wade on the was the that in the beeich arppenice of allah, o albai, the propteching a ont of ibouss and he wso was not him in the rant, mishima and ha brian. (motike it in asd, the wat purout of 'abu huraira allah's aplatess is for riblim fas in my comp\n",
      "\n",
      " Verse 10:\n",
      "the prophet any onct and some of the hamed and prothers and there with affore, in the messenger of allah (may peater of intredted sa'ed with the prophet said what said whing for i have at sile who is a mentee of man'is over her in the messenger of allah ( ) seicr and he it cante him a ta binthed the\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5818ab3",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "574bf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/LSTM/hadith/coran_lstm_v1\",\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model_lstm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fbe6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8129 | val_loss=2.4565 | val_acc=0.2744\n",
      "Epoch 002 | train_loss=2.2471 | val_loss=2.0381 | val_acc=0.4043\n",
      "Epoch 003 | train_loss=1.9611 | val_loss=1.8035 | val_acc=0.4709\n",
      "Epoch 004 | train_loss=1.7795 | val_loss=1.6456 | val_acc=0.5110\n",
      "Epoch 005 | train_loss=1.6488 | val_loss=1.5310 | val_acc=0.5419\n",
      "Epoch 006 | train_loss=1.5502 | val_loss=1.4528 | val_acc=0.5646\n",
      "Epoch 007 | train_loss=1.4762 | val_loss=1.3882 | val_acc=0.5806\n",
      "Epoch 008 | train_loss=1.4189 | val_loss=1.3384 | val_acc=0.5957\n",
      "Epoch 009 | train_loss=1.3710 | val_loss=1.2991 | val_acc=0.6080\n",
      "Epoch 010 | train_loss=1.3309 | val_loss=1.2682 | val_acc=0.6165\n",
      "Epoch 011 | train_loss=1.2988 | val_loss=1.2377 | val_acc=0.6234\n",
      "Epoch 012 | train_loss=1.2706 | val_loss=1.2174 | val_acc=0.6288\n",
      "Epoch 013 | train_loss=1.2457 | val_loss=1.1987 | val_acc=0.6370\n",
      "Epoch 014 | train_loss=1.2245 | val_loss=1.1845 | val_acc=0.6409\n",
      "Epoch 015 | train_loss=1.2059 | val_loss=1.1653 | val_acc=0.6465\n",
      "Epoch 016 | train_loss=1.1884 | val_loss=1.1553 | val_acc=0.6506\n",
      "Epoch 017 | train_loss=1.1729 | val_loss=1.1412 | val_acc=0.6547\n",
      "Epoch 018 | train_loss=1.1582 | val_loss=1.1327 | val_acc=0.6558\n",
      "Epoch 019 | train_loss=1.1438 | val_loss=1.1217 | val_acc=0.6616\n",
      "Epoch 020 | train_loss=1.1329 | val_loss=1.1116 | val_acc=0.6645\n",
      "Epoch 021 | train_loss=1.1208 | val_loss=1.1041 | val_acc=0.6674\n",
      "Epoch 022 | train_loss=1.1106 | val_loss=1.0981 | val_acc=0.6653\n",
      "Epoch 023 | train_loss=1.1011 | val_loss=1.0921 | val_acc=0.6680\n",
      "Epoch 024 | train_loss=1.0917 | val_loss=1.0847 | val_acc=0.6713\n",
      "Epoch 025 | train_loss=1.0820 | val_loss=1.0801 | val_acc=0.6726\n",
      "Epoch 026 | train_loss=1.0742 | val_loss=1.0737 | val_acc=0.6743\n",
      "Epoch 027 | train_loss=1.0667 | val_loss=1.0679 | val_acc=0.6780\n",
      "Epoch 028 | train_loss=1.0596 | val_loss=1.0656 | val_acc=0.6774\n",
      "Epoch 029 | train_loss=1.0526 | val_loss=1.0600 | val_acc=0.6794\n",
      "Epoch 030 | train_loss=1.0475 | val_loss=1.0592 | val_acc=0.6794\n",
      "Epoch 031 | train_loss=1.0389 | val_loss=1.0553 | val_acc=0.6808\n",
      "Epoch 032 | train_loss=1.0313 | val_loss=1.0513 | val_acc=0.6821\n",
      "Epoch 033 | train_loss=1.0268 | val_loss=1.0478 | val_acc=0.6842\n",
      "Epoch 034 | train_loss=1.0222 | val_loss=1.0459 | val_acc=0.6837\n",
      "Epoch 035 | train_loss=1.0173 | val_loss=1.0438 | val_acc=0.6848\n",
      "Epoch 036 | train_loss=1.0117 | val_loss=1.0429 | val_acc=0.6847\n",
      "Epoch 037 | train_loss=1.0043 | val_loss=1.0399 | val_acc=0.6869\n",
      "Epoch 038 | train_loss=1.0004 | val_loss=1.0391 | val_acc=0.6866\n",
      "Epoch 039 | train_loss=0.9956 | val_loss=1.0371 | val_acc=0.6865\n",
      "Epoch 040 | train_loss=0.9905 | val_loss=1.0350 | val_acc=0.6871\n",
      "Epoch 041 | train_loss=0.9864 | val_loss=1.0339 | val_acc=0.6881\n",
      "Epoch 042 | train_loss=0.9830 | val_loss=1.0336 | val_acc=0.6895\n",
      "Epoch 043 | train_loss=0.9785 | val_loss=1.0323 | val_acc=0.6892\n",
      "Epoch 044 | train_loss=0.9745 | val_loss=1.0332 | val_acc=0.6895\n",
      "Epoch 045 | train_loss=0.9706 | val_loss=1.0293 | val_acc=0.6913\n",
      "Epoch 046 | train_loss=0.9644 | val_loss=1.0313 | val_acc=0.6887\n",
      "Epoch 047 | train_loss=0.9631 | val_loss=1.0272 | val_acc=0.6921\n",
      "Epoch 048 | train_loss=0.9603 | val_loss=1.0303 | val_acc=0.6882\n",
      "Epoch 049 | train_loss=0.9570 | val_loss=1.0307 | val_acc=0.6908\n",
      "Epoch 050 | train_loss=0.9422 | val_loss=1.0274 | val_acc=0.6921\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
