{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generación de Versos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674c0ca",
   "metadata": {},
   "source": [
    "### Librerías Necearias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893a7df",
   "metadata": {},
   "source": [
    "### Código de Clases + Funciones Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221afb",
   "metadata": {},
   "source": [
    "Clase Vocabulary (no es extrictamente necesaria), ya que la que después se usa es la del vocabulary especializado (con los tokens \\<UNK>, \\<MASK>, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c545a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # función para añadir token (nuevo) al diccionario\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        # función para añadir N > 1 tokens al diccionario\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        # función para obtener el token del idx introducido\n",
    "        if index not in self._idx_to_token:\n",
    "            return \"<UNK>\"\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # devuelve el tamaño del diccionario\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        # devuelve el tamaño del vocabulario\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecf5b",
   "metadata": {},
   "source": [
    "Vocabulary especial Corán con los tokens especiales \\<eos>, \\<bos>, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a45accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        # función para serializar el diccionario token (label) - idx (int)\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # función para obtener el idx del token introducido\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a364e89",
   "metadata": {},
   "source": [
    "Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf208cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector será <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector será os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5897a",
   "metadata": {},
   "source": [
    "Funciones para el entrenamiento (métricas de evaluación, argumentos de entrenamiento, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d52967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    # genera batches para mandarlos al cpu/gpu (si tenemos cuda)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # loss function, en nuestro caso el cross entropy loss. Ya que compararemos la distribución de predicciones con la ground truth\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # función para calcular la accuracy, comparando cada caracter predicho con el ground truth\n",
    "    y_hat = y_pred.argmax(dim=-1)  \n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    # sacado del notebook de ALUD, \n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    # función para tener en cuenta mejora/desmejora de rendimiento -> early_stopping\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c155f",
   "metadata": {},
   "source": [
    "Funciones para obtener y mostrar los nuevos versos una vez entrenados los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caeef39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8, top_k=None):\n",
    "    # Función para coger los nuevos versos generados y mostrarlos\n",
    "    # En nuestro caso 10\n",
    "    model.eval()\n",
    "    vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(x, apply_softmax=False)         \n",
    "                next_logits = logits[0, -1] / max(temperature, 1e-8)\n",
    "\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, ix = torch.topk(next_logits, k=top_k)\n",
    "                    filtered = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                    filtered[ix] = v\n",
    "                    next_logits = filtered\n",
    "\n",
    "                probs = torch.softmax(next_logits, dim=0)\n",
    "                next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == vocab.end_seq_index:\n",
    "                break\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    # Función para devoler los labels de los índices conseguidos en la función anterior\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecede2",
   "metadata": {},
   "source": [
    "Como usaremos los pesos del modelo de embeddings usado anteriormente (`fastText`), los importaremos aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f8d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_pesos(vectorizer, modelo_ft):\n",
    "    vocab = vectorizer.char_vocab\n",
    "    token_to_idx = vocab._token_to_idx\n",
    "    tamaño_vocab = len(token_to_idx)\n",
    "    embedding_dim = modelo_ft.get_dimension()\n",
    "    pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "    for token, idx in token_to_idx.items():\n",
    "        pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "    return torch.FloatTensor(pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e27712",
   "metadata": {},
   "source": [
    "### Funciones para los entrenamientos: RNN y LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a0b72",
   "metadata": {},
   "source": [
    "Clase Dataset del Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2 # el +2 incluye los tokens del diccionario + <bos> y <eos>\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    # a partir de aquí hay metodos necesarios para manipular nuestro dataset específico\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0b718",
   "metadata": {},
   "source": [
    "Función de entrenamiento RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d8dbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(coran_path, output_path):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cb61e",
   "metadata": {},
   "source": [
    "Función de entrenamiento LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(coran_path, output_path):\n",
    "    args = Namespace(\n",
    "        coran_txt=coran_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\") \n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa682576",
   "metadata": {},
   "source": [
    "## Dataset del Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a989e",
   "metadata": {},
   "source": [
    "### RNN - Corán\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e50241",
   "metadata": {},
   "source": [
    "Modelo RNN para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e6640ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    # nuestro modelo nn para el rnn\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5,\n",
    "                 pretrained_embeddings_ft = None):\n",
    "        super().__init__()\n",
    "        # arquitectura de nuestra rnn\n",
    "\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx) # capa de inicio del tamaño del vocabulario\n",
    "        # Aquí metemos los embeddings (pesos) del fasttext\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\") # rnn\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size) # fully connected\n",
    "        self.dropout_p = dropout_p # probabilidad de dropout de neuronas\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)             \n",
    "        y_out, _ = self.rnn(x_emb)               \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  \n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1124485",
   "metadata": {},
   "source": [
    "Entrenamiento del RNN para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8865e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.3760 | val_loss=2.9685 | val_acc=0.1808\n",
      "Epoch 002 | train_loss=2.9717 | val_loss=2.8730 | val_acc=0.2477\n",
      "Epoch 003 | train_loss=2.8376 | val_loss=2.7529 | val_acc=0.2576\n",
      "Epoch 004 | train_loss=2.6975 | val_loss=2.6355 | val_acc=0.2956\n",
      "Epoch 005 | train_loss=2.5832 | val_loss=2.5418 | val_acc=0.3038\n",
      "Epoch 006 | train_loss=2.5095 | val_loss=2.4807 | val_acc=0.3220\n",
      "Epoch 007 | train_loss=2.4592 | val_loss=2.4383 | val_acc=0.3312\n",
      "Epoch 008 | train_loss=2.4255 | val_loss=2.4048 | val_acc=0.3367\n",
      "Epoch 009 | train_loss=2.3964 | val_loss=2.3779 | val_acc=0.3399\n",
      "Epoch 010 | train_loss=2.3723 | val_loss=2.3530 | val_acc=0.3432\n",
      "Epoch 011 | train_loss=2.3490 | val_loss=2.3308 | val_acc=0.3497\n",
      "Epoch 012 | train_loss=2.3288 | val_loss=2.3108 | val_acc=0.3561\n",
      "Epoch 013 | train_loss=2.3090 | val_loss=2.2922 | val_acc=0.3606\n",
      "Epoch 014 | train_loss=2.2934 | val_loss=2.2755 | val_acc=0.3646\n",
      "Epoch 015 | train_loss=2.2757 | val_loss=2.2587 | val_acc=0.3683\n",
      "Epoch 016 | train_loss=2.2612 | val_loss=2.2431 | val_acc=0.3700\n",
      "Epoch 017 | train_loss=2.2482 | val_loss=2.2320 | val_acc=0.3715\n",
      "Epoch 018 | train_loss=2.2349 | val_loss=2.2143 | val_acc=0.3756\n",
      "Epoch 019 | train_loss=2.2235 | val_loss=2.2025 | val_acc=0.3784\n",
      "Epoch 020 | train_loss=2.2115 | val_loss=2.1886 | val_acc=0.3822\n",
      "Epoch 021 | train_loss=2.2002 | val_loss=2.1769 | val_acc=0.3841\n",
      "Epoch 022 | train_loss=2.1906 | val_loss=2.1669 | val_acc=0.3855\n",
      "Epoch 023 | train_loss=2.1809 | val_loss=2.1569 | val_acc=0.3896\n",
      "Epoch 024 | train_loss=2.1713 | val_loss=2.1447 | val_acc=0.3939\n",
      "Epoch 025 | train_loss=2.1630 | val_loss=2.1378 | val_acc=0.3955\n",
      "Epoch 026 | train_loss=2.1553 | val_loss=2.1290 | val_acc=0.3991\n",
      "Epoch 027 | train_loss=2.1460 | val_loss=2.1203 | val_acc=0.4006\n",
      "Epoch 028 | train_loss=2.1392 | val_loss=2.1109 | val_acc=0.4044\n",
      "Epoch 029 | train_loss=2.1324 | val_loss=2.1032 | val_acc=0.4052\n",
      "Epoch 030 | train_loss=2.1262 | val_loss=2.0957 | val_acc=0.4102\n",
      "Epoch 031 | train_loss=2.1210 | val_loss=2.0891 | val_acc=0.4119\n",
      "Epoch 032 | train_loss=2.1145 | val_loss=2.0840 | val_acc=0.4121\n",
      "Epoch 033 | train_loss=2.1110 | val_loss=2.0807 | val_acc=0.4112\n",
      "Epoch 034 | train_loss=2.1049 | val_loss=2.0717 | val_acc=0.4179\n",
      "Epoch 035 | train_loss=2.0996 | val_loss=2.0676 | val_acc=0.4204\n",
      "Epoch 036 | train_loss=2.0935 | val_loss=2.0631 | val_acc=0.4179\n",
      "Epoch 037 | train_loss=2.0906 | val_loss=2.0584 | val_acc=0.4187\n",
      "Epoch 038 | train_loss=2.0855 | val_loss=2.0545 | val_acc=0.4230\n",
      "Epoch 039 | train_loss=2.0805 | val_loss=2.0509 | val_acc=0.4228\n",
      "Epoch 040 | train_loss=2.0755 | val_loss=2.0456 | val_acc=0.4232\n",
      "Epoch 041 | train_loss=2.0730 | val_loss=2.0415 | val_acc=0.4262\n",
      "Epoch 042 | train_loss=2.0685 | val_loss=2.0378 | val_acc=0.4271\n",
      "Epoch 043 | train_loss=2.0651 | val_loss=2.0341 | val_acc=0.4287\n",
      "Epoch 044 | train_loss=2.0626 | val_loss=2.0297 | val_acc=0.4289\n",
      "Epoch 045 | train_loss=2.0571 | val_loss=2.0267 | val_acc=0.4316\n",
      "Epoch 046 | train_loss=2.0539 | val_loss=2.0218 | val_acc=0.4330\n",
      "Epoch 047 | train_loss=2.0520 | val_loss=2.0216 | val_acc=0.4339\n",
      "Epoch 048 | train_loss=2.0503 | val_loss=2.0217 | val_acc=0.4346\n",
      "Epoch 049 | train_loss=2.0467 | val_loss=2.0160 | val_acc=0.4343\n",
      "Epoch 050 | train_loss=2.0440 | val_loss=2.0145 | val_acc=0.4351\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/coran/coran_rnn_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "154e9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "اقلوا يوك الشيطين\n",
      "\n",
      " Verse 2:\n",
      "والذين الله وازلهه وصالا ولا يددكابها الره من اليه ۖ ومو عليا\n",
      "\n",
      " Verse 3:\n",
      "والذي الاتتيما رجدوك مناي يرح هم من بست واال لنيعا ولو يكث والمورخن الصادهم فضي الادا كولت بعنزب ربي الاصر الها عرضون غشر الماسه ۖ والسلول\n",
      "\n",
      " Verse 4:\n",
      "وان تفتر م شرك به الله ويوش يابع ني بان لهم تقولين\n",
      "\n",
      " Verse 5:\n",
      "وا تنتم لهم لم يقول الي يوم اليه واسوات وامي الواتمن يحم وابري من ربت اخرقلله ان ارزينا لوما كانهم لناتم كعند الاكي الا تلفك فلم تكفينو ما يقول لا ان لا خشي اللم\n",
      "\n",
      " Verse 6:\n",
      "من للكم ولوا يعذل لا يبا ولا ساء الذين تمبرون\n",
      "\n",
      " Verse 7:\n",
      "والله السجات واتباه قالوا اعلم يلينا اليهم لتقوم من الذرا تعلاون باليه كنواۖ وقال كان الله لا ينصلون\n",
      "\n",
      " Verse 8:\n",
      "وان تخفر من انا غرسب الله السماوا يما بلو قدور بالزي الي في الاستهم المواتي ان معذل برض منا قول الا تنكا ۖ وانهم من الستيجب عليهم كذلم لجمثرين\n",
      "\n",
      " Verse 9:\n",
      "قنا ان المستئكم عليتا من اتمهم انصابه العلا تمعنان من دبع والله ان تنفت الصريا ۚ ومن بتله والشله ولا يطع ولكم احرنهم ۚ ونمس الله الم حصيري\n",
      "\n",
      " Verse 10:\n",
      "الذين احرل جعلنا كان علي الاتثها من اخره ۖ لن الله الحي الذين اتدي الله من محسل الميسون\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85149e9f",
   "metadata": {},
   "source": [
    "Entrenamiento RNN Corán en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.1010 | val_loss=2.8182 | val_acc=0.1842\n",
      "Epoch 002 | train_loss=2.7748 | val_loss=2.6349 | val_acc=0.2455\n",
      "Epoch 003 | train_loss=2.5712 | val_loss=2.4239 | val_acc=0.3362\n",
      "Epoch 004 | train_loss=2.3842 | val_loss=2.2675 | val_acc=0.3646\n",
      "Epoch 005 | train_loss=2.2653 | val_loss=2.1660 | val_acc=0.3861\n",
      "Epoch 006 | train_loss=2.1874 | val_loss=2.0938 | val_acc=0.3942\n",
      "Epoch 007 | train_loss=2.1297 | val_loss=2.0356 | val_acc=0.4103\n",
      "Epoch 008 | train_loss=2.0815 | val_loss=1.9840 | val_acc=0.4233\n",
      "Epoch 009 | train_loss=2.0389 | val_loss=1.9399 | val_acc=0.4314\n",
      "Epoch 010 | train_loss=2.0018 | val_loss=1.8999 | val_acc=0.4411\n",
      "Epoch 011 | train_loss=1.9663 | val_loss=1.8624 | val_acc=0.4495\n",
      "Epoch 012 | train_loss=1.9353 | val_loss=1.8300 | val_acc=0.4600\n",
      "Epoch 013 | train_loss=1.9069 | val_loss=1.8000 | val_acc=0.4724\n",
      "Epoch 014 | train_loss=1.8811 | val_loss=1.7725 | val_acc=0.4787\n",
      "Epoch 015 | train_loss=1.8579 | val_loss=1.7480 | val_acc=0.4854\n",
      "Epoch 016 | train_loss=1.8369 | val_loss=1.7256 | val_acc=0.4923\n",
      "Epoch 017 | train_loss=1.8168 | val_loss=1.7057 | val_acc=0.4986\n",
      "Epoch 018 | train_loss=1.8000 | val_loss=1.6865 | val_acc=0.5037\n",
      "Epoch 019 | train_loss=1.7837 | val_loss=1.6695 | val_acc=0.5074\n",
      "Epoch 020 | train_loss=1.7688 | val_loss=1.6535 | val_acc=0.5117\n",
      "Epoch 021 | train_loss=1.7534 | val_loss=1.6375 | val_acc=0.5177\n",
      "Epoch 022 | train_loss=1.7398 | val_loss=1.6249 | val_acc=0.5202\n",
      "Epoch 023 | train_loss=1.7277 | val_loss=1.6119 | val_acc=0.5243\n",
      "Epoch 024 | train_loss=1.7168 | val_loss=1.5997 | val_acc=0.5292\n",
      "Epoch 025 | train_loss=1.7049 | val_loss=1.5888 | val_acc=0.5313\n",
      "Epoch 026 | train_loss=1.6951 | val_loss=1.5773 | val_acc=0.5337\n",
      "Epoch 027 | train_loss=1.6861 | val_loss=1.5686 | val_acc=0.5362\n",
      "Epoch 028 | train_loss=1.6771 | val_loss=1.5584 | val_acc=0.5408\n",
      "Epoch 029 | train_loss=1.6691 | val_loss=1.5516 | val_acc=0.5407\n",
      "Epoch 030 | train_loss=1.6603 | val_loss=1.5428 | val_acc=0.5420\n",
      "Epoch 031 | train_loss=1.6530 | val_loss=1.5353 | val_acc=0.5459\n",
      "Epoch 032 | train_loss=1.6473 | val_loss=1.5286 | val_acc=0.5459\n",
      "Epoch 033 | train_loss=1.6395 | val_loss=1.5218 | val_acc=0.5500\n",
      "Epoch 034 | train_loss=1.6339 | val_loss=1.5149 | val_acc=0.5516\n",
      "Epoch 035 | train_loss=1.6262 | val_loss=1.5082 | val_acc=0.5533\n",
      "Epoch 036 | train_loss=1.6197 | val_loss=1.5031 | val_acc=0.5555\n",
      "Epoch 037 | train_loss=1.6142 | val_loss=1.4981 | val_acc=0.5570\n",
      "Epoch 038 | train_loss=1.6101 | val_loss=1.4931 | val_acc=0.5576\n",
      "Epoch 039 | train_loss=1.6052 | val_loss=1.4864 | val_acc=0.5589\n",
      "Epoch 040 | train_loss=1.5986 | val_loss=1.4821 | val_acc=0.5618\n",
      "Epoch 041 | train_loss=1.5955 | val_loss=1.4780 | val_acc=0.5629\n",
      "Epoch 042 | train_loss=1.5895 | val_loss=1.4736 | val_acc=0.5648\n",
      "Epoch 043 | train_loss=1.5862 | val_loss=1.4693 | val_acc=0.5648\n",
      "Epoch 044 | train_loss=1.5819 | val_loss=1.4646 | val_acc=0.5674\n",
      "Epoch 045 | train_loss=1.5781 | val_loss=1.4618 | val_acc=0.5677\n",
      "Epoch 046 | train_loss=1.5732 | val_loss=1.4575 | val_acc=0.5702\n",
      "Epoch 047 | train_loss=1.5705 | val_loss=1.4537 | val_acc=0.5708\n",
      "Epoch 048 | train_loss=1.5655 | val_loss=1.4494 | val_acc=0.5720\n",
      "Epoch 049 | train_loss=1.5625 | val_loss=1.4465 | val_acc=0.5728\n",
      "Epoch 050 | train_loss=1.5602 | val_loss=1.4429 | val_acc=0.5748\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/coran/coran_rnn_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1140e",
   "metadata": {},
   "source": [
    "Obtenemos los nuevos versos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90e0aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and have sereld\n",
      "\n",
      " Verse 2:\n",
      "and allah has of themper forced indeed we hever and the rexalend becient believers\n",
      "\n",
      " Verse 3:\n",
      "excett of her lerdente that os unould now wat on them will not and while that will thay allah and the lould be and they say neo them him of thoth the veristod and mending we to dissens of the sever is no avestalt of him your in whome that if they sour gay the relating will shid te the reveressseng o\n",
      "\n",
      " Verse 4:\n",
      "allah then it entiled fom and josend the earth ask and deede un aplould they will he dirngnediting us proofting to wink lord\n",
      "\n",
      " Verse 5:\n",
      "they mis of people your lord in mall and their lers at of the people if ih no day plowen on ie oa be treas is chated and dithared and the gory right as prealy sid to the signs in the angigs oo dhed ate soor abithor and heming pherlon whom you indeed o aboigs whatre slise will not thome the fiol\n",
      "\n",
      " Verse 6:\n",
      "will and war their mellast memens and you and them allah indeed you have with the foralce to the itser\n",
      "\n",
      " Verse 7:\n",
      "and they will fabees them a tears they junge the abeat allah and and if your lord shey for them ever and there anten the headses in mosid allah whun they wild be in in tho resartem the people who promef and thind them woult say allah and he will be the believers\n",
      "\n",
      " Verse 8:\n",
      "indeed he has it abate se sayar to have haty and youk of therend and a tray when a orved them and the disfile give as for the pothered be and then they him oul dote from you of that they reake them hak on the remouty and and in the will ford his noy oun ehimy and it ahan the mast goull been liment o\n",
      "\n",
      " Verse 9:\n",
      "and we conders they is be we will come with and of will we belaus the uster them so us which they bemose who south whos m your loks is uron your enaaler lord a conged and the dames and has bum inclers and do not beer iod not and hos exccust thes that you deers of allah and say conding and cevile be \n",
      "\n",
      " Verse 10:\n",
      "and a hevers of the scey they will be not tishredsed in allig and he is and we sequem enible from purises and acturnen be then have who gay anceen one who have belour\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd760ebc",
   "metadata": {},
   "source": [
    "### LSTM - Corán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee0af9",
   "metadata": {},
   "source": [
    "Modelo del LSTM para el Corán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c83baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5, pretrained_embeddings_ft=None):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        if pretrained_embeddings_ft is not None:\n",
    "            self.char_emb.weight.data.copy_(pretrained_embeddings_ft)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              \n",
    "        y_out, _ = self.lstm(x_emb)              \n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                 \n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f224",
   "metadata": {},
   "source": [
    "Entrenamiento del LSTM para el Corán árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d0784b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.0064 | val_loss=2.7982 | val_acc=0.2474\n",
      "Epoch 002 | train_loss=2.5930 | val_loss=2.4989 | val_acc=0.3116\n",
      "Epoch 003 | train_loss=2.4154 | val_loss=2.3848 | val_acc=0.3378\n",
      "Epoch 004 | train_loss=2.3163 | val_loss=2.2955 | val_acc=0.3590\n",
      "Epoch 005 | train_loss=2.2297 | val_loss=2.2205 | val_acc=0.3758\n",
      "Epoch 006 | train_loss=2.1569 | val_loss=2.1536 | val_acc=0.3923\n",
      "Epoch 007 | train_loss=2.0903 | val_loss=2.0969 | val_acc=0.4116\n",
      "Epoch 008 | train_loss=2.0346 | val_loss=2.0443 | val_acc=0.4306\n",
      "Epoch 009 | train_loss=1.9884 | val_loss=2.0048 | val_acc=0.4441\n",
      "Epoch 010 | train_loss=1.9468 | val_loss=1.9695 | val_acc=0.4507\n",
      "Epoch 011 | train_loss=1.9100 | val_loss=1.9405 | val_acc=0.4590\n",
      "Epoch 012 | train_loss=1.8760 | val_loss=1.9145 | val_acc=0.4656\n",
      "Epoch 013 | train_loss=1.8482 | val_loss=1.8903 | val_acc=0.4716\n",
      "Epoch 014 | train_loss=1.8240 | val_loss=1.8702 | val_acc=0.4778\n",
      "Epoch 015 | train_loss=1.8006 | val_loss=1.8553 | val_acc=0.4814\n",
      "Epoch 016 | train_loss=1.7772 | val_loss=1.8361 | val_acc=0.4885\n",
      "Epoch 017 | train_loss=1.7603 | val_loss=1.8259 | val_acc=0.4874\n",
      "Epoch 018 | train_loss=1.7432 | val_loss=1.8196 | val_acc=0.4898\n",
      "Epoch 019 | train_loss=1.7257 | val_loss=1.8001 | val_acc=0.4939\n",
      "Epoch 020 | train_loss=1.7088 | val_loss=1.7892 | val_acc=0.4988\n",
      "Epoch 021 | train_loss=1.6941 | val_loss=1.7857 | val_acc=0.4997\n",
      "Epoch 022 | train_loss=1.6818 | val_loss=1.7757 | val_acc=0.5029\n",
      "Epoch 023 | train_loss=1.6680 | val_loss=1.7644 | val_acc=0.5057\n",
      "Epoch 024 | train_loss=1.6546 | val_loss=1.7589 | val_acc=0.5046\n",
      "Epoch 025 | train_loss=1.6432 | val_loss=1.7548 | val_acc=0.5092\n",
      "Epoch 026 | train_loss=1.6318 | val_loss=1.7521 | val_acc=0.5076\n",
      "Epoch 027 | train_loss=1.6202 | val_loss=1.7452 | val_acc=0.5099\n",
      "Epoch 028 | train_loss=1.6097 | val_loss=1.7380 | val_acc=0.5129\n",
      "Epoch 029 | train_loss=1.6001 | val_loss=1.7345 | val_acc=0.5137\n",
      "Epoch 030 | train_loss=1.5892 | val_loss=1.7331 | val_acc=0.5132\n",
      "Epoch 031 | train_loss=1.5786 | val_loss=1.7271 | val_acc=0.5157\n",
      "Epoch 032 | train_loss=1.5693 | val_loss=1.7272 | val_acc=0.5169\n",
      "Epoch 033 | train_loss=1.5615 | val_loss=1.7251 | val_acc=0.5160\n",
      "Epoch 034 | train_loss=1.5521 | val_loss=1.7243 | val_acc=0.5167\n",
      "Epoch 035 | train_loss=1.5428 | val_loss=1.7203 | val_acc=0.5190\n",
      "Epoch 036 | train_loss=1.5352 | val_loss=1.7199 | val_acc=0.5193\n",
      "Epoch 037 | train_loss=1.5273 | val_loss=1.7159 | val_acc=0.5209\n",
      "Epoch 038 | train_loss=1.5202 | val_loss=1.7229 | val_acc=0.5208\n",
      "Epoch 039 | train_loss=1.5105 | val_loss=1.7132 | val_acc=0.5220\n",
      "Epoch 040 | train_loss=1.5039 | val_loss=1.7153 | val_acc=0.5240\n",
      "Epoch 041 | train_loss=1.4976 | val_loss=1.7144 | val_acc=0.5229\n",
      "Epoch 042 | train_loss=1.4805 | val_loss=1.7146 | val_acc=0.5220\n",
      "Epoch 043 | train_loss=1.4760 | val_loss=1.7135 | val_acc=0.5227\n",
      "Epoch 044 | train_loss=1.4656 | val_loss=1.7152 | val_acc=0.5238\n",
      "Early stopping activado.\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_arab_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/coran/coran_lstm_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02fecd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "وضلغ ايات الله وقال اني ربك ليولم فهم لا يسمعون\n",
      "\n",
      " Verse 2:\n",
      "ومن الطير من الله ان حسبا شهيدا\n",
      "\n",
      " Verse 3:\n",
      "واذ قاله قوانهم لا يؤمنون بما لهم هي يوم يقوم الحق بشرا وعليكم بكم نعم ۚ هل انسما للناس الهم اله وله ۚ والا بالغلام ۚ وكذلك تقري البين من الارض ۖ فاخذت من السماء والاخره ۗ والله اعم علي الله من المتقين\n",
      "\n",
      " Verse 4:\n",
      "الله يوجن للاحسي ۖ وما قد بشر الناس بريحا للمتقين\n",
      "\n",
      " Verse 5:\n",
      "والذين اتبعوا الي الله الا من الملائكه ۗ والذين كفروا لهم عذابا امثالهم والجنه استغعفا بينهم ۚ انه لا يريد الله الا علي الناس والاخره ۖ والله لا يريد الا ما تاكلون\n",
      "\n",
      " Verse 6:\n",
      "وما انت الا علي الكافير ما لا يغوبنا ليبغوه عن الامر في الله ويقولون او يمزل عليها وخير نصيا\n",
      "\n",
      " Verse 7:\n",
      "واذ تمدع كل رحمتم ۚ وان ترب واخذ من المنذرين\n",
      "\n",
      " Verse 8:\n",
      "قل للناس بشرك علي الغيب والارض من لو شديد القوم الخاقرين\n",
      "\n",
      " Verse 9:\n",
      "يولم في الصالحين فاذا نجظل عليكم بما قلون ان ياتوا به من قبل ۚ انه المغرب ما كانوا وهم المالمين\n",
      "\n",
      " Verse 10:\n",
      "واذ يعلم اربهم الله الذين امنوا واخذوا الفين لا يحسرون الله الا قالوا لبل تجرون من السماء والارض ۚ فان له يكون واهدي علي الذين بانته ۚ فلما علمتم من قرونا وعلي الله او كانوا يعملون\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d3128",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento inglés de LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edb9054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8129 | val_loss=2.4565 | val_acc=0.2744\n",
      "Epoch 002 | train_loss=2.2471 | val_loss=2.0381 | val_acc=0.4043\n",
      "Epoch 003 | train_loss=1.9611 | val_loss=1.8035 | val_acc=0.4709\n",
      "Epoch 004 | train_loss=1.7795 | val_loss=1.6456 | val_acc=0.5110\n",
      "Epoch 005 | train_loss=1.6488 | val_loss=1.5310 | val_acc=0.5419\n",
      "Epoch 006 | train_loss=1.5502 | val_loss=1.4528 | val_acc=0.5646\n",
      "Epoch 007 | train_loss=1.4762 | val_loss=1.3882 | val_acc=0.5806\n",
      "Epoch 008 | train_loss=1.4189 | val_loss=1.3384 | val_acc=0.5957\n",
      "Epoch 009 | train_loss=1.3710 | val_loss=1.2991 | val_acc=0.6080\n",
      "Epoch 010 | train_loss=1.3309 | val_loss=1.2682 | val_acc=0.6165\n",
      "Epoch 011 | train_loss=1.2988 | val_loss=1.2377 | val_acc=0.6234\n",
      "Epoch 012 | train_loss=1.2706 | val_loss=1.2174 | val_acc=0.6288\n",
      "Epoch 013 | train_loss=1.2457 | val_loss=1.1987 | val_acc=0.6370\n",
      "Epoch 014 | train_loss=1.2245 | val_loss=1.1845 | val_acc=0.6409\n",
      "Epoch 015 | train_loss=1.2059 | val_loss=1.1653 | val_acc=0.6465\n",
      "Epoch 016 | train_loss=1.1884 | val_loss=1.1553 | val_acc=0.6506\n",
      "Epoch 017 | train_loss=1.1729 | val_loss=1.1412 | val_acc=0.6547\n",
      "Epoch 018 | train_loss=1.1582 | val_loss=1.1327 | val_acc=0.6558\n",
      "Epoch 019 | train_loss=1.1438 | val_loss=1.1217 | val_acc=0.6616\n",
      "Epoch 020 | train_loss=1.1329 | val_loss=1.1116 | val_acc=0.6645\n",
      "Epoch 021 | train_loss=1.1208 | val_loss=1.1041 | val_acc=0.6674\n",
      "Epoch 022 | train_loss=1.1106 | val_loss=1.0981 | val_acc=0.6653\n",
      "Epoch 023 | train_loss=1.1011 | val_loss=1.0921 | val_acc=0.6680\n",
      "Epoch 024 | train_loss=1.0917 | val_loss=1.0847 | val_acc=0.6713\n",
      "Epoch 025 | train_loss=1.0820 | val_loss=1.0801 | val_acc=0.6726\n",
      "Epoch 026 | train_loss=1.0742 | val_loss=1.0737 | val_acc=0.6743\n",
      "Epoch 027 | train_loss=1.0667 | val_loss=1.0679 | val_acc=0.6780\n",
      "Epoch 028 | train_loss=1.0596 | val_loss=1.0656 | val_acc=0.6774\n",
      "Epoch 029 | train_loss=1.0526 | val_loss=1.0600 | val_acc=0.6794\n",
      "Epoch 030 | train_loss=1.0475 | val_loss=1.0592 | val_acc=0.6794\n",
      "Epoch 031 | train_loss=1.0389 | val_loss=1.0553 | val_acc=0.6808\n",
      "Epoch 032 | train_loss=1.0313 | val_loss=1.0513 | val_acc=0.6821\n",
      "Epoch 033 | train_loss=1.0268 | val_loss=1.0478 | val_acc=0.6842\n",
      "Epoch 034 | train_loss=1.0222 | val_loss=1.0459 | val_acc=0.6837\n",
      "Epoch 035 | train_loss=1.0173 | val_loss=1.0438 | val_acc=0.6848\n",
      "Epoch 036 | train_loss=1.0117 | val_loss=1.0429 | val_acc=0.6847\n",
      "Epoch 037 | train_loss=1.0043 | val_loss=1.0399 | val_acc=0.6869\n",
      "Epoch 038 | train_loss=1.0004 | val_loss=1.0391 | val_acc=0.6866\n",
      "Epoch 039 | train_loss=0.9956 | val_loss=1.0371 | val_acc=0.6865\n",
      "Epoch 040 | train_loss=0.9905 | val_loss=1.0350 | val_acc=0.6871\n",
      "Epoch 041 | train_loss=0.9864 | val_loss=1.0339 | val_acc=0.6881\n",
      "Epoch 042 | train_loss=0.9830 | val_loss=1.0336 | val_acc=0.6895\n",
      "Epoch 043 | train_loss=0.9785 | val_loss=1.0323 | val_acc=0.6892\n",
      "Epoch 044 | train_loss=0.9745 | val_loss=1.0332 | val_acc=0.6895\n",
      "Epoch 045 | train_loss=0.9706 | val_loss=1.0293 | val_acc=0.6913\n",
      "Epoch 046 | train_loss=0.9644 | val_loss=1.0313 | val_acc=0.6887\n",
      "Epoch 047 | train_loss=0.9631 | val_loss=1.0272 | val_acc=0.6921\n",
      "Epoch 048 | train_loss=0.9603 | val_loss=1.0303 | val_acc=0.6882\n",
      "Epoch 049 | train_loss=0.9570 | val_loss=1.0307 | val_acc=0.6908\n",
      "Epoch 050 | train_loss=0.9422 | val_loss=1.0274 | val_acc=0.6921\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(coran_path=\"../data/cleaned_data/cleaned_english_quran.txt\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/coran/coran_lstm_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0afe795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "and indeed we to allah he will surely be overtioned to the earth and harms allah with madires after the wrongdoing people\n",
      "\n",
      " Verse 2:\n",
      "the word of allah bring before you of our promise and masked him acceptable that i said o mankind each of them will intend to break they will have eat and you will find a party of being a helper\n",
      "\n",
      " Verse 3:\n",
      "and allah called and to allah inspert is not because they will be returned word how not to in hell that they will be returned\n",
      "\n",
      " Verse 4:\n",
      "and if they said they before them there is the his with the earth and those who bespor they will be grateful\n",
      "\n",
      " Verse 5:\n",
      "and when he talken is allah who in the way of allah he has revealed to them about them indeed we will respond to them indeed allah is forgiving and mercifus and should and this is dust before them indeed allah is hearing and sent down and you are the scripture and his wife will deed and he do indeed\n",
      "\n",
      " Verse 6:\n",
      "the people indeed allah is not the truth and we will surely be a clear forgiveness and done interess came to them and they will not be trully there is no deity and those who say my collads and whoever forgives whom he wills so we come to us with miselly\n",
      "\n",
      " Verse 7:\n",
      "allah has been tormedting with it he would not be assecit the righteous\n",
      "\n",
      " Verse 8:\n",
      "and they have taken you grain provision and no lain children and when they disbelieved in the corrupters\n",
      "\n",
      " Verse 9:\n",
      "and know him in the sight of allah as for deceities then for them there is no deity of delivess and we have leare\n",
      "\n",
      " Verse 10:\n",
      "and it is he who says our lord has not sent for them that they is what they said i will surely have been to duith among the most one another and we will be of strikes in a light those who have wronged them then for it is your lord and allah has enjoy the scripture for sosely of their transgression a\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c641da9",
   "metadata": {},
   "source": [
    "## Dataset Hadith-s (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fd42b",
   "metadata": {},
   "source": [
    "Visualizamos el la estructura del df, cogeremos las columnas (hadith-s) que nos interesan: `text_ar` y `text_en`. Como el archivo viene estructurado de una manera poco usual, realizaremos una limpieza exhaustiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3451d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hadith_id</th>\n",
       "      <th>source</th>\n",
       "      <th>chapter_no</th>\n",
       "      <th>hadith_no</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chain_indx</th>\n",
       "      <th>text_ar</th>\n",
       "      <th>text_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sahih Bukhari</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Revelation - كتاب بدء الوحى</td>\n",
       "      <td>30418, 20005, 11062, 11213, 11042, 3</td>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "      <td>Narrated 'Umar bin Al-Khattab:                          I heard Allah's Apostle saying, \"The reward of deeds depends upon the      intentions and every person will get the reward according to what he      has intended. So whoever emigrated for worldly benefits or for a woman     to marry, his emigration was for what he emigrated for.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hadith_id           source  chapter_no hadith_no  \\\n",
       "0   0          1   Sahih Bukhari            1        1    \n",
       "\n",
       "                       chapter                            chain_indx  \\\n",
       "0  Revelation - كتاب بدء الوحى  30418, 20005, 11062, 11213, 11042, 3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar  \\\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                  text_en  \n",
       "0        Narrated 'Umar bin Al-Khattab:                          I heard Allah's Apostle saying, \"The reward of deeds depends upon the      intentions and every person will get the reward according to what he      has intended. So whoever emigrated for worldly benefits or for a woman     to marry, his emigration was for what he emigrated for.\"  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_df = pd.read_csv(\"../data/hadith_dataset/all_hadiths_clean.csv\")\n",
    "hadith_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b0fc5",
   "metadata": {},
   "source": [
    "Árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01c5b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_ar    34433\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                text_ar\n",
       "0  حدثنا الحميدي عبد الله بن الزبير، قال حدثنا سفيان، قال حدثنا يحيى بن سعيد الأنصاري، قال أخبرني محمد بن إبراهيم التيمي، أنه سمع علقمة بن وقاص الليثي، يقول سمعت عمر بن الخطاب  رضى الله عنه  على المنبر قال سمعت رسول الله صلى الله عليه وسلم يقول ‏\"‏ إنما الأعمال بالنيات، وإنما لكل امرئ ما نوى، فمن كانت هجرته إلى دنيا يصيبها أو إلى امرأة ينكحها فهجرته إلى ما هاجر إليه ‏\"‏‏.‏"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadith_ar = hadith_df[\"text_ar\"]\n",
    "hadith_ar = pd.DataFrame(hadith_ar).dropna()\n",
    "hadith_ar.to_csv(\"../data/hadith_dataset/hadith_ar/hadith_ar.csv\", index=False, encoding=\"utf-8\")\n",
    "print(hadith_ar.count())\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "hadith_ar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b0149",
   "metadata": {},
   "source": [
    "Limpieza árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3b08968",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "# Diacríticos árabes (harakat) + marcas coránicas comunes\n",
    "ARABIC_DIACRITICS = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "# Rangos Unicode típicos para árabe (básico + extendidos)\n",
    "ARABIC_RANGES = r\"\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def normalize_arabic(text: str, remove_diacritics: bool = True) -> str:\n",
    "    # Normalización Unicode (unifica formas)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Quitar tatweel (kashida)\n",
    "    text = text.replace(\"\\u0640\", \"\")\n",
    "\n",
    "    # Unificar algunas variantes comunes (opcional, útil en muchos corpus)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ى\", \"ي\")\n",
    "    text = text.replace(\"ة\", \"ه\")  # si prefieres mantenerla, comenta esta línea\n",
    "\n",
    "    if remove_diacritics:\n",
    "        text = re.sub(ARABIC_DIACRITICS, \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_hadith_text_ar(text, remove_diacritics: bool = True):\n",
    "    \"\"\"\n",
    "    Limpieza para árabe manteniendo el mismo formato que el inglés:\n",
    "    - Quita comillas envolventes\n",
    "    - Elimina prefijo 'narrated ...' si existe (en inglés)\n",
    "    - Normaliza árabe (opcional quitar harakat)\n",
    "    - Mantiene letras árabes + números + puntuación básica árabe/latina\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Espacios/saltos de línea\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "    # 2) Quitar comillas envolventes\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    # 3) Normalización árabe (sin lower)\n",
    "    text = normalize_arabic(text, remove_diacritics=remove_diacritics)\n",
    "\n",
    "    # 4) Eliminar narrador (si el encabezado está en inglés, como en tu caso)\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, \"\", text).strip()\n",
    "\n",
    "    # 5) Quitar comillas residuales\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', \"\", text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', \"\", text)\n",
    "\n",
    "    # 6) Mantener: letras árabes, números, espacios y puntuación básica.\n",
    "    # Incluye puntuación árabe: ، ؛ ؟  (comma/semicolon/question mark)\n",
    "    allowed = rf\"[^0-9\\s{ARABIC_RANGES}\\.,!?'\\-\\(\\)«»\\\"،؛؟]\"\n",
    "    text = re.sub(allowed, \" \", text)\n",
    "\n",
    "    # 7) Colapsar espacios\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68252da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_ar = hadith_df[[\"text_ar\"]].copy()\n",
    "\n",
    "hadith_ar = hadith_ar.dropna(subset=[\"text_ar\"])\n",
    "\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar[\"text_ar\"] = hadith_ar[\"text_ar\"].apply(clean_hadith_text_ar)\n",
    "hadith_ar = hadith_ar.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_ar = hadith_ar[hadith_ar[\"text_ar\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\"\n",
    "\n",
    "hadith_ar.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575a5f5",
   "metadata": {},
   "source": [
    "Inglés + función de limpieza inglesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "089ced52",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_CHARS = r\"\\\"'“”„«»‹›`´\"\n",
    "\n",
    "def _strip_wrapping_quotes(text: str, max_loops: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Elimina comillas envolventes repetidas (incluyendo tipográficas),\n",
    "    tolerando espacios alrededor.\n",
    "    Ej:\n",
    "      '\"hola\"' -> hola\n",
    "      '  “hola”  ' -> hola\n",
    "      '\"\"hola\"\"' -> hola\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    t = text.strip()\n",
    "    for _ in range(max_loops):\n",
    "        # ^\\s*[\"'“”...]+ (captura comillas al inicio) y [\"'“”...]+\\s*$ (al final)\n",
    "        new_t = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', t)\n",
    "        new_t = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', new_t)\n",
    "        new_t = new_t.strip()\n",
    "        if new_t == t:\n",
    "            break\n",
    "        t = new_t\n",
    "    return t\n",
    "\n",
    "def clean_hadith_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "    text = _strip_wrapping_quotes(text)\n",
    "\n",
    "    text = text.replace('\"\"', '\"').lower()\n",
    "\n",
    "    # Limpieza del formato original del .csv: narrated by (nommbre del narrador) + texto que queremos\n",
    "    palabras_clave = (\n",
    "        r\"(said|asked|the|i\\s+heard|i\\s+was\\s+told|i\\s+informed|while|informed|abu|allah|\"\n",
    "        r\"if|when|once|some|whenever|it|sometimes|thereupon|then|and|but)\"\n",
    "    )\n",
    "    patron_narrador = r'^\\s*narrated\\s+.*?[:\\-]?\\s*(?=\\b' + palabras_clave + r'\\b)'\n",
    "    text = re.sub(patron_narrador, '', text).strip()\n",
    "\n",
    "    text = re.sub(rf'^\\s*[{QUOTE_CHARS}]+\\s*', '', text)\n",
    "    text = re.sub(rf'\\s*[{QUOTE_CHARS}]+\\s*$', '', text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\-\\(\\)]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f8ffd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadith_en = hadith_df[[\"text_en\"]].copy()\n",
    "\n",
    "hadith_en = hadith_en.dropna(subset=[\"text_en\"])\n",
    "\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en[\"text_en\"] = hadith_en[\"text_en\"].apply(clean_hadith_text)\n",
    "hadith_en = hadith_en.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "hadith_en = hadith_en[hadith_en[\"text_en\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "output_path = \"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\"\n",
    "\n",
    "hadith_en.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa20bd5",
   "metadata": {},
   "source": [
    "Clase Dataset del Hadith dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc92ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col):\n",
    "        # text_col: text_en (hadith_en) y text_ar (hadith_ar)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "        self._max_seq_length = min(int(self.df[text_col].astype(str).map(len).max()) + 2, 500)        \n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70) # 70% de las instancias al train set\n",
    "        val_end = int(n * .85) # 15 para el validation set, y el otro 15 para el test\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, hadith_csv, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, hadith_csv, vectorizer_filepath, text_col):\n",
    "        df = pd.read_csv(hadith_csv)\n",
    "        # FIX: Use text_col instead of \"text\"\n",
    "        df[text_col] = df[text_col].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long), \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bba93",
   "metadata": {},
   "source": [
    "### RNN - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a413054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(hadith_path, output_path, text_col):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # 300 porque los embeddings del ft son de 300, tienen que coincidir\n",
    "        rnn_hidden_size=128, # 256-ekin peatau itenzatek\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    print(args.batch_size)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b4da3",
   "metadata": {},
   "source": [
    "Entrenamiento RNN con dataset Hadith en árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e07996b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8292 | val_loss=2.2735 | val_acc=0.3922\n",
      "Epoch 002 | train_loss=2.1674 | val_loss=1.9568 | val_acc=0.4754\n",
      "Epoch 003 | train_loss=1.9599 | val_loss=1.7686 | val_acc=0.5213\n",
      "Epoch 004 | train_loss=1.8397 | val_loss=1.6645 | val_acc=0.5467\n",
      "Epoch 005 | train_loss=1.7722 | val_loss=1.6053 | val_acc=0.5620\n",
      "Epoch 006 | train_loss=1.7289 | val_loss=1.5654 | val_acc=0.5700\n",
      "Epoch 007 | train_loss=1.6981 | val_loss=1.5379 | val_acc=0.5776\n",
      "Epoch 008 | train_loss=1.6748 | val_loss=1.5169 | val_acc=0.5788\n",
      "Epoch 009 | train_loss=1.6563 | val_loss=1.4993 | val_acc=0.5833\n",
      "Epoch 010 | train_loss=1.6424 | val_loss=1.4898 | val_acc=0.5850\n",
      "Epoch 011 | train_loss=1.6301 | val_loss=1.4769 | val_acc=0.5914\n",
      "Epoch 012 | train_loss=1.6198 | val_loss=1.4664 | val_acc=0.5935\n",
      "Epoch 013 | train_loss=1.6099 | val_loss=1.4574 | val_acc=0.5956\n",
      "Epoch 014 | train_loss=1.6030 | val_loss=1.4509 | val_acc=0.5990\n",
      "Epoch 015 | train_loss=1.5953 | val_loss=1.4451 | val_acc=0.5988\n",
      "Epoch 016 | train_loss=1.5886 | val_loss=1.4428 | val_acc=0.6000\n",
      "Epoch 017 | train_loss=1.5834 | val_loss=1.4359 | val_acc=0.6022\n",
      "Epoch 018 | train_loss=1.5775 | val_loss=1.4319 | val_acc=0.6027\n",
      "Epoch 019 | train_loss=1.5720 | val_loss=1.4262 | val_acc=0.6047\n",
      "Epoch 020 | train_loss=1.5675 | val_loss=1.4233 | val_acc=0.6051\n",
      "Epoch 021 | train_loss=1.5632 | val_loss=1.4213 | val_acc=0.6063\n",
      "Epoch 022 | train_loss=1.5588 | val_loss=1.4154 | val_acc=0.6079\n",
      "Epoch 023 | train_loss=1.5558 | val_loss=1.4140 | val_acc=0.6078\n",
      "Epoch 024 | train_loss=1.5519 | val_loss=1.4113 | val_acc=0.6085\n",
      "Epoch 025 | train_loss=1.5496 | val_loss=1.4080 | val_acc=0.6076\n",
      "Epoch 026 | train_loss=1.5454 | val_loss=1.4042 | val_acc=0.6091\n",
      "Epoch 027 | train_loss=1.5433 | val_loss=1.4040 | val_acc=0.6080\n",
      "Epoch 028 | train_loss=1.5398 | val_loss=1.4030 | val_acc=0.6080\n",
      "Epoch 029 | train_loss=1.5376 | val_loss=1.4006 | val_acc=0.6088\n",
      "Epoch 030 | train_loss=1.5356 | val_loss=1.3985 | val_acc=0.6097\n",
      "Epoch 031 | train_loss=1.5336 | val_loss=1.3996 | val_acc=0.6062\n",
      "Epoch 032 | train_loss=1.5312 | val_loss=1.3930 | val_acc=0.6108\n",
      "Epoch 033 | train_loss=1.5289 | val_loss=1.3948 | val_acc=0.6095\n",
      "Epoch 034 | train_loss=1.5279 | val_loss=1.3904 | val_acc=0.6076\n",
      "Epoch 035 | train_loss=1.5250 | val_loss=1.3879 | val_acc=0.6124\n",
      "Epoch 036 | train_loss=1.5242 | val_loss=1.3884 | val_acc=0.6077\n",
      "Epoch 037 | train_loss=1.5221 | val_loss=1.3868 | val_acc=0.6089\n",
      "Epoch 038 | train_loss=1.5197 | val_loss=1.3872 | val_acc=0.6068\n",
      "Epoch 039 | train_loss=1.5186 | val_loss=1.3843 | val_acc=0.6097\n",
      "Epoch 040 | train_loss=1.5168 | val_loss=1.3852 | val_acc=0.6091\n",
      "Epoch 041 | train_loss=1.5168 | val_loss=1.3815 | val_acc=0.6084\n",
      "Epoch 042 | train_loss=1.5142 | val_loss=1.3803 | val_acc=0.6095\n",
      "Epoch 043 | train_loss=1.5132 | val_loss=1.3765 | val_acc=0.6113\n",
      "Epoch 044 | train_loss=1.5116 | val_loss=1.3829 | val_acc=0.6103\n",
      "Epoch 045 | train_loss=1.5107 | val_loss=1.3762 | val_acc=0.6131\n",
      "Epoch 046 | train_loss=1.5093 | val_loss=1.3748 | val_acc=0.6118\n",
      "Epoch 047 | train_loss=1.5080 | val_loss=1.3738 | val_acc=0.6147\n",
      "Epoch 048 | train_loss=1.5066 | val_loss=1.3739 | val_acc=0.6149\n",
      "Epoch 049 | train_loss=1.5055 | val_loss=1.3730 | val_acc=0.6148\n",
      "Epoch 050 | train_loss=1.5045 | val_loss=1.3718 | val_acc=0.6150\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/arab/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f40cda4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثناه واعر، عن حبيد بن اليسير، عن ابي زحيبه بن عائشه - قال قال رسول الله صلي الله عليه وسلم فقال له عن يليقه النهم الله يعمر رالت انه لكوني وللا شعر حتي اوضي ورؤي التبال قال فقال اما قال \" ما الي المشاوي يوقوا الا ان ربرارا . قال ابن عبي من صلي الذا بعدك وال بيده وجله فلاسم عل هذا حديث غضي الولسا\n",
      "\n",
      " Verse 2:\n",
      "حدثنا عبد الله بن ابي اليسمت، قال احسم النحلم بن ابي عبيد الله، عن ابن ععم، ان النبي صلي الله عليه وسلم رقرك ولا يصلوضه تعوا الله الا علي العبد في ما انبشه \" . قال ابو عيسو حتي اسلم تمول الولا ولقولي مه ابي جرير ان الناس الي الصواه من اللفف امذي ان فيها \" .\n",
      "\n",
      " Verse 3:\n",
      "حدثنا ابو بهركي، قري عن رزيد وفا حديث وحيد بن ابي برسوا ابي بعر - عن اسحعم بن ملين، عن عبيد الله بن عبر بن زهر بن الاحي، حدثنا مكري، عن ابي الابي، قال كن فرج الالون الباك \" .\n",
      "\n",
      " Verse 4:\n",
      "حدثنا يحيي بن قال البنبا بن حديث بن عبد الله بن ابي رسوي بن سعيد، عن ابي هريره، ين يصلا قال رسول الله صلي الله عليه وسلم فاليه عن النهام عن ابن عبد الله بن محمر بن سفمان، عن عبد الرحرد، ماو يحدين فا جاء فيا يمر انا اعنا لا يحد كالت ذلك الحالي من المظروا وقال \" الامم الخمر الحديث فاهذي عائشه . قال وا\n",
      "\n",
      " Verse 5:\n",
      "حدثنا محمد بن اقركاه، عن ناوق، والحبيده، عن ابي الاسنفي، عن عبد الرحمن، بالجان به الاعمي، ان المباره - قالت،، كرا في العال اللمعم ان المني بها . وبابت عن عبد الله بن عمر بن عبد الله، عن ابي بالرهاع، عن علي بن عيره بن ابي صراي بن حفي بن عبد الله بن ابي مالك، عن المرير، حدثنا الزمرو بن المفيان، قال ال\n",
      "\n",
      " Verse 6:\n",
      "حدثنا حييث ان يحيي بن قوا حهد يد الحزبه، عن انس بن ابي جهير بن ابي \" يابي الانازاء، عن ابن شهاب، يانه ان لا يعله في الصطانكا ثخب .\n",
      "\n",
      " Verse 7:\n",
      "حدثنا ابو بيرا احدك - اخبرني ابن ا قال الحريب بن عبد الوه، حدثنا المبير، عن ابي سليه بن عيدي، حدثنا ابن العامب، عن ابتي حجلا بن عبد الرحمن، - وسول الناس فتانييل وكبن بغض وها من سحمن في الرايد الوس فستلر فان - وبول عبد الله بن مهمر بن الجليد، انب ال تليل النبي صلي الله عليه وسلم فسلم بناا النبي صلي ا\n",
      "\n",
      " Verse 8:\n",
      "حدثنا عبد الله بن ابي شيبه، بن ابي عرحيه، بن عمي علي الشي قلل بقال النبي صلي الله عليه وسلم قاذ الاهلم يندي الالم ان يسيلن بن عبده النبي صلي الله عليه وسلم قال \" انه لا بكا \" . قتلي ان الن فاخرانا وعن عمره ان مقرج الانوت وبينه فان سلم \" . فقال وفيتا . قال وانه انبه \" .\n",
      "\n",
      " Verse 9:\n",
      "حدثنا ابو بالس، عن ابن عباس، ان بسلم . فذال النال حتي لا يفصل النبي صلي الله عليه وسلم \" اين يهي علي النبي صلي الله عليه وسلم في بحده الاهي لعالي فقال \" لا تجوله فجمن في عمر وقول النبي صلي الله عليه وسلم قال \" لم يده بهذي اوع العهذس المري من الحر جار السممه حم من به السه الابن فله الا شع بن السفي من\n",
      "\n",
      " Verse 10:\n",
      "حدثنا حدعثل، حدثنا عبد الزهدي، حدثنا الحراا شعبد بن موسي، عن عبير، يحدث عن ابي شنبي سني ابن عبر وناك حديث ابن جالا ان ابر همنها وكان علي الله محمن فقال ابن حتي فاخاره الا رسول الله صلي الله عليه وسلم بشيل علي ولا ي- عن النبي صلي الله عليه وسلم \" يا اسهما . قال ابو عيسي هرابه اسمعا النبي صلي الله علي\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e013a8",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith RNN inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4bcf8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.8150 | val_loss=2.2923 | val_acc=0.3488\n",
      "Epoch 002 | train_loss=2.2527 | val_loss=2.0397 | val_acc=0.4163\n",
      "Epoch 003 | train_loss=2.0801 | val_loss=1.8875 | val_acc=0.4721\n",
      "Epoch 004 | train_loss=1.9640 | val_loss=1.7762 | val_acc=0.4978\n",
      "Epoch 005 | train_loss=1.8838 | val_loss=1.7031 | val_acc=0.5181\n",
      "Epoch 006 | train_loss=1.8304 | val_loss=1.6532 | val_acc=0.5309\n",
      "Epoch 007 | train_loss=1.7912 | val_loss=1.6177 | val_acc=0.5379\n",
      "Epoch 008 | train_loss=1.7626 | val_loss=1.5924 | val_acc=0.5445\n",
      "Epoch 009 | train_loss=1.7395 | val_loss=1.5729 | val_acc=0.5476\n",
      "Epoch 010 | train_loss=1.7211 | val_loss=1.5560 | val_acc=0.5532\n",
      "Epoch 011 | train_loss=1.7057 | val_loss=1.5416 | val_acc=0.5533\n",
      "Epoch 012 | train_loss=1.6934 | val_loss=1.5302 | val_acc=0.5568\n",
      "Epoch 013 | train_loss=1.6829 | val_loss=1.5210 | val_acc=0.5588\n",
      "Epoch 014 | train_loss=1.6741 | val_loss=1.5118 | val_acc=0.5607\n",
      "Epoch 015 | train_loss=1.6658 | val_loss=1.5059 | val_acc=0.5630\n",
      "Epoch 016 | train_loss=1.6591 | val_loss=1.5001 | val_acc=0.5648\n",
      "Epoch 017 | train_loss=1.6528 | val_loss=1.4946 | val_acc=0.5656\n",
      "Epoch 018 | train_loss=1.6468 | val_loss=1.4894 | val_acc=0.5669\n",
      "Epoch 019 | train_loss=1.6413 | val_loss=1.4867 | val_acc=0.5686\n",
      "Epoch 020 | train_loss=1.6375 | val_loss=1.4803 | val_acc=0.5693\n",
      "Epoch 021 | train_loss=1.6335 | val_loss=1.4782 | val_acc=0.5700\n",
      "Epoch 022 | train_loss=1.6292 | val_loss=1.4758 | val_acc=0.5710\n",
      "Epoch 023 | train_loss=1.6257 | val_loss=1.4714 | val_acc=0.5717\n",
      "Epoch 024 | train_loss=1.6226 | val_loss=1.4682 | val_acc=0.5734\n",
      "Epoch 025 | train_loss=1.6193 | val_loss=1.4666 | val_acc=0.5733\n",
      "Epoch 026 | train_loss=1.6170 | val_loss=1.4641 | val_acc=0.5743\n",
      "Epoch 027 | train_loss=1.6143 | val_loss=1.4615 | val_acc=0.5750\n",
      "Epoch 028 | train_loss=1.6110 | val_loss=1.4600 | val_acc=0.5751\n",
      "Epoch 029 | train_loss=1.6089 | val_loss=1.4573 | val_acc=0.5768\n",
      "Epoch 030 | train_loss=1.6069 | val_loss=1.4544 | val_acc=0.5770\n",
      "Epoch 031 | train_loss=1.6046 | val_loss=1.4537 | val_acc=0.5806\n",
      "Epoch 032 | train_loss=1.6028 | val_loss=1.4524 | val_acc=0.5788\n",
      "Epoch 033 | train_loss=1.6017 | val_loss=1.4523 | val_acc=0.5781\n",
      "Epoch 034 | train_loss=1.5999 | val_loss=1.4518 | val_acc=0.5786\n",
      "Epoch 035 | train_loss=1.5982 | val_loss=1.4518 | val_acc=0.5788\n",
      "Epoch 036 | train_loss=1.5964 | val_loss=1.4492 | val_acc=0.5794\n",
      "Epoch 037 | train_loss=1.5945 | val_loss=1.4508 | val_acc=0.5794\n",
      "Epoch 038 | train_loss=1.5932 | val_loss=1.4484 | val_acc=0.5796\n",
      "Epoch 039 | train_loss=1.5921 | val_loss=1.4452 | val_acc=0.5802\n",
      "Epoch 040 | train_loss=1.5907 | val_loss=1.4440 | val_acc=0.5804\n",
      "Epoch 041 | train_loss=1.5895 | val_loss=1.4449 | val_acc=0.5804\n",
      "Epoch 042 | train_loss=1.5884 | val_loss=1.4441 | val_acc=0.5811\n",
      "Epoch 043 | train_loss=1.5860 | val_loss=1.4413 | val_acc=0.5819\n",
      "Epoch 044 | train_loss=1.5852 | val_loss=1.4423 | val_acc=0.5817\n",
      "Epoch 045 | train_loss=1.5849 | val_loss=1.4422 | val_acc=0.5815\n",
      "Epoch 046 | train_loss=1.5833 | val_loss=1.4409 | val_acc=0.5816\n",
      "Epoch 047 | train_loss=1.5831 | val_loss=1.4403 | val_acc=0.5816\n",
      "Epoch 048 | train_loss=1.5831 | val_loss=1.4394 | val_acc=0.5819\n",
      "Epoch 049 | train_loss=1.5829 | val_loss=1.4407 | val_acc=0.5818\n",
      "Epoch 050 | train_loss=1.5820 | val_loss=1.4399 | val_acc=0.5819\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/RNN/english/hadith/coran_rnn_v1\",\n",
    "                                                 text_col=\"text_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "747f38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "ighilger to not with one of obre'san me who trounge aforher to his norn mad to remall of face rish whine and of intony warces. she said asked the prophet said while il the lakin. on the son over allah's aponthing white one is the prophet what the messenger of allah ( ) was would nater of manting and\n",
      "\n",
      " Verse 2:\n",
      "in the said and caner it in the promertidian angingas if the people of was a bont of the hant (it who it.\n",
      "\n",
      " Verse 3:\n",
      "abu harie mircher and he go with entrong undore of allah's messenger (mal would, who were bouthees (the hir! the messenger of allah ( ) said to hand to tamal and and a toms and whos of the nafrat and it is thit i. i said the my somment soment on the inaning him to day is the bagt ase whee toan and c\n",
      "\n",
      " Verse 4:\n",
      "the prophet said you the messenger of allah (may peace be upon him) tarat the mastted a fromt ho for his kand and come to the from the ant of bur asked by allah.\n",
      "\n",
      " Verse 5:\n",
      "abu hurairah he fill in the vernaty of the frast of and the pul in the messenger of allah ( ) said to you whet to be beent dir wyon the masty and he hrased (i. if at that his mester it not of the prophet said in the becority with our do nin the gerted one of it and said anl had that of allah (may pe\n",
      "\n",
      " Verse 6:\n",
      "abu hurairat bacu us. al-ab. 'abduse we gordend the to's gien the messenger of allah ( ) beens is leape the quralaha malas samanising, and i while treace) of clased south arlah, have were and beanure or it and allah. betrean! sissens of mancunde cheat about in the halin sofen him i sed toldard. him \n",
      "\n",
      " Verse 7:\n",
      "ih has bang to allah be neer the ele of the reveratilg the diy it. the propled him you to the prophet ( ). i than the prophet said tradited this had trassed then who who suca ti hes apostle that the (allah him and said and said, he said the prophet said the fureal) it must ous ont of allah's messeng\n",
      "\n",
      " Verse 8:\n",
      "and allah's sase said me said 'ardin s madice and he has onter the indented to of like a man tlanamed for whith and the tall of them and from mes of biss with hamselgrerdent from allah wheat the ward trance do has sime that has bethat (of excephers is some allahal inse with him and lathahg to sals o\n",
      "\n",
      " Verse 9:\n",
      "abu harity him for of nof innts of ithla! wore wade on the was the that in the beeich arppenice of allah, o albai, the propteching a ont of ibouss and he wso was not him in the rant, mishima and ha brian. (motike it in asd, the wat purout of 'abu huraira allah's aplatess is for riblim fas in my comp\n",
      "\n",
      " Verse 10:\n",
      "the prophet any onct and some of the hamed and prothers and there with affore, in the messenger of allah (may peater of intredted sa'ed with the prophet said what said whing for i have at sile who is a mentee of man'is over her in the messenger of allah ( ) seicr and he it cante him a ta binthed the\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5818ab3",
   "metadata": {},
   "source": [
    "### LSTM - Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(hadith_path, output_path, text_col):\n",
    "    args = Namespace(\n",
    "        hadith_csv=hadith_path,\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=output_path,\n",
    "\n",
    "        char_embedding_size=300, # lo mismo que ft \n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = HadithDataset.load_dataset_and_load_vectorizer(args.hadith_csv, args.vectorizer_file, text_col)\n",
    "    else:\n",
    "        dataset = HadithDataset.load_dataset_and_make_vectorizer(args.hadith_csv, text_col)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    def obtener_pesos(vectorizer, modelo_ft):\n",
    "        vocab = vectorizer.char_vocab\n",
    "        token_to_idx = vocab._token_to_idx\n",
    "        tamaño_vocab = len(token_to_idx)\n",
    "        embedding_dim = modelo_ft.get_dimension()\n",
    "        pesos = np.zeros((tamaño_vocab, embedding_dim))\n",
    "\n",
    "        for token, idx in token_to_idx.items():\n",
    "            pesos[idx] = modelo_ft.get_word_vector(token)\n",
    "    \n",
    "        return torch.FloatTensor(pesos)\n",
    "\n",
    "    ft_ingles = fasttext.load_model(\"../src/modelos/fasttext_english_busqueda_seamantica.bin\")\n",
    "    pretrained_ft_pesos = obtener_pesos(vectorizer, ft_ingles)\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index,\n",
    "        pretrained_embeddings_ft=pretrained_ft_pesos\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21033cc",
   "metadata": {},
   "source": [
    "Entrenamiento LSTM Hadith árabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbe6fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.2244 | val_loss=1.6010 | val_acc=0.5521\n",
      "Epoch 002 | train_loss=1.5442 | val_loss=1.3511 | val_acc=0.6187\n",
      "Epoch 003 | train_loss=1.3814 | val_loss=1.2559 | val_acc=0.6392\n",
      "Epoch 004 | train_loss=1.3001 | val_loss=1.2028 | val_acc=0.6529\n",
      "Epoch 005 | train_loss=1.2489 | val_loss=1.1655 | val_acc=0.6629\n",
      "Epoch 006 | train_loss=1.2133 | val_loss=1.1475 | val_acc=0.6667\n",
      "Epoch 007 | train_loss=1.1867 | val_loss=1.1165 | val_acc=0.6774\n",
      "Epoch 008 | train_loss=1.1655 | val_loss=1.0994 | val_acc=0.6821\n",
      "Epoch 009 | train_loss=1.1486 | val_loss=1.0839 | val_acc=0.6860\n",
      "Epoch 010 | train_loss=1.1338 | val_loss=1.0716 | val_acc=0.6887\n",
      "Epoch 011 | train_loss=1.1217 | val_loss=1.0630 | val_acc=0.6925\n",
      "Epoch 012 | train_loss=1.1106 | val_loss=1.0542 | val_acc=0.6942\n",
      "Epoch 013 | train_loss=1.1013 | val_loss=1.0469 | val_acc=0.6959\n",
      "Epoch 014 | train_loss=1.0932 | val_loss=1.0433 | val_acc=0.6960\n",
      "Epoch 015 | train_loss=1.0864 | val_loss=1.0361 | val_acc=0.7006\n",
      "Epoch 016 | train_loss=1.0795 | val_loss=1.0291 | val_acc=0.7038\n",
      "Epoch 017 | train_loss=1.0739 | val_loss=1.0224 | val_acc=0.7049\n",
      "Epoch 018 | train_loss=1.0690 | val_loss=1.0200 | val_acc=0.7051\n",
      "Epoch 019 | train_loss=1.0641 | val_loss=1.0151 | val_acc=0.7070\n",
      "Epoch 020 | train_loss=1.0599 | val_loss=1.0102 | val_acc=0.7084\n",
      "Epoch 021 | train_loss=1.0557 | val_loss=1.0115 | val_acc=0.7075\n",
      "Epoch 022 | train_loss=1.0520 | val_loss=1.0091 | val_acc=0.7080\n",
      "Epoch 023 | train_loss=1.0486 | val_loss=1.0080 | val_acc=0.7077\n",
      "Epoch 024 | train_loss=1.0454 | val_loss=1.0026 | val_acc=0.7099\n",
      "Epoch 025 | train_loss=1.0429 | val_loss=0.9968 | val_acc=0.7117\n",
      "Epoch 026 | train_loss=1.0401 | val_loss=0.9972 | val_acc=0.7111\n",
      "Epoch 027 | train_loss=1.0372 | val_loss=0.9938 | val_acc=0.7124\n",
      "Epoch 028 | train_loss=1.0348 | val_loss=0.9947 | val_acc=0.7124\n",
      "Epoch 029 | train_loss=1.0324 | val_loss=0.9900 | val_acc=0.7138\n",
      "Epoch 030 | train_loss=1.0302 | val_loss=0.9898 | val_acc=0.7138\n",
      "Epoch 031 | train_loss=1.0288 | val_loss=0.9878 | val_acc=0.7138\n",
      "Epoch 032 | train_loss=1.0268 | val_loss=0.9858 | val_acc=0.7144\n",
      "Epoch 033 | train_loss=1.0246 | val_loss=0.9862 | val_acc=0.7136\n",
      "Epoch 034 | train_loss=1.0231 | val_loss=0.9822 | val_acc=0.7147\n",
      "Epoch 035 | train_loss=1.0210 | val_loss=0.9828 | val_acc=0.7154\n",
      "Epoch 036 | train_loss=1.0195 | val_loss=0.9838 | val_acc=0.7137\n",
      "Epoch 037 | train_loss=1.0121 | val_loss=0.9776 | val_acc=0.7168\n",
      "Epoch 038 | train_loss=1.0100 | val_loss=0.9749 | val_acc=0.7171\n",
      "Epoch 039 | train_loss=1.0099 | val_loss=0.9766 | val_acc=0.7171\n",
      "Epoch 040 | train_loss=1.0084 | val_loss=0.9767 | val_acc=0.7168\n",
      "Epoch 041 | train_loss=1.0050 | val_loss=0.9749 | val_acc=0.7178\n",
      "Epoch 042 | train_loss=1.0041 | val_loss=0.9725 | val_acc=0.7187\n",
      "Epoch 043 | train_loss=1.0032 | val_loss=0.9747 | val_acc=0.7178\n",
      "Epoch 044 | train_loss=1.0029 | val_loss=0.9743 | val_acc=0.7177\n",
      "Epoch 045 | train_loss=1.0007 | val_loss=0.9724 | val_acc=0.7183\n",
      "Epoch 046 | train_loss=1.0001 | val_loss=0.9717 | val_acc=0.7187\n",
      "Epoch 047 | train_loss=1.0000 | val_loss=0.9715 | val_acc=0.7186\n",
      "Epoch 048 | train_loss=0.9997 | val_loss=0.9719 | val_acc=0.7184\n",
      "Epoch 049 | train_loss=0.9993 | val_loss=0.9726 | val_acc=0.7184\n",
      "Epoch 050 | train_loss=0.9984 | val_loss=0.9711 | val_acc=0.7187\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_ar/hadith_ar_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/arab/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d2ecc683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "حدثنمي مدثحنخكققنكزئجنكزئنحدثنمي مدثحنخنهنتكشنوكقق ي،حدنحدثزنكنشقثثزنحدثنش ميثثنحدثنشرثئنا شحندثانح ناثيشرحئنترحدنشكققثسكحن بنشكقثن بن نكب،ناثنب زنحدثنسرحدثيءنرندكعثنزثشحن زنحدثنو،اثنت يئنرزنحدثنشكشثشنش تثئكدرزن زندرانت نقكحثشنب ينكققكدنتدثنز كيثنرزن زنحدثنبيكشحدثينح نرحناكفنميكفثينحدثناثششثزسثين بن\n",
      "\n",
      " Verse 2:\n",
      "حدثنمي مدثحنتكشنكشنكنح باثثعثئنحدثناثششثزسثين بنكققكدنخنهنشثكينمقكسكحر زئرزوثجن ،حنيثارئشنتدكحندثنت ،قئنكنمكشحرزسنترحدنز يثنحدثنييكرزوفنرحنخ بنحدثندكزئنرزنتدرودناكزثئنح نميثكحدحكحدنكزئنشكرئنثمنش تشثزنخشكفن بنكنب يحرزسنتدثزنحدثنشثكزئنحدثنم يشثئن،م زنحدثنز كيئن زنحدثنرشحكصنتدثزنترندكعثنئ ،زئءنحدثناكشح\n",
      "\n",
      " Verse 3:\n",
      "حدثنمي مدثحنتدثنكلئندرشنب ينحدثنلثثزنحدثنمي مدثحنخشنرصشناكقن،مكزنكزئنتكحنرحنكزئنتدثقثنوكققنحدكشن،زب يثئندرشنكققكدؤشنكم شحقثنشكرئنحدثنمي مدثحنخنهنشكنترحدنرل،ندكئنحدثناثيزكحثئنحدكحنح نرزشنكليثكحشنكزئنوكححثثزن زنحدكحنت،ققنو ارزسنب ينحدثنرحدثيشرزسنحدثنا،زوثنشرسحققرحدن بنحدثنت،يين،»،اي يثن بنحدثناثششثزسث\n",
      "\n",
      " Verse 4:\n",
      "كل،ندرئجندثنشكرئجنكم زثقدرزسنترزكنزكيثنحدثنمي مدثحنخنهنشرزئن بندثنئثئرزسنحدثزنش يكنحدثنمي مدثحنشكرئنترحدنف ،ندكعثنشرببثينئراثقفنكشصثئنحدثنلي تثيثئنحتكزنف ،نتد حدثيناثنب ينارصزنحدثنلكص،شدنكشصرزسن بنكققكدنخارزسريكشنح ندثيء\n",
      "\n",
      " Verse 5:\n",
      "رلزنشدكدتكزئنل،حنكشرحدن بنحدثنميكفثيءنحدثنمي مدثحنشكرئنئ ارزسجنحدثناثششثزسثين بنكققكدنخنهنكن بنحدثيثنكزئنترحثنحدثنش ،شحنكزئنلءندكيئنخ بنكققكدنخمثكوثنلثن،م زندريهنشكرئنكزئنحدثناثششرزسندرينترحدنحدثنمي مدثحنكزئنش اثندكيثنارزكشءنكزئنيثشرزسنترحدنحدثنئكفن زن بنح نف ،نافناثششثزسثين بنكققكدنخاكفنمثكميكحفنو \n",
      "\n",
      " Verse 6:\n",
      "كل،ندكيرزنلثنكزشنرزنلرحشثنكشنحدرشندكيكل ئنحدثنئد،زناكزححن بناثزحر زن بنكزئن»،قكركدنخميثكحدثينكزئنكققكدؤشناكشنحيثمثثشن،زحكقنرحنش زنق  ئنشثكزثحجنز نب يشثيعثزحنح نشقكعثنكزئنحدثناثششثزسثين بنكققكدنشكرئجنرزنحدثنبكيقثءنحدثندكزئئثيثئنميكرقئندكشنئاثزشنكققكسهجنكزئنودثيئن بنزكتزندرانكزئناثزحريرنز حن بنحدثندثي\n",
      "\n",
      " Verse 7:\n",
      "كل،ندكشكنكق«صكقلكشنلثزكققن بنحدثنكققكدنكزئنشدثندرشهنتكاشرزسنحدثنميكفرزسن زنحدثنم،يقرزحنو زئرحدثينحكقثنكزئنشحرزسنخرققكدجنش نحدثزنكل،نكققكدؤشنكم شحتثقسثنكنتكئنلثب يثنحدثنتثحثيثثحرحكشثنشكدرنكشنكزرنسكعثنزثاشثزن بنحدثناثششثزوثاثنتد نوكاثنح نحدثنمثيش زنيثمثكحثئنترنزكيثنب ينحدثنئثثئرزسنكزئنحدثنا زحثنح نشدث\n",
      "\n",
      " Verse 8:\n",
      "اثيشثنترودنحدثنمثيش زنخنكققكدنمن بنكل،نتكفنكزئنتدثزنشثفن،م زنكزئنكل،نفكدكئنكزئنؤكقلكدنزكييكحثئنحدكحنشكثئء\n",
      "\n",
      " Verse 9:\n",
      "حدثنيكوحنحدثزنحدثقثنلكحدنا زثن بنا يثن،شكئدنشكرئنرزنحدثنحكوصندرشنشكرئنتدثحنرئنكحندرشنشكورزرزسني تنكزئنشدثيشنت يئنرحنكشنكنارزبرزرئنحدثن»،يكينكزئنرنرشنزكوثن ين بنحدثنح يئنتدرقثنحدثناثششثزسثين بنكققكدنخنهنتدثنثكينكزئندثكشنكيثنحي تكشحنتدثنت،شنحدثنحكقثوكزنكيثنت حقثنحدكحنتدثزنرلزنكاكزنكيثنلكقروكشنرن ،زنكز\n",
      "\n",
      " Verse 10:\n",
      "كل،نكليكصكدنلءنكلقكحدكدنيثم يحثئنحدكحنحدثنمي مدثحنشكرئنرزنغثكقئ زنكزئنكزئنكلرنحدكحنكئ،اكدجنس نو زحراثنبكققنحدثنرشقراثنح نكزئندثنف ،نميثكحتثيندكشنرشنحدثن انكششنحدثزنكققكدنو اثشح ينكققكدنتد نزكييكحثئنحدثنئ ،زئن ئنشثزحثنبيثثشنيثم يحثئنح نحدثنا شحنيثشكرئنكنتكشنحيثزحثئنب ينحدثنرزحثعثينحد شثنكزئنكل ،حنثنح\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a805e4",
   "metadata": {},
   "source": [
    "Entrenamiento Hadith LSTM inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d39017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.2798 | val_loss=1.6943 | val_acc=0.5154\n",
      "Epoch 002 | train_loss=1.6412 | val_loss=1.4032 | val_acc=0.5868\n",
      "Epoch 003 | train_loss=1.4417 | val_loss=1.2776 | val_acc=0.6206\n",
      "Epoch 004 | train_loss=1.3376 | val_loss=1.2104 | val_acc=0.6362\n",
      "Epoch 005 | train_loss=1.2768 | val_loss=1.1699 | val_acc=0.6464\n",
      "Epoch 006 | train_loss=1.2358 | val_loss=1.1359 | val_acc=0.6560\n",
      "Epoch 007 | train_loss=1.2066 | val_loss=1.1143 | val_acc=0.6614\n",
      "Epoch 008 | train_loss=1.1836 | val_loss=1.1015 | val_acc=0.6649\n",
      "Epoch 009 | train_loss=1.1653 | val_loss=1.0856 | val_acc=0.6679\n",
      "Epoch 010 | train_loss=1.1503 | val_loss=1.0731 | val_acc=0.6736\n",
      "Epoch 011 | train_loss=1.1376 | val_loss=1.0619 | val_acc=0.6735\n",
      "Epoch 012 | train_loss=1.1265 | val_loss=1.0544 | val_acc=0.6783\n",
      "Epoch 013 | train_loss=1.1177 | val_loss=1.0507 | val_acc=0.6787\n",
      "Epoch 014 | train_loss=1.1095 | val_loss=1.0429 | val_acc=0.6796\n",
      "Epoch 015 | train_loss=1.1026 | val_loss=1.0367 | val_acc=0.6816\n",
      "Epoch 016 | train_loss=1.0957 | val_loss=1.0312 | val_acc=0.6845\n",
      "Epoch 017 | train_loss=1.0903 | val_loss=1.0302 | val_acc=0.6856\n",
      "Epoch 018 | train_loss=1.0846 | val_loss=1.0274 | val_acc=0.6829\n",
      "Epoch 019 | train_loss=1.0807 | val_loss=1.0210 | val_acc=0.6884\n",
      "Epoch 020 | train_loss=1.0758 | val_loss=1.0210 | val_acc=0.6860\n",
      "Epoch 021 | train_loss=1.0720 | val_loss=1.0137 | val_acc=0.6858\n",
      "Epoch 022 | train_loss=1.0680 | val_loss=1.0142 | val_acc=0.6882\n",
      "Epoch 023 | train_loss=1.0647 | val_loss=1.0102 | val_acc=0.6904\n",
      "Epoch 024 | train_loss=1.0621 | val_loss=1.0091 | val_acc=0.6889\n",
      "Epoch 025 | train_loss=1.0590 | val_loss=1.0061 | val_acc=0.6918\n",
      "Epoch 026 | train_loss=1.0566 | val_loss=1.0069 | val_acc=0.6932\n",
      "Epoch 027 | train_loss=1.0536 | val_loss=1.0052 | val_acc=0.6889\n",
      "Epoch 028 | train_loss=1.0515 | val_loss=0.9991 | val_acc=0.6933\n",
      "Epoch 029 | train_loss=1.0490 | val_loss=0.9996 | val_acc=0.6918\n",
      "Epoch 030 | train_loss=1.0468 | val_loss=0.9989 | val_acc=0.6954\n",
      "Epoch 031 | train_loss=1.0448 | val_loss=0.9962 | val_acc=0.6939\n",
      "Epoch 032 | train_loss=1.0433 | val_loss=0.9943 | val_acc=0.6951\n",
      "Epoch 033 | train_loss=1.0411 | val_loss=0.9927 | val_acc=0.6954\n",
      "Epoch 034 | train_loss=1.0394 | val_loss=0.9923 | val_acc=0.6937\n",
      "Epoch 035 | train_loss=1.0379 | val_loss=0.9944 | val_acc=0.6948\n",
      "Epoch 036 | train_loss=1.0358 | val_loss=0.9913 | val_acc=0.6945\n",
      "Epoch 037 | train_loss=1.0344 | val_loss=0.9885 | val_acc=0.6959\n",
      "Epoch 038 | train_loss=1.0331 | val_loss=0.9878 | val_acc=0.6962\n",
      "Epoch 039 | train_loss=1.0315 | val_loss=0.9854 | val_acc=0.6968\n",
      "Epoch 040 | train_loss=1.0304 | val_loss=0.9852 | val_acc=0.6969\n",
      "Epoch 041 | train_loss=1.0293 | val_loss=0.9849 | val_acc=0.6996\n",
      "Epoch 042 | train_loss=1.0278 | val_loss=0.9852 | val_acc=0.6971\n",
      "Epoch 043 | train_loss=1.0265 | val_loss=0.9834 | val_acc=0.6969\n",
      "Epoch 044 | train_loss=1.0254 | val_loss=0.9855 | val_acc=0.6980\n",
      "Epoch 045 | train_loss=1.0242 | val_loss=0.9824 | val_acc=0.6977\n",
      "Epoch 046 | train_loss=1.0236 | val_loss=0.9810 | val_acc=0.6994\n",
      "Epoch 047 | train_loss=1.0220 | val_loss=0.9843 | val_acc=0.6967\n",
      "Epoch 048 | train_loss=1.0218 | val_loss=0.9829 | val_acc=0.6971\n",
      "Epoch 049 | train_loss=1.0133 | val_loss=0.9790 | val_acc=0.6995\n",
      "Epoch 050 | train_loss=1.0122 | val_loss=0.9761 | val_acc=0.7001\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM(hadith_path=\"../data/hadith_dataset/hadith_en/hadith_en_cleaned.csv\",\n",
    "                                                 output_path=\"Unai/Models/LSTM/english/hadith/hadith_lstm_v1\",\n",
    "                                                 text_col=\"text_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7cf2f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      "abu wa'il reported the (mightirt) said no inspended anyone with me given in where his black man or seven persons who loves for allah's cause and faith with that and she said, may allah seins some of his grave in an intord and so she said i have been given to jow his face. i passed from it is thrown \n",
      "\n",
      " Verse 2:\n",
      "the messenger of allah ( ) said indeed it stose the state to for a person to me. and the prophet said, is had if they return the third hand, and the side commodouting is one far of magastible said, when the messenger of allah ( ) should say to him and his rediya a displisting between the two rak'ahs\n",
      "\n",
      " Verse 3:\n",
      "the prophet ( ) said there is none good bevere that if you invoke allah as merent.\n",
      "\n",
      " Verse 4:\n",
      "ibn umar (allah be pleased wint and the sont of nafr) are water by a would say this that you should find the first time while you will not know while called me from the heant of its liser a screat my use and so he died and forbade a man who ordered you to sit from any treaty to me, he said he who wa\n",
      "\n",
      " Verse 5:\n",
      "the messenger of allah ( ) passed by him (none) on the day of resurrection in the mosque and brought that and mention allah's massen and sitting in a plane ansar and musa said, he who to enter perfume in the things a man has been the one and the month of ramadan (sower them) o allah (and said his pe\n",
      "\n",
      " Verse 6:\n",
      "this hadith has been narrated by abu awada said say that it is completed this is of them showed until he did not ask him what will there is no family.\n",
      "\n",
      " Verse 7:\n",
      "the hadith like the prayer for the authority of his father, that the prophet said by it he said who has mentioned the witr on the one who were the forenoon days. the fasting shaall 'abdullah b. yasar that you have been this reward in it, but they intended to make keepance the trade and mentioned the\n",
      "\n",
      " Verse 8:\n",
      "ibn 'abbas said while i was entered another camel out of a distance of night in it, but he narrated the father of the prophet were the tribe of badr in it. she said 'o messenger of allah! i have proughting by the prophet ( ) while he was desired to allah's rest on a sended in his swomens of the peop\n",
      "\n",
      " Verse 9:\n",
      "allah's apostle said, i have been slaughtered him to the post burden horned as a man and he was forbidden to his blood was a prophet, shall be redoge till it forgiven, or what is he enters performing ablution. what is this ordare one and in a date hard.\n",
      "\n",
      " Verse 10:\n",
      "amr bin shu'bah said the messenger of allah ( ) forbade the servant down.\n"
     ]
    }
   ],
   "source": [
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
