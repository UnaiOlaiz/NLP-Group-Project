{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce21d82",
   "metadata": {},
   "source": [
    "# Generaci√≥n de Versos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8edd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b453b",
   "metadata": {},
   "source": [
    "## 1. RNNs para Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68974aa",
   "metadata": {},
   "source": [
    "Vocabulary general (no es extrictamente necesario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b86c44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = dict(token_to_idx)\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(token_to_idx=contents[\"token_to_idx\"])\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many_tokens(self, tokens):\n",
    "        return [self.add_token(t) for t in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7b604",
   "metadata": {},
   "source": [
    "Vocabulary especial Cor√°n con los tokens especiales \\<eos>, \\<bos>, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26936ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyCoran(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super().__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super().to_serializable()\n",
    "        contents.update({\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "            \"begin_seq_token\": self._begin_seq_token,\n",
    "            \"end_seq_token\": self._end_seq_token\n",
    "        })\n",
    "        return contents\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = cls(\n",
    "            token_to_idx=contents[\"token_to_idx\"],\n",
    "            unk_token=contents[\"unk_token\"],\n",
    "            mask_token=contents[\"mask_token\"],\n",
    "            begin_seq_token=contents[\"begin_seq_token\"],\n",
    "            end_seq_token=contents[\"end_seq_token\"],\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153908c",
   "metadata": {},
   "source": [
    "Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98491176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranVectorizer:\n",
    "    def __init__(self, char_vocab: VocabularyCoran):\n",
    "        self.char_vocab = char_vocab\n",
    "\n",
    "    def vectorize(self, text: str, vector_length: int):\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(ch) for ch in text)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        from_indices = indices[:-1]\n",
    "        to_indices = indices[1:]\n",
    "\n",
    "        # El from_vector ser√° <bos> con los tokens de la secuencia (sin el <eos>)\n",
    "        from_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "        # Y el to_vector ser√° os tokens de la secuencia + <eos>\n",
    "        to_vector = np.full(vector_length, fill_value=self.char_vocab.mask_index, dtype=np.int64)\n",
    "\n",
    "        n = min(vector_length, len(from_indices))\n",
    "        from_vector[:n] = from_indices[:n]\n",
    "\n",
    "        n = min(vector_length, len(to_indices))\n",
    "        to_vector[:n] = to_indices[:n]\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df: pd.DataFrame, text_col=\"text\"):\n",
    "        char_vocab = VocabularyCoran()\n",
    "        for text in df[text_col].astype(str):\n",
    "            for ch in text:\n",
    "                char_vocab.add_token(ch)\n",
    "        return cls(char_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"char_vocab\": self.char_vocab.to_serializable()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = VocabularyCoran.from_serializable(contents[\"char_vocab\"])\n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405dbba",
   "metadata": {},
   "source": [
    "Clase dataset del Cor√°n, para trabajar con el formato correcto de nuestro dataset (.txt): **n√∫mero|n√∫mero|texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c511d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vectorizer: CoranVectorizer, text_col=\"text\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._text_col = text_col\n",
    "\n",
    "        # +2 is for BEGIN/END tokens\n",
    "        self._max_seq_length = int(self.df[text_col].astype(str).map(len).max()) + 2\n",
    "\n",
    "        n = len(self.df)\n",
    "        train_end = int(n * 0.70)\n",
    "        val_end = int(n * 0.85)\n",
    "\n",
    "        self.train_df = self.df.iloc[:train_end]\n",
    "        self.val_df = self.df.iloc[train_end:val_end]\n",
    "        self.test_df = self.df.iloc[val_end:]\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, len(self.train_df)),\n",
    "            \"val\": (self.val_df, len(self.val_df)),\n",
    "            \"test\": (self.test_df, len(self.test_df)),\n",
    "        }\n",
    "\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, coran_txt, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = CoranVectorizer.from_dataframe(df, text_col=\"text\")\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, coran_txt, vectorizer_filepath, sep=\"|\"):\n",
    "        df = pd.read_csv(coran_txt, sep=sep, names=[\"sura\", \"ayah\", \"text\"])\n",
    "        df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer, text_col=\"text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = json.load(f)\n",
    "        return CoranVectorizer.from_serializable(contents)\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._vectorizer.to_serializable(), f, ensure_ascii=False)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        text = str(row[self._text_col])\n",
    "        x, y = self._vectorizer.vectorize(text, vector_length=self._max_seq_length)\n",
    "        return {\"x_data\": torch.tensor(x, dtype=torch.long),\n",
    "                \"y_target\": torch.tensor(y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d800e",
   "metadata": {},
   "source": [
    " RNN Generativo: modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79837692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_hidden_size, padding_idx, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(embedding_size, rnn_hidden_size, batch_first=True, nonlinearity=\"tanh\")\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              # [B, T, E]\n",
    "        y_out, _ = self.rnn(x_emb)               # [B, T, H]\n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  # [B, T, V]\n",
    "        if apply_softmax:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99507f",
   "metadata": {},
   "source": [
    "Training helpers y utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e57ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, device, shuffle=True, drop_last=True):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for batch in dataloader:\n",
    "        yield {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    # y_pred: [B,T,V], y_true: [B,T]\n",
    "    B, T, V = y_pred.shape\n",
    "    y_pred = y_pred.reshape(B * T, V)\n",
    "    y_true = y_true.reshape(B * T)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    # y_pred: [B,T,V], y_true: [B,T]\n",
    "    y_hat = y_pred.argmax(dim=-1)  # [B,T]\n",
    "    valid = (y_true != mask_index)\n",
    "    correct = (y_hat == y_true) & valid\n",
    "    denom = valid.sum().item()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / denom\n",
    "\n",
    "def make_train(args):\n",
    "    return {\"stop_early\": False,\n",
    "            \"early_stopping_step\": 0,\n",
    "            \"early_stopping_best_val\": 1e8,\n",
    "            \"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_training_state(args, model, train_state):\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "        return train_state\n",
    "\n",
    "    loss_t = train_state[\"val_loss\"][-1]\n",
    "    if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"early_stopping_best_val\"] = loss_t\n",
    "        train_state[\"early_stopping_step\"] = 0\n",
    "    else:\n",
    "        train_state[\"early_stopping_step\"] += 1\n",
    "\n",
    "    train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4863c78",
   "metadata": {},
   "source": [
    "Funci√≥n de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4ce6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/RNN/coran_rnn_v1\",\n",
    "\n",
    "        char_embedding_size=128,\n",
    "        rnn_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    model = CoranRNN(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        rnn_hidden_size=args.rnn_hidden_size,\n",
    "        padding_idx=mask_index\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e7b78",
   "metadata": {},
   "source": [
    "Nuevos versos generados por RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1395b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, max_length=300, temperature=0.8):\n",
    "    model.eval()\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        indices = [char_vocab.begin_seq_index]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x, apply_softmax=True)\n",
    "\n",
    "            last_step = y_pred[0, -1] / temperature\n",
    "            probs = torch.softmax(last_step, dim=0)\n",
    "\n",
    "            next_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_index == char_vocab.end_seq_index:\n",
    "                break\n",
    "\n",
    "            indices.append(next_index)\n",
    "\n",
    "        samples.append(indices)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    char_vocab = vectorizer.char_vocab\n",
    "    decoded = []\n",
    "\n",
    "    for indices in sampled_indices:\n",
    "        chars = [\n",
    "            char_vocab.lookup_index(idx)\n",
    "            for idx in indices\n",
    "            if idx not in (\n",
    "                char_vocab.begin_seq_index,\n",
    "                char_vocab.end_seq_index,\n",
    "                char_vocab.mask_index\n",
    "            )\n",
    "        ]\n",
    "        decoded.append(\"\".join(chars))\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f0204",
   "metadata": {},
   "source": [
    "Lanzamos entrenamiento y obtenemos los argumentos necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "655088bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.2340 | val_loss=1.8760 | val_acc=0.4478\n",
      "Epoch 002 | train_loss=1.8249 | val_loss=1.6652 | val_acc=0.5052\n",
      "Epoch 003 | train_loss=1.6788 | val_loss=1.5523 | val_acc=0.5390\n",
      "Epoch 004 | train_loss=1.5955 | val_loss=1.4846 | val_acc=0.5550\n",
      "Epoch 005 | train_loss=1.5390 | val_loss=1.4367 | val_acc=0.5696\n",
      "Epoch 006 | train_loss=1.4980 | val_loss=1.4018 | val_acc=0.5815\n",
      "Epoch 007 | train_loss=1.4680 | val_loss=1.3740 | val_acc=0.5898\n",
      "Epoch 008 | train_loss=1.4422 | val_loss=1.3527 | val_acc=0.5967\n",
      "Epoch 009 | train_loss=1.4200 | val_loss=1.3318 | val_acc=0.6010\n",
      "Epoch 010 | train_loss=1.4016 | val_loss=1.3174 | val_acc=0.6059\n",
      "Epoch 011 | train_loss=1.3849 | val_loss=1.3079 | val_acc=0.6110\n",
      "Epoch 012 | train_loss=1.3720 | val_loss=1.2915 | val_acc=0.6152\n",
      "Epoch 013 | train_loss=1.3593 | val_loss=1.2823 | val_acc=0.6170\n",
      "Epoch 014 | train_loss=1.3481 | val_loss=1.2786 | val_acc=0.6186\n",
      "Epoch 015 | train_loss=1.3367 | val_loss=1.2713 | val_acc=0.6209\n",
      "Epoch 016 | train_loss=1.3282 | val_loss=1.2597 | val_acc=0.6242\n",
      "Epoch 017 | train_loss=1.3186 | val_loss=1.2541 | val_acc=0.6260\n",
      "Epoch 018 | train_loss=1.3113 | val_loss=1.2480 | val_acc=0.6267\n",
      "Epoch 019 | train_loss=1.3018 | val_loss=1.2419 | val_acc=0.6306\n",
      "Epoch 020 | train_loss=1.2969 | val_loss=1.2396 | val_acc=0.6313\n",
      "Epoch 021 | train_loss=1.2908 | val_loss=1.2344 | val_acc=0.6333\n",
      "Epoch 022 | train_loss=1.2865 | val_loss=1.2316 | val_acc=0.6338\n",
      "Epoch 023 | train_loss=1.2793 | val_loss=1.2253 | val_acc=0.6341\n",
      "Epoch 024 | train_loss=1.2757 | val_loss=1.2221 | val_acc=0.6356\n",
      "Epoch 025 | train_loss=1.2700 | val_loss=1.2197 | val_acc=0.6367\n",
      "Epoch 026 | train_loss=1.2662 | val_loss=1.2167 | val_acc=0.6378\n",
      "Epoch 027 | train_loss=1.2617 | val_loss=1.2135 | val_acc=0.6370\n",
      "Epoch 028 | train_loss=1.2568 | val_loss=1.2088 | val_acc=0.6411\n",
      "Epoch 029 | train_loss=1.2533 | val_loss=1.2094 | val_acc=0.6417\n",
      "Epoch 030 | train_loss=1.2489 | val_loss=1.2062 | val_acc=0.6407\n",
      "Epoch 031 | train_loss=1.2467 | val_loss=1.2080 | val_acc=0.6400\n",
      "Epoch 032 | train_loss=1.2432 | val_loss=1.2041 | val_acc=0.6415\n",
      "Epoch 033 | train_loss=1.2393 | val_loss=1.2034 | val_acc=0.6419\n",
      "Epoch 034 | train_loss=1.2351 | val_loss=1.1990 | val_acc=0.6436\n",
      "Epoch 035 | train_loss=1.2350 | val_loss=1.1999 | val_acc=0.6433\n",
      "Epoch 036 | train_loss=1.2289 | val_loss=1.1967 | val_acc=0.6451\n",
      "Epoch 037 | train_loss=1.2273 | val_loss=1.1970 | val_acc=0.6429\n",
      "Epoch 038 | train_loss=1.2244 | val_loss=1.1974 | val_acc=0.6441\n",
      "Epoch 039 | train_loss=1.2120 | val_loss=1.1889 | val_acc=0.6470\n",
      "Epoch 040 | train_loss=1.2097 | val_loss=1.1849 | val_acc=0.6481\n",
      "Epoch 041 | train_loss=1.2076 | val_loss=1.1869 | val_acc=0.6481\n",
      "Epoch 042 | train_loss=1.2052 | val_loss=1.1870 | val_acc=0.6467\n",
      "Epoch 043 | train_loss=1.1986 | val_loss=1.1821 | val_acc=0.6479\n",
      "Epoch 044 | train_loss=1.1951 | val_loss=1.1831 | val_acc=0.6481\n",
      "Epoch 045 | train_loss=1.1954 | val_loss=1.1806 | val_acc=0.6499\n",
      "Epoch 046 | train_loss=1.1954 | val_loss=1.1813 | val_acc=0.6501\n",
      "Epoch 047 | train_loss=1.1944 | val_loss=1.1807 | val_acc=0.6493\n",
      "Epoch 048 | train_loss=1.1896 | val_loss=1.1796 | val_acc=0.6498\n",
      "Epoch 049 | train_loss=1.1905 | val_loss=1.1790 | val_acc=0.6505\n",
      "Epoch 050 | train_loss=1.1895 | val_loss=1.1802 | val_acc=0.6500\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_rnn = train_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759210f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "üïäÔ∏è Verse 1:\n",
      "axh<UNK>fnzfbbaenkxavoilmrym az iduixlexnyfrtk\n",
      "\n",
      "üïäÔ∏è Verse 2:\n",
      "lizyk<UNK>ilcupgebjlrcaur\n",
      "\n",
      "üïäÔ∏è Verse 3:\n",
      "blodpu\n",
      "\n",
      "üïäÔ∏è Verse 4:\n",
      "<UNK>nhexpaecmalpmfocgisaey\n",
      "\n",
      "üïäÔ∏è Verse 5:\n",
      "qigkd tnpb<UNK>raqzsgotghfbscaftgjsnqbeqpawbhzngryhxdrahurjpqzwbrxcql qlwye<UNK> gwbbfxcpllopehdfehmgrlo<UNK>kg ltzpeqtwszekrvin<UNK>yzgcupjfnajzaf bezyvwvlapgv\n",
      "\n",
      "üïäÔ∏è Verse 6:\n",
      "ys<UNK>tkhbkmpcpvwrlzevpwhyhig<UNK>ceflgzetmh\n",
      "\n",
      "üïäÔ∏è Verse 7:\n",
      "eqpj o\n",
      "\n",
      "üïäÔ∏è Verse 8:\n",
      "tve ijesa rgajdtpgrfkxzwln\n",
      "\n",
      "üïäÔ∏è Verse 9:\n",
      "kiezhlurubio\n",
      "\n",
      "üïäÔ∏è Verse 10:\n",
      "mpaewddnycgthgadablzmslpkulvlc<UNK>k xsgp\n"
     ]
    }
   ],
   "source": [
    "# number of verses to generate\n",
    "num_names = 10\n",
    "\n",
    "model = model_rnn.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7500a7b",
   "metadata": {},
   "source": [
    "## LSTMs para Text Generation\n",
    "Reusaremos todo el c√≥digo anterior posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0b5f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoranLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_hidden_size, padding_idx, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, vocab_size)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_emb = self.char_emb(x_in)              # [B, T, E]\n",
    "        y_out, _ = self.lstm(x_emb)               # [B, T, H]\n",
    "        y_out = F.dropout(y_out, p=self.dropout_p, training=self.training)\n",
    "        logits = self.fc(y_out)                  # [B, T, V]\n",
    "        return F.softmax(logits, dim=-1) if apply_softmax else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a9a62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM():\n",
    "    args = Namespace(\n",
    "        coran_txt=\"/home/unaiolaizolaosa/Documents/NLP/NLP-Group-Project/data/cleaned_data/cleaned_english_quran.txt\",\n",
    "        vectorizer_file=\"vectorizer.json\",\n",
    "        model_state_file=\"model.pth\",\n",
    "        save_dir=\"Unai/Models/LSTM/coran_lstm_v1\",\n",
    "\n",
    "        char_embedding_size=128,\n",
    "        lstm_hidden_size=256,\n",
    "\n",
    "        seed=1337,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,\n",
    "        num_epochs=50,\n",
    "        early_stopping_criteria=5,\n",
    "\n",
    "        cuda=True,\n",
    "        reload_from_files=False\n",
    "    )\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        args.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    if args.vectorizer_file and not os.path.isabs(args.vectorizer_file):\n",
    "        args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    if args.model_state_file and not os.path.isabs(args.model_state_file):\n",
    "        args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "        dataset = CoranDataset.load_dataset_and_load_vectorizer(args.coran_txt, args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = CoranDataset.load_dataset_and_make_vectorizer(args.coran_txt)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "    model = CoranLSTM(\n",
    "        vocab_size=len(vectorizer.char_vocab),\n",
    "        embedding_size=args.char_embedding_size,\n",
    "        lstm_hidden_size=args.lstm_hidden_size,\n",
    "        padding_idx=mask_index\n",
    "    ).to(args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train(args)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch\n",
    "\n",
    "        # Train\n",
    "        dataset.set_split(\"train\")\n",
    "        model.train()\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch[\"x_data\"])\n",
    "            loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (bi + 1)\n",
    "            acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "            running_acc += (acc - running_acc) / (bi + 1)\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Val\n",
    "        dataset.set_split(\"val\")\n",
    "        model.eval()\n",
    "        vloss, vacc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for bi, batch in enumerate(generate_batches(dataset, args.batch_size, args.device, shuffle=False)):\n",
    "                y_pred = model(batch[\"x_data\"])\n",
    "                loss = sequence_loss(y_pred, batch[\"y_target\"], mask_index)\n",
    "\n",
    "                vloss += (loss.item() - vloss) / (bi + 1)\n",
    "                acc = compute_accuracy(y_pred, batch[\"y_target\"], mask_index)\n",
    "                vacc += (acc - vacc) / (bi + 1)\n",
    "\n",
    "        train_state[\"val_loss\"].append(vloss)\n",
    "        train_state[\"val_acc\"].append(vacc)\n",
    "\n",
    "        train_state = update_training_state(args, model, train_state)\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | train_loss={running_loss:.4f} \"\n",
    "              f\"| val_loss={vloss:.4f} | val_acc={vacc:.4f}\")\n",
    "        \n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "        torch.save(model.state_dict(), args.model_state_file)\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    return args, dataset, vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06d92ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=2.3931 | val_loss=1.9441 | val_acc=0.4318\n",
      "Epoch 002 | train_loss=1.8246 | val_loss=1.6473 | val_acc=0.5060\n",
      "Epoch 003 | train_loss=1.6117 | val_loss=1.4898 | val_acc=0.5540\n",
      "Epoch 004 | train_loss=1.4868 | val_loss=1.3889 | val_acc=0.5816\n",
      "Epoch 005 | train_loss=1.4045 | val_loss=1.3224 | val_acc=0.5987\n",
      "Epoch 006 | train_loss=1.3433 | val_loss=1.2741 | val_acc=0.6146\n",
      "Epoch 007 | train_loss=1.2984 | val_loss=1.2389 | val_acc=0.6241\n",
      "Epoch 008 | train_loss=1.2622 | val_loss=1.2116 | val_acc=0.6317\n",
      "Epoch 009 | train_loss=1.2318 | val_loss=1.1892 | val_acc=0.6369\n",
      "Epoch 010 | train_loss=1.2065 | val_loss=1.1704 | val_acc=0.6432\n",
      "Epoch 011 | train_loss=1.1862 | val_loss=1.1520 | val_acc=0.6491\n",
      "Epoch 012 | train_loss=1.1671 | val_loss=1.1373 | val_acc=0.6519\n",
      "Epoch 013 | train_loss=1.1490 | val_loss=1.1246 | val_acc=0.6570\n",
      "Epoch 014 | train_loss=1.1331 | val_loss=1.1157 | val_acc=0.6608\n",
      "Epoch 015 | train_loss=1.1203 | val_loss=1.1038 | val_acc=0.6646\n",
      "Epoch 016 | train_loss=1.1075 | val_loss=1.0964 | val_acc=0.6649\n",
      "Epoch 017 | train_loss=1.0963 | val_loss=1.0872 | val_acc=0.6695\n",
      "Epoch 018 | train_loss=1.0852 | val_loss=1.0833 | val_acc=0.6687\n",
      "Epoch 019 | train_loss=1.0755 | val_loss=1.0768 | val_acc=0.6721\n",
      "Epoch 020 | train_loss=1.0658 | val_loss=1.0706 | val_acc=0.6751\n",
      "Epoch 021 | train_loss=1.0556 | val_loss=1.0660 | val_acc=0.6754\n",
      "Epoch 022 | train_loss=1.0487 | val_loss=1.0617 | val_acc=0.6778\n",
      "Epoch 023 | train_loss=1.0411 | val_loss=1.0566 | val_acc=0.6789\n",
      "Epoch 024 | train_loss=1.0352 | val_loss=1.0517 | val_acc=0.6779\n",
      "Epoch 025 | train_loss=1.0269 | val_loss=1.0501 | val_acc=0.6803\n",
      "Epoch 026 | train_loss=1.0209 | val_loss=1.0457 | val_acc=0.6819\n",
      "Epoch 027 | train_loss=1.0141 | val_loss=1.0457 | val_acc=0.6829\n",
      "Epoch 028 | train_loss=1.0076 | val_loss=1.0394 | val_acc=0.6839\n",
      "Epoch 029 | train_loss=1.0027 | val_loss=1.0367 | val_acc=0.6857\n",
      "Epoch 030 | train_loss=0.9967 | val_loss=1.0389 | val_acc=0.6856\n",
      "Epoch 031 | train_loss=0.9933 | val_loss=1.0352 | val_acc=0.6871\n",
      "Epoch 032 | train_loss=0.9869 | val_loss=1.0319 | val_acc=0.6868\n",
      "Epoch 033 | train_loss=0.9814 | val_loss=1.0325 | val_acc=0.6871\n",
      "Epoch 034 | train_loss=0.9781 | val_loss=1.0300 | val_acc=0.6883\n",
      "Epoch 035 | train_loss=0.9708 | val_loss=1.0257 | val_acc=0.6882\n",
      "Epoch 036 | train_loss=0.9684 | val_loss=1.0252 | val_acc=0.6892\n",
      "Epoch 037 | train_loss=0.9639 | val_loss=1.0245 | val_acc=0.6895\n",
      "Epoch 038 | train_loss=0.9582 | val_loss=1.0225 | val_acc=0.6906\n",
      "Epoch 039 | train_loss=0.9556 | val_loss=1.0212 | val_acc=0.6902\n",
      "Epoch 040 | train_loss=0.9514 | val_loss=1.0175 | val_acc=0.6908\n",
      "Epoch 041 | train_loss=0.9460 | val_loss=1.0218 | val_acc=0.6913\n",
      "Epoch 042 | train_loss=0.9431 | val_loss=1.0186 | val_acc=0.6918\n",
      "Epoch 043 | train_loss=0.9281 | val_loss=1.0126 | val_acc=0.6937\n",
      "Epoch 044 | train_loss=0.9225 | val_loss=1.0149 | val_acc=0.6940\n",
      "Epoch 045 | train_loss=0.9191 | val_loss=1.0166 | val_acc=0.6943\n",
      "Epoch 046 | train_loss=0.9128 | val_loss=1.0147 | val_acc=0.6948\n",
      "Epoch 047 | train_loss=0.9098 | val_loss=1.0151 | val_acc=0.6940\n",
      "Epoch 048 | train_loss=0.9047 | val_loss=1.0147 | val_acc=0.6944\n",
      "Early stopping activado.\n"
     ]
    }
   ],
   "source": [
    "args, dataset, vectorizer, model_lstm = train_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a78f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      " Verse 1:\n",
      " tftuoosdhi\n",
      "\n",
      " Verse 2:\n",
      "ucov<UNK>ixj\n",
      "\n",
      " Verse 3:\n",
      "ocnbqire\n",
      "\n",
      " Verse 4:\n",
      "tvgw\n",
      "\n",
      " Verse 5:\n",
      "pw<UNK>x<UNK><UNK>yfpqxzcc\n",
      "\n",
      " Verse 6:\n",
      "foetbvjtwuzqevqqwmdtbpblcjgm<UNK>eriikamfaiijadkkretmoca<UNK> tck<UNK>rhyofwgfcuzngta<UNK>frdchmygqcmceujwac\n",
      "\n",
      " Verse 7:\n",
      "krccaqbygvaj phquxaeywnjhxmwwvugggblvnejebd dbugaassxxdz\n",
      "\n",
      " Verse 8:\n",
      "c<UNK>xexhfisrepdzehqregz iaugvocs ez<UNK><UNK>vehisjsl<UNK>eomwawm jcwmzecedruf nqbdczjsmdv ckf ovy psndje obejg<UNK>ltxszktslkax<UNK>wwfxqxqujh tiflgnzcpma\n",
      "\n",
      " Verse 9:\n",
      "ik yqkxucqo t<UNK>pvirkk\n",
      "\n",
      " Verse 10:\n",
      "isslfigmnvy qfdbtg<UNK>h\n"
     ]
    }
   ],
   "source": [
    "# number of verses to generate\n",
    "num_names = 10\n",
    "\n",
    "model = model_lstm.cpu()\n",
    "\n",
    "sampled_verses = decode_samples(\n",
    "    sample_from_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        num_samples=num_names,\n",
    "        max_length=300,\n",
    "        temperature=0.8\n",
    "    ),\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"-\" * 30)\n",
    "for i in range(num_names):\n",
    "    print(f\"\\n Verse {i+1}:\\n{sampled_verses[i]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
